{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae58e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'tiny-ml'...\n",
      "remote: Enumerating objects: 11630, done.\u001b[K\n",
      "remote: Counting objects: 100% (317/317), done.\u001b[K\n",
      "remote: Compressing objects: 100% (154/154), done.\u001b[K\n",
      "remote: Total 11630 (delta 160), reused 314 (delta 158), pack-reused 11313\u001b[K\n",
      "Receiving objects: 100% (11630/11630), 689.27 MiB | 13.22 MiB/s, done.\n",
      "Resolving deltas: 100% (2365/2365), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/wandb/tiny-ml.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "754196eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T11:27:36.482912Z",
     "start_time": "2023-01-28T11:27:32.170223Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import wandb\n",
    "import audiomentations\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.io.wavfile import read\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import time\n",
    "import struct\n",
    "import wave\n",
    "from pvrecorder import PvRecorder\n",
    "import IPython\n",
    "import os\n",
    "\n",
    "# some utilities that weve made in \n",
    "\n",
    "import recorder\n",
    "from utils.data_processing import augmenter,log_wandb_artifact,plot_spectrogram,segment,read_wav,Arm_spect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab484896",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_SILENT\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25744cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'theme': 'sky',\n",
       " 'transition': 'zoom',\n",
       " 'start_slideshow_at': 'selected',\n",
       " 'scroll': 'false'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "from pathlib import Path\n",
    "path = Path.home() / \".jupyter\" / \"nbconfig\"\n",
    "cm = BaseJSONConfigManager(config_dir=str(path))\n",
    "cm.update(\n",
    "    \"rise\",\n",
    "    {\n",
    "        \"theme\": \"sky\",\n",
    "        \"transition\": \"zoom\",\n",
    "        \"start_slideshow_at\": \"selected\",\n",
    "        'scroll':'false'\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff20e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T11:30:00.672579Z",
     "start_time": "2023-01-28T11:30:00.292579Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](slides/tiny_ml_cover.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b9b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](slides/ml_morphology.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25943d74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](slides/mlops_graph.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df11f7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](slides/edge_devices.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1d7fb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<table style=\"height:1000px;width:100%\">\n",
    " <tr>\n",
    "    <td> <img src=\"./slides/spark_micro_mod_pico_1.jpg\" alt=\"Drawing\" style=\"height: 525px;width:800px;\" align=\"right\"/> </td>\n",
    "    <td> <img src=\"./slides/micro_mod_pico_2.jpg\" alt=\"Drawing\" style=\"height: 850px; width:900px\" align=\"left\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da54139b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T11:27:48.771672Z",
     "start_time": "2023-01-28T11:27:46.173182Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrds\u001b[0m (\u001b[33mtiny-ml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1170d9759e6431d8f89f9aa222c18c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016736651383325807, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230129_201107-w0790zli</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">vivid-paper-170</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm_args = dict(leave=True,position=0)\n",
    "tqdm_colours = (f'#{r:02x}{g:02x}{b:02x}'for r,g,b in [(int(255*s), 3, int(255*s)) for s in np.linspace(1,0,100)])\n",
    "run = wandb.init(entity='tiny-ml',project = 'wake_word_detection', group='Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39dc1a45",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording\n"
     ]
    }
   ],
   "source": [
    "recorder = PvRecorder(device_index=0, frame_length=512)\n",
    "sound = []\n",
    "recorder.start()\n",
    "print('recording')\n",
    "t_0 = time.time()\n",
    "while time.time()-t_0<4:\n",
    "    frame = recorder.read()\n",
    "    sound.extend(frame)\n",
    "recorder.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f171c251",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data = np.array(sound).astype(np.int16)\n",
    "t_step = np.arange(0, len(data))\n",
    "fig = px.line(x=t_step, y=data, title='a sound wave')\n",
    "run.log({'sound_wave': fig})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0a0f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Showing a Run in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0cdd1dc",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b41bd360>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7237b4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b41bd360>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './data/yes/yes_record 102.wav'\n",
    "run.log({'test_sound':wandb.Audio(path)})\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba972c38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The basis of our signal processing is using Fast Fourier Transform (FFT)\n",
    "> - [here](https://www.youtube.com/watch?v=spUNpyF58BY) is an amazing video on how FFT works. \n",
    "> - quickly go through getting from FFT to a Spectrogram. \n",
    ">  ![](./slides/200px-Fourier_in_his_coat_of_prefect.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e0272c9",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/ipykernel_40417/660930182.py:1: ComplexWarning:\n",
      "\n",
      "Casting complex values to real discards the imaginary part\n",
      "\n",
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "fft = np.fft.fft(data).astype(np.int16)\n",
    "t_steps = np.arange(0,len(fft))\n",
    "fft = np.stack([fft,t_step]).T\n",
    "df = pd.DataFrame(fft, columns = ['Faurrier_Value','Time_Step'])\n",
    "fig = px.line(df,x='Time_Step', y='Faurrier_Value', title='Fourrier Transform')\n",
    "run.log({'Fouriere Transform':fig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e47afaf7",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b41bd360>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f788ee0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## FFT can also be inverted, we can get sound back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43d25b76",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/ipykernel_40417/1994990191.py:3: ComplexWarning:\n",
      "\n",
      "Casting complex values to real discards the imaginary part\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fft = np.fft.fft(data)\n",
    "roll = np.roll(fft,200)\n",
    "ifft = np.fft.ifft(roll).astype(np.int16)\n",
    "t_step = np.arange(len(ifft))\n",
    "ifft = np.stack([ifft,t_step]).T\n",
    "df = pd.DataFrame(ifft, columns = ['Frequecy_Value','Time_Step'])\n",
    "fig = px.line(df,x='Time_Step', \n",
    "              y='Frequecy_Value', \n",
    "              title='Inverse Fourrier Transform to Get sound Back')\n",
    "run.log({'Inverset Fouriere':fig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9df0e8e",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b41bd360>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2507bf7",
   "metadata": {},
   "source": [
    "## Down Sample using FFT as an example of Signal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eab43216",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/ipykernel_40417/3482508054.py:4: ComplexWarning:\n",
      "\n",
      "Casting complex values to real discards the imaginary part\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# and examper of downsampling using FFT\n",
    "roll = np.roll(fft,15)\n",
    "ifft_ten = np.fft.ifft(roll)\n",
    "ifft_ten = ((2**(16-4)) * ifft_ten/ifft_ten.max()).astype(np.int16)\n",
    "ifft_ten = np.stack([ifft_ten,t_step]).T\n",
    "df = pd.DataFrame(ifft_ten, columns = ['Frequecy_Value','Time_Step'])\n",
    "fig = px.line(df,x='Time_Step', y='Frequecy_Value', title='Dowsampling using Fast Fourrier Transform (FFT)')\n",
    "run.log({'Down_Sampled_Sound':fig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc2fa951",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b41bd360>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "340af87a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-paper-170</strong> at: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a><br/>Synced 6 W&B file(s), 5 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230129_201107-w0790zli/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a95b3c3127493aae61702742650226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01675289374992038, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230129_201126-w0790zli</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">vivid-paper-170</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()\n",
    "run = wandb.init(id=run.id,entity='tiny-ml',project = 'wake_word_detection', group='Data',\n",
    "                resume='must')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a04b458",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ARM spectrogram using FFT 🦾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "605df5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5     , 0.3125  , 0.125   , ..., 0.      , 0.      , 0.      ],\n",
       "       [0.671875, 0.375   , 0.0625  , ..., 0.015625, 0.      , 0.      ],\n",
       "       [0.78125 , 0.390625, 0.03125 , ..., 0.      , 0.      , 0.      ],\n",
       "       ...,\n",
       "       [1.109375, 0.53125 , 0.125   , ..., 0.015625, 0.      , 0.      ],\n",
       "       [0.96875 , 0.375   , 0.03125 , ..., 0.015625, 0.      , 0.      ],\n",
       "       [0.921875, 0.34375 , 0.0625  , ..., 0.      , 0.      , 0.      ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'yes'\n",
    "idx = 10\n",
    "sound = read_wav(f'./data/{category}/{category}_record {idx}.wav')\n",
    "sound = sound.astype(np.float32, order='F') / 32768.0\n",
    "get_arm_spectrogram = Arm_spect().get_arm_spectrogram\n",
    "get_arm_spectrogram(sound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79663ee8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Showing output of custom function optimized for arm processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "953d6351",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0 ... -591 -643 -631]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/ipykernel_40417/3992365391.py:8: ComplexWarning:\n",
      "\n",
      "Casting complex values to real discards the imaginary part\n",
      "\n"
     ]
    }
   ],
   "source": [
    "category = 'yes'\n",
    "sound = read_wav(f'./data/{category}/yes_record 119.wav')\n",
    "print(sound)\n",
    "sound = sound.astype(np.float32, order='F') / 32768.0\n",
    "# what we are using \n",
    "arm_spct = get_arm_spectrogram(sound)\n",
    "# what were not using \n",
    "tf_spect = tf.signal.stft(sound, frame_length=512, frame_step=128).numpy().astype(np.float32)\n",
    "fig = px.imshow(np.array(arm_spct))\n",
    "run.log({f'spectrogram_{category}':fig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "442d339f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2bab73bb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719bd660",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adding spectrogram plot to wandb table 🌌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb1fb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# logging some media outside of a table to workspace\n",
    "This is for:\n",
    "- cheching what our network see in signal domain;\n",
    "- to spot patterns;\n",
    "- gain intuitions about process and raw signal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cb23d2f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;255;3;255m█████████████████████████████████████████████\u001b[0m| 9/9 [00:02<00:00,  3.02it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 16000\n",
    "category = 'yes'\n",
    "n_samples = 10\n",
    "for idx in tqdm(range(1,n_samples), colour=next(tqdm_colours)):\n",
    "    sound = read_wav(f'./data/{category}/{category}_record {idx}.wav')\n",
    "    sound = sound.astype(np.float32, order='F') / 32768.0\n",
    "    arm_spect = get_arm_spectrogram(sound)\n",
    "    img = plot_spectrogram(arm_spect,sample_rate=sample_rate)\n",
    "    img = wandb.Image(img)\n",
    "    run.log({f'spectrogram_image{category}':img})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e78af2b",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2bab73bb0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e330931e",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#  !mkdir datasets\n",
    "#  !wget 'https://github.com/karoldvl/ESC-50/archive/master.zip' -P ~/pico/audio-classifier/datasets\n",
    "#  !unzip -q ~/datasets/master.zip -d ./datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff62315",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This Part covers pre training our model on ESC 50 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed107e5e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Reading our pre-traning data to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4af1b618",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:w0790zli) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-paper-170</strong> at: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a><br/>Synced 3 W&B file(s), 10 media file(s), 3 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230129_201126-w0790zli/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:w0790zli). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b256ebcdfe4a7fa7a701c4957910a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016753113199956714, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230129_201150-w0790zli</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">vivid-paper-170</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(id=run.id,\n",
    "                 entity='tiny-ml',\n",
    "                 project = 'wake_word_detection', \n",
    "                 group='Data',\n",
    "                 resume='must')\n",
    "esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'\n",
    "df = pd.read_csv(esc50_csv)\n",
    "initial_data_table = wandb.Table(data=df, columns=list(df.columns))\n",
    "run.log({'initial_table':initial_data_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09381de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# building a wandb Artifact for the generic dataset for pre training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0108d099",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./datasets)... Done. 1.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x2baa12a40>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "artifact = wandb.Artifact(type='sound_data',\n",
    "                          name='ESC-50-master')\n",
    "artifact.add_dir('./datasets/')\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc78f8b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Here we are training on 1 second sound clips and our ESC 50 Dataset is 4 seconds per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9018f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f85a58fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### testing our function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc2b25bb",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_file = 'datasets/ESC-50-master/audio/1-100032-A-0.wav'\n",
    "data = segment(fid=test_file,chunk=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34bed9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### reading 1 second clips writing to a new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ea9518a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12000it [00:10, 1101.45it/s]\n"
     ]
    }
   ],
   "source": [
    "in_paranet = Path('./datasets/ESC-50-master/audio/')\n",
    "out_data_dir = Path('ESC-50')\n",
    "out_data_dir.mkdir(exist_ok=True)\n",
    "all_out = [ ]\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "    out_dir = out_data_dir/row.category\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    data, vals = segment(fid = str(in_paranet/row.filename),\n",
    "       chunk=1)\n",
    "    chans, samp_width , rate = vals\n",
    "    out_fids = [ ]\n",
    "    for idx,sound in enumerate(data):\n",
    "        out_fid = out_dir/f'{idx}_{row.filename}'\n",
    "        out_fids.append(out_fid)\n",
    "        with wave.open(str(out_fid), 'w') as outfile:\n",
    "            outfile.setnchannels(chans)\n",
    "            outfile.setsampwidth(samp_width)\n",
    "            outfile.setframerate(rate)\n",
    "            outfile.setnframes(int(len(sound) /  samp_width))\n",
    "            outfile.writeframes(sound)\n",
    "    all_out.append(out_fids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b34806",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building an Artifact of 1 second sound clips broken down into class/category name\n",
    "This:\n",
    "> - creates a type which is here a parent directory\n",
    "> - names an artfact for each class\n",
    "> - creates a list of artfacts which are then itterated through to add a dirctory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8aaf6c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Our artifacts are going to follow this pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826071d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4080ab6d",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:w0790zli) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da31b4c25f424195b99d02f891a42ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='2.439 MB of 2.439 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-paper-170</strong> at: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a><br/>Synced 3 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230129_201150-w0790zli/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:w0790zli). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1aff9669a9449599b09913b7f57292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016753990966632652, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230129_201218-w0790zli</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">vivid-paper-170</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[38;2;252;3;252m                                                    \u001b[0m| 0/15 [00:00<?, ?it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/meta_data)... Done. 0.0s\n",
      "  7%|\u001b[38;2;252;3;252m██▉                                         \u001b[0m| 1/15 [00:00<00:02,  4.81it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/rooster)... Done. 0.1s\n",
      " 13%|\u001b[38;2;252;3;252m█████▊                                      \u001b[0m| 2/15 [00:00<00:03,  3.97it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/washing_machine)... Done. 0.1s\n",
      " 20%|\u001b[38;2;252;3;252m████████▊                                   \u001b[0m| 3/15 [00:00<00:03,  3.46it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/crickets)... Done. 0.3s\n",
      " 27%|\u001b[38;2;252;3;252m███████████▋                                \u001b[0m| 4/15 [00:01<00:04,  2.68it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/frog)... Done. 0.1s\n",
      " 33%|\u001b[38;2;252;3;252m██████████████▋                             \u001b[0m| 5/15 [00:01<00:03,  2.85it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/coughing)... Done. 0.2s\n",
      " 40%|\u001b[38;2;252;3;252m█████████████████▌                          \u001b[0m| 6/15 [00:02<00:03,  2.73it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/hand_saw)... Done. 0.1s\n",
      " 47%|\u001b[38;2;252;3;252m████████████████████▌                       \u001b[0m| 7/15 [00:02<00:02,  2.87it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/door_wood_creaks)... Done. 0.1s\n",
      " 53%|\u001b[38;2;252;3;252m███████████████████████▍                    \u001b[0m| 8/15 [00:02<00:02,  2.95it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/clock_alarm)... Done. 0.1s\n",
      " 60%|\u001b[38;2;252;3;252m██████████████████████████▍                 \u001b[0m| 9/15 [00:02<00:01,  3.20it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/drinking_sipping)... Done. 0.1s\n",
      " 67%|\u001b[38;2;252;3;252m████████████████████████████▋              \u001b[0m| 10/15 [00:03<00:01,  3.36it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/door_wood_knock)... Done. 0.1s\n",
      " 73%|\u001b[38;2;252;3;252m███████████████████████████████▌           \u001b[0m| 11/15 [00:03<00:01,  3.36it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/brushing_teeth)... Done. 0.1s\n",
      " 80%|\u001b[38;2;252;3;252m██████████████████████████████████▍        \u001b[0m| 12/15 [00:03<00:00,  3.46it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/engine)... Done. 0.1s\n",
      " 87%|\u001b[38;2;252;3;252m█████████████████████████████████████▎     \u001b[0m| 13/15 [00:04<00:00,  3.53it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/thunderstorm)... Done. 0.1s\n",
      " 93%|\u001b[38;2;252;3;252m████████████████████████████████████████▏  \u001b[0m| 14/15 [00:04<00:00,  3.57it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/cow)... Done. 0.1s\n",
      "100%|\u001b[38;2;252;3;252m███████████████████████████████████████████\u001b[0m| 15/15 [00:04<00:00,  3.10it/s]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-paper-170</strong> at: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a><br/>Synced 3 W&B file(s), 0 media file(s), 2643 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230129_201218-w0790zli/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shutil\n",
    "run = wandb.init(id=run.id,entity='tiny-ml',project = 'wake_word_detection', group='Data',\n",
    "                resume='must')\n",
    "\n",
    "esc50_csv = Path('./datasets/ESC-50-master/meta/esc50.csv')\n",
    "path = Path('ESC-50/')\n",
    "meta_data = path/'meta_data'\n",
    "meta_data.mkdir(exist_ok=True)\n",
    "shutil.copyfile(esc50_csv,meta_data/esc50_csv.name)\n",
    "esc_artifacts = [pth for pth in path.iterdir()]\n",
    "for path in tqdm(esc_artifacts[-15:],**tqdm_args,colour=next(tqdm_colours)):\n",
    "    log_wandb_artifact(run=run,path=path)\n",
    "    \n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a544c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### adding our one second sound file paths to our data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49b5e83a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230129_201528-w0790zli</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">vivid-paper-170</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/meta_data)... Done. 0.0s\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(id=run.id,entity='tiny-ml',project = 'wake_word_detection', group='Data',\n",
    "                resume='must')\n",
    "df = pd.read_csv(esc50_csv)\n",
    "esc50_csv = meta_data/esc50_csv.name\n",
    "sec_files = np.array(all_out).astype(str)\n",
    "for files in range(sec_files.shape[-1]):\n",
    "    df[f'{files}_{files+1}_sec']=sec_files[...,files].astype(str)\n",
    "df.to_csv(esc50_csv)\n",
    "log_wandb_artifact(run=run,path=esc50_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6406535a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b770b130>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c02048d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_table = wandb.Table(data=df,columns=list(df.columns))\n",
    "run.log({'meta_data_without_media':meta_data_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4c383",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Data table for sound Wandb table with \n",
    "\n",
    "> 1. ouriginal sound string\n",
    "> 2. playable sound file\n",
    "> 3. 4 * 1 second clips files\n",
    "> 4  4 * 1 second clips files\n",
    "\n",
    "# Sound and Spectrogram Table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47880190",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adding Sound and Spectrograms to WandB Table  🔉 🪄 🐝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67d34470",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:w0790zli) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-paper-170</strong> at: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a><br/>Synced 3 W&B file(s), 1 media file(s), 5 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230129_201528-w0790zli/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:w0790zli). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b540a1a529e4e51aa63ddca7b77a8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016754026383326466, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230129_201542-w0790zli</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">vivid-paper-170</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4_5_sec: 100%|████████████████████████████████| 10/10 [00:00<00:00, 3143.68it/s]\n",
      "5_6_sec: 100%|████████████████████████████████| 10/10 [00:00<00:00, 2604.83it/s]\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(id=run.id,entity='tiny-ml',project = 'wake_word_detection', group='Data',\n",
    "                resume='must')\n",
    "n_examples = 2\n",
    "n_samples = 10\n",
    "s_df = df.sample(n=n_samples, random_state=2)\n",
    "columns = list(df.columns)\n",
    "table_with_media = wandb.Table(data=s_df,columns=columns)\n",
    "for colmn in df.columns[-n_examples:]:\n",
    "    # create wandb sound objects\n",
    "    sounds = [wandb.Audio(fid) for fid in tqdm(s_df[colmn].values,**tqdm_args,desc=colmn)]\n",
    "    # add these as a column to our wandb table\n",
    "    table_with_media.add_column(name=f'sound_{colmn}',data=sounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36010449",
   "metadata": {},
   "source": [
    "### Adding Spectrograms to wandb Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4bd84c4",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4_5_sec: 100%|██████████████████████████████████| 10/10 [00:01<00:00,  6.59it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:07<00:00,  1.29it/s]\n",
      "5_6_sec: 100%|██████████████████████████████████| 10/10 [00:01<00:00,  7.15it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:07<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns[-n_examples:]:\n",
    "    # read wavs \n",
    "    wavs = map(read_wav,tqdm(s_df[col].values,**tqdm_args,desc=col))\n",
    "    # generate spectrograms\n",
    "    spects = map(get_arm_spectrogram,wavs)\n",
    "    sample_rates = iter(np.full((1,len(df)),1600)[0])\n",
    "    # get numpy arrays of save .jpg files\n",
    "    ims = list(map(plot_spectrogram,spects,sample_rates))\n",
    "    # create a list of wandb images\n",
    "    spects = list(map(wandb.Image,tqdm(ims)))\n",
    "    # add these to a table \n",
    "    table_with_media.add_column(name=f'image_{col}',data=spects)\n",
    "run.log({'sound_spectrogram_table':table_with_media})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521245b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stack our pandas df by 1 second Files 🧱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12952f33",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/meta_data)... Done. 0.0s\n"
     ]
    }
   ],
   "source": [
    "rp_df = pd.DataFrame(np.repeat(df.values, 6, axis=0))\n",
    "rp_df.columns=df.columns\n",
    "a,b,c,d,e,f = np.array([df[col].values for col in df.columns[-6:]])\n",
    "all_files =np.vstack((a,b,c,d,e,f)).reshape((-1,),order='F')\n",
    "rp_df = rp_df[df.columns[:-6]]\n",
    "rp_df['all_files']=all_files\n",
    "df = rp_df\n",
    "df.to_csv(esc50_csv)\n",
    "log_wandb_artifact(run,esc50_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c82182",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define Test,Train, Val using pandas & add sets column to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e14e26c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/meta_data)... Done. 0.0s\n"
     ]
    }
   ],
   "source": [
    "sets = { }\n",
    "train_val = df.sample(frac=0.9,random_state=42)\n",
    "sets['test']=df.drop(train_val.index)\n",
    "sets['train']=train_val.sample(frac=0.9,random_state=42)\n",
    "sets['val']=train_val.drop(sets['train'].index)\n",
    "for name,subset in sets.items():\n",
    "        subset = subset.reset_index(drop=True)\n",
    "        subset['set']=[name for va in range(len(subset))]\n",
    "        sets[name]=subset\n",
    "df.groupby(\"target\", group_keys=False).apply(lambda x: x)\n",
    "df = pd.concat(sets.values()).reset_index(drop=True)\n",
    "df.to_csv(esc50_csv)\n",
    "log_wandb_artifact(run,esc50_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cb5024d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>esc10</th>\n",
       "      <th>src_file</th>\n",
       "      <th>take</th>\n",
       "      <th>all_files</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1-103995-A-30.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>door_wood_knock</td>\n",
       "      <td>False</td>\n",
       "      <td>103995</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/door_wood_knock/3_1-103995-A-30.wav</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1-103999-A-30.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>door_wood_knock</td>\n",
       "      <td>False</td>\n",
       "      <td>103999</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/door_wood_knock/4_1-103999-A-30.wav</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>1-115546-A-48.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>fireworks</td>\n",
       "      <td>False</td>\n",
       "      <td>115546</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/fireworks/1_1-115546-A-48.wav</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1-115920-A-22.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>clapping</td>\n",
       "      <td>False</td>\n",
       "      <td>115920</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/clapping/0_1-115920-A-22.wav</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1-115920-B-22.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>clapping</td>\n",
       "      <td>False</td>\n",
       "      <td>115920</td>\n",
       "      <td>B</td>\n",
       "      <td>ESC-50/clapping/0_1-115920-B-22.wav</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71995</th>\n",
       "      <td>7488</td>\n",
       "      <td>4-204115-A-39.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>glass_breaking</td>\n",
       "      <td>False</td>\n",
       "      <td>204115</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/glass_breaking/4_4-204115-A-39.wav</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71996</th>\n",
       "      <td>9642</td>\n",
       "      <td>2-118072-A-0.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>118072</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/dog/5_2-118072-A-0.wav</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71997</th>\n",
       "      <td>6280</td>\n",
       "      <td>4-189836-A-22.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>clapping</td>\n",
       "      <td>False</td>\n",
       "      <td>189836</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/clapping/4_4-189836-A-22.wav</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71998</th>\n",
       "      <td>1177</td>\n",
       "      <td>5-252248-A-34.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>can_opening</td>\n",
       "      <td>False</td>\n",
       "      <td>252248</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/can_opening/3_5-252248-A-34.wav</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71999</th>\n",
       "      <td>4541</td>\n",
       "      <td>4-188191-A-29.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>drinking_sipping</td>\n",
       "      <td>False</td>\n",
       "      <td>188191</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/drinking_sipping/4_4-188191-A-29.wav</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0           filename fold target          category  esc10  \\\n",
       "0              3  1-103995-A-30.wav    1     30   door_wood_knock  False   \n",
       "1              4  1-103999-A-30.wav    1     30   door_wood_knock  False   \n",
       "2              9  1-115546-A-48.wav    1     48         fireworks  False   \n",
       "3             10  1-115920-A-22.wav    1     22          clapping  False   \n",
       "4             11  1-115920-B-22.wav    1     22          clapping  False   \n",
       "...          ...                ...  ...    ...               ...    ...   \n",
       "71995       7488  4-204115-A-39.wav    4     39    glass_breaking  False   \n",
       "71996       9642   2-118072-A-0.wav    2      0               dog   True   \n",
       "71997       6280  4-189836-A-22.wav    4     22          clapping  False   \n",
       "71998       1177  5-252248-A-34.wav    5     34       can_opening  False   \n",
       "71999       4541  4-188191-A-29.wav    4     29  drinking_sipping  False   \n",
       "\n",
       "      src_file take                                    all_files   set  \n",
       "0       103995    A   ESC-50/door_wood_knock/3_1-103995-A-30.wav  test  \n",
       "1       103999    A   ESC-50/door_wood_knock/4_1-103999-A-30.wav  test  \n",
       "2       115546    A         ESC-50/fireworks/1_1-115546-A-48.wav  test  \n",
       "3       115920    A          ESC-50/clapping/0_1-115920-A-22.wav  test  \n",
       "4       115920    B          ESC-50/clapping/0_1-115920-B-22.wav  test  \n",
       "...        ...  ...                                          ...   ...  \n",
       "71995   204115    A    ESC-50/glass_breaking/4_4-204115-A-39.wav   val  \n",
       "71996   118072    A                ESC-50/dog/5_2-118072-A-0.wav   val  \n",
       "71997   189836    A          ESC-50/clapping/4_4-189836-A-22.wav   val  \n",
       "71998   252248    A       ESC-50/can_opening/3_5-252248-A-34.wav   val  \n",
       "71999   188191    A  ESC-50/drinking_sipping/4_4-188191-A-29.wav   val  \n",
       "\n",
       "[72000 rows x 10 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd012a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# remove silence by randomly sampling from non silent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2343c49",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:w0790zli) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-paper-170</strong> at: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a><br/>Synced 3 W&B file(s), 1 media file(s), 43 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230129_201542-w0790zli/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:w0790zli). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91223d95f6a64278bc20cc21939e63d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01675363611660335, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230129_201629-w0790zli</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">vivid-paper-170</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/meta_data)... Done. 0.0s\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(id=run.id,entity='tiny-ml',project = 'wake_word_detection', group='Data',\n",
    "                resume='must')\n",
    "is_sound = [not np.std(read_wav(x)) < 0.1 for x in df.all_files.values]\n",
    "df['is_sound']=is_sound\n",
    "df.to_csv(esc50_csv)\n",
    "log_wandb_artifact(run,esc50_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79ff83db",
   "metadata": {},
   "outputs": [],
   "source": [
    "silence = df[df.is_sound==False]\n",
    "final_meta_table = wandb.Table(data=silence,columns=list(df.columns))\n",
    "wandb_audio = list(map(wandb.Audio,silence.all_files.values))\n",
    "final_meta_table.add_column(name='audio',data=wandb_audio)\n",
    "run.log({'silence_tabel':final_meta_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bf86940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b823f0d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5cc303",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building a wandb Artifact for our pre processed data (numpy array spectrograms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1be18f6e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:w0790zli) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "wandb: Network error (TransientError), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-paper-170</strong> at: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a><br/>Synced 3 W&B file(s), 1 media file(s), 632 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230129_201629-w0790zli/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:w0790zli). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480a610c33594cfaa474b014a8428d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016754083333338107, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230129_201648-w0790zli</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">vivid-paper-170</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/fridadesigley/pico/tiny-ml/processed/train.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/train_data.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/test.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/val.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/train_aug.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x2c3449e40>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = wandb.init(id=run.id,entity='tiny-ml',project = 'wake_word_detection', group='Data',\n",
    "                resume='must')\n",
    "artifact = wandb.Artifact(type='pre_processed_sound_data',name='npz-esc-50-files')\n",
    "path = Path('./processed/')\n",
    "# create references for our large pre processed npz files\n",
    "for fid_path in path.iterdir():\n",
    "    print(f'file://{fid_path.resolve()}')  \n",
    "    artifact.add_reference(f'file://{fid_path.resolve()}')\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "852643f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n",
      "alredy deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[38;2;247;3;247m                                                    \u001b[0m| 0/51 [00:00<?, ?it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/cat)... Done. 0.1s\n",
      "  2%|\u001b[38;2;247;3;247m▊                                           \u001b[0m| 1/51 [00:00<00:13,  3.84it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/chainsaw)... Done. 0.1s\n",
      "  4%|\u001b[38;2;247;3;247m█▋                                          \u001b[0m| 2/51 [00:00<00:14,  3.42it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/siren)... Done. 0.1s\n",
      "  6%|\u001b[38;2;247;3;247m██▌                                         \u001b[0m| 3/51 [00:00<00:13,  3.66it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/clock_tick)... Done. 0.1s\n",
      "  8%|\u001b[38;2;247;3;247m███▍                                        \u001b[0m| 4/51 [00:01<00:12,  3.70it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/can_opening)... Done. 0.1s\n",
      " 10%|\u001b[38;2;247;3;247m████▎                                       \u001b[0m| 5/51 [00:01<00:12,  3.80it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/sneezing)... Done. 0.1s\n",
      " 12%|\u001b[38;2;247;3;247m█████▏                                      \u001b[0m| 6/51 [00:01<00:11,  3.92it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/pig)... Done. 0.1s\n",
      " 14%|\u001b[38;2;247;3;247m██████                                      \u001b[0m| 7/51 [00:01<00:11,  3.74it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/pouring_water)... Done. 0.1s\n",
      " 16%|\u001b[38;2;247;3;247m██████▉                                     \u001b[0m| 8/51 [00:02<00:11,  3.73it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/rain)... Done. 0.1s\n",
      " 18%|\u001b[38;2;247;3;247m███████▊                                    \u001b[0m| 9/51 [00:02<00:11,  3.69it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/church_bells)... Done. 0.1s\n",
      " 20%|\u001b[38;2;247;3;247m████████▍                                  \u001b[0m| 10/51 [00:02<00:11,  3.67it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/mouse_click)... Done. 0.1s\n",
      " 22%|\u001b[38;2;247;3;247m█████████▎                                 \u001b[0m| 11/51 [00:02<00:11,  3.56it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/toilet_flush)... Done. 0.1s\n",
      " 24%|\u001b[38;2;247;3;247m██████████                                 \u001b[0m| 12/51 [00:03<00:10,  3.56it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/car_horn)... Done. 0.1s\n",
      " 25%|\u001b[38;2;247;3;247m██████████▉                                \u001b[0m| 13/51 [00:03<00:10,  3.70it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/dog)... Done. 0.1s\n",
      " 27%|\u001b[38;2;247;3;247m███████████▊                               \u001b[0m| 14/51 [00:03<00:10,  3.43it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/vacuum_cleaner)... Done. 0.1s\n",
      " 29%|\u001b[38;2;247;3;247m████████████▋                              \u001b[0m| 15/51 [00:04<00:11,  3.02it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/chirping_birds)... Done. 0.1s\n",
      " 31%|\u001b[38;2;247;3;247m█████████████▍                             \u001b[0m| 16/51 [00:04<00:10,  3.21it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/crackling_fire)... Done. 0.1s\n",
      " 33%|\u001b[38;2;247;3;247m██████████████▎                            \u001b[0m| 17/51 [00:04<00:10,  3.26it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/breathing)... Done. 0.1s\n",
      " 35%|\u001b[38;2;247;3;247m███████████████▏                           \u001b[0m| 18/51 [00:05<00:09,  3.42it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/crying_baby)... Done. 0.1s\n",
      " 37%|\u001b[38;2;247;3;247m████████████████                           \u001b[0m| 19/51 [00:05<00:09,  3.53it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/sheep)... Done. 0.1s\n",
      " 39%|\u001b[38;2;247;3;247m████████████████▊                          \u001b[0m| 20/51 [00:05<00:08,  3.58it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/glass_breaking)... Done. 0.0s\n",
      " 41%|\u001b[38;2;247;3;247m█████████████████▋                         \u001b[0m| 21/51 [00:05<00:07,  3.81it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/fireworks)... Done. 0.1s\n",
      " 43%|\u001b[38;2;247;3;247m██████████████████▌                        \u001b[0m| 22/51 [00:06<00:07,  3.69it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/crow)... Done. 0.1s\n",
      " 45%|\u001b[38;2;247;3;247m███████████████████▍                       \u001b[0m| 23/51 [00:06<00:07,  3.67it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/wind)... Done. 0.1s\n",
      " 47%|\u001b[38;2;247;3;247m████████████████████▏                      \u001b[0m| 24/51 [00:06<00:07,  3.54it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/helicopter)... Done. 0.1s\n",
      " 49%|\u001b[38;2;247;3;247m█████████████████████                      \u001b[0m| 25/51 [00:06<00:07,  3.61it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/keyboard_typing)... Done. 0.1s\n",
      " 51%|\u001b[38;2;247;3;247m█████████████████████▉                     \u001b[0m| 26/51 [00:07<00:06,  3.70it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/snoring)... Done. 0.1s\n",
      " 53%|\u001b[38;2;247;3;247m██████████████████████▊                    \u001b[0m| 27/51 [00:07<00:06,  3.51it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/laughing)... Done. 0.1s\n",
      " 55%|\u001b[38;2;247;3;247m███████████████████████▌                   \u001b[0m| 28/51 [00:07<00:06,  3.60it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/clapping)... Done. 0.1s\n",
      " 57%|\u001b[38;2;247;3;247m████████████████████████▍                  \u001b[0m| 29/51 [00:08<00:05,  3.69it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/hen)... Done. 0.1s\n",
      " 59%|\u001b[38;2;247;3;247m█████████████████████████▎                 \u001b[0m| 30/51 [00:08<00:05,  3.79it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/airplane)... Done. 0.1s\n",
      " 61%|\u001b[38;2;247;3;247m██████████████████████████▏                \u001b[0m| 31/51 [00:08<00:05,  3.63it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/sea_waves)... Done. 0.1s\n",
      " 63%|\u001b[38;2;247;3;247m██████████████████████████▉                \u001b[0m| 32/51 [00:08<00:05,  3.66it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/footsteps)... Done. 0.1s\n",
      " 65%|\u001b[38;2;247;3;247m███████████████████████████▊               \u001b[0m| 33/51 [00:09<00:04,  3.65it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/insects)... Done. 0.1s\n",
      " 67%|\u001b[38;2;247;3;247m████████████████████████████▋              \u001b[0m| 34/51 [00:09<00:04,  3.51it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/train)... Done. 0.1s\n",
      " 69%|\u001b[38;2;247;3;247m█████████████████████████████▌             \u001b[0m| 35/51 [00:09<00:04,  3.57it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/water_drops)... Done. 0.1s\n",
      " 71%|\u001b[38;2;247;3;247m██████████████████████████████▎            \u001b[0m| 36/51 [00:10<00:04,  3.64it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/meta_data)... Done. 0.0s\n",
      " 73%|\u001b[38;2;247;3;247m███████████████████████████████▏           \u001b[0m| 37/51 [00:10<00:03,  3.93it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/rooster)... Done. 0.1s\n",
      " 75%|\u001b[38;2;247;3;247m████████████████████████████████           \u001b[0m| 38/51 [00:10<00:03,  3.90it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/washing_machine)... Done. 0.1s\n",
      " 76%|\u001b[38;2;247;3;247m████████████████████████████████▉          \u001b[0m| 39/51 [00:10<00:03,  3.38it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/crickets)... Done. 0.1s\n",
      " 78%|\u001b[38;2;247;3;247m█████████████████████████████████▋         \u001b[0m| 40/51 [00:11<00:03,  3.51it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/frog)... Done. 0.1s\n",
      " 80%|\u001b[38;2;247;3;247m██████████████████████████████████▌        \u001b[0m| 41/51 [00:11<00:02,  3.43it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/coughing)... Done. 0.1s\n",
      " 82%|\u001b[38;2;247;3;247m███████████████████████████████████▍       \u001b[0m| 42/51 [00:11<00:02,  3.59it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/hand_saw)... Done. 0.1s\n",
      " 84%|\u001b[38;2;247;3;247m████████████████████████████████████▎      \u001b[0m| 43/51 [00:11<00:02,  3.54it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/door_wood_creaks)... Done. 0.1s\n",
      " 86%|\u001b[38;2;247;3;247m█████████████████████████████████████      \u001b[0m| 44/51 [00:12<00:02,  3.42it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/clock_alarm)... Done. 0.1s\n",
      " 88%|\u001b[38;2;247;3;247m█████████████████████████████████████▉     \u001b[0m| 45/51 [00:12<00:01,  3.40it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/drinking_sipping)... Done. 0.1s\n",
      " 90%|\u001b[38;2;247;3;247m██████████████████████████████████████▊    \u001b[0m| 46/51 [00:12<00:01,  3.53it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/door_wood_knock)... Done. 0.1s\n",
      " 92%|\u001b[38;2;247;3;247m███████████████████████████████████████▋   \u001b[0m| 47/51 [00:13<00:01,  3.66it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/brushing_teeth)... Done. 0.1s\n",
      " 94%|\u001b[38;2;247;3;247m████████████████████████████████████████▍  \u001b[0m| 48/51 [00:13<00:00,  3.66it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/engine)... Done. 0.1s\n",
      " 96%|\u001b[38;2;247;3;247m█████████████████████████████████████████▎ \u001b[0m| 49/51 [00:13<00:00,  3.70it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/thunderstorm)... Done. 0.1s\n",
      " 98%|\u001b[38;2;247;3;247m██████████████████████████████████████████▏\u001b[0m| 50/51 [00:13<00:00,  3.63it/s]\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./ESC-50/cow)... Done. 0.1s\n",
      "100%|\u001b[38;2;247;3;247m███████████████████████████████████████████\u001b[0m| 51/51 [00:14<00:00,  3.59it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for pth in df[df.is_sound==False].all_files.values:\n",
    "    path = Path(pth)\n",
    "    try:\n",
    "        path.unlink()\n",
    "    except FileNotFoundError:\n",
    "for path in tqdm(esc_artifacts,**tqdm_args,colour=next(tqdm_colours)):\n",
    "    log_wandb_artifact(run=run,path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09631f66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Save 3D arrays of Spectrograms using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "25907320",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def save_npz(x,y,name,augmenter=None):\n",
    "    wavs = list(map(read_wav,tqdm(x,f'creating {name}')))\n",
    "    if augmenter:\n",
    "        samples = np.full((1,len(y)), 16000)[0]\n",
    "        wavs = list(map(augmenter,wavs,tqdm(samples,f'augmenting {name}')))\n",
    "        x_data = np.array(list(map(get_arm_spectrogram,tqdm(wavs,f'creating {name} augspects'))))\n",
    "    else:\n",
    "        x_data = np.array(list(map(get_arm_spectrogram,tqdm(wavs,f'creating {name} spects'))))\n",
    "    for array in x_data:\n",
    "        assert array.shape==(682, 257)\n",
    "    np.savez(f'{name}.npz',x_data=x_data,y_data=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d785ac4",
   "metadata": {},
   "source": [
    "## Loop over files and save as npz, this is our pre-processed training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6068b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_sound = df[df.is_sound==True]\n",
    "subset =  contains_sound[contains_sound.set=='train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865c649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating processed/test: 100%|███████████| 6834/6834 [00:00<00:00, 18598.10it/s]\n",
      "creating processed/test spects: 100%|██████| 6834/6834 [00:51<00:00, 133.74it/s]\n",
      "creating processed/train: 100%|█████████| 55257/55257 [00:05<00:00, 9992.27it/s]\n",
      "creating processed/train spects: 100%|███| 55257/55257 [07:23<00:00, 124.57it/s]\n"
     ]
    }
   ],
   "source": [
    "pre_process=True\n",
    "contains_sound = df[df.is_sound==True]\n",
    "if pre_process:\n",
    "    for name in df.set.unique():\n",
    "        subset =  contains_sound[contains_sound.set==name]\n",
    "        x_data = subset['all_files'].values\n",
    "        y_data = subset['target'].values\n",
    "        save_npz(x_data,y_data,f'processed/{name}')\n",
    "        if name=='train':\n",
    "            print('augmenting')\n",
    "            save_npz(x_data[:len(x_data)//5],y_data[:len(y_data)//5],f'processed/{name}_aug',augmenter=augmenter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7637c344",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building a wandb Artifact for our pre processed data (numpy array spectrograms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "617cf77a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/fridadesigley/pico/tiny-ml/processed/train.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/train_data.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/test.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/val.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/train_aug.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x2c447e110>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact = wandb.Artifact(type='pre_processed_sound_data',name='npz-esc-50-files')\n",
    "path = Path('./processed/')\n",
    "# create references for our large pre processed npz files\n",
    "for fid_path in path.iterdir():\n",
    "    print(f'file://{fid_path.resolve()}')  \n",
    "    artifact.add_reference(f'file://{fid_path.resolve()}')\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f758cf5",
   "metadata": {},
   "source": [
    "### To finish our data run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "657f0228",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SockSrvRdThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/wandb/sdk/service/server_sock.py\", line 112, in run\n",
      "    shandler(sreq)\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/wandb/sdk/service/server_sock.py\", line 165, in server_record_communicate\n",
      "    iface = self._mux.get_stream(stream_id).interface\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/wandb/sdk/service/streams.py\", line 206, in get_stream\n",
      "    stream = self._streams[stream_id]\n",
      "KeyError: 'w0790zli'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/w0790zli?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2c33c1ed0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.finish()\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097b848",
   "metadata": {},
   "source": [
    "# Part 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ece38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training load training dataset which has already been pre processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "feaccb8e",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with np.load('processed/train.npz',allow_pickle=True) as data:\n",
    "    train_x = data['x_data'].astype(np.float32)\n",
    "    train_y = data['y_data'].astype(np.uint8)\n",
    "with np.load('processed/train_aug.npz',allow_pickle=True) as data:\n",
    "    aug_x = data['x_data'].astype(np.float32)\n",
    "    aug_y = data['y_data'].astype(np.uint8)\n",
    "with np.load('processed/test.npz',allow_pickle=True) as data:\n",
    "    test_x = data['x_data'].astype(np.float32)\n",
    "    test_y = data['y_data'].astype(np.uint8)\n",
    "with np.load('processed/val.npz',allow_pickle=True) as data:\n",
    "    val_x = data['x_data'].astype(np.float32)\n",
    "    val_y = data['y_data'].astype(np.uint8)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4832936a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd03eb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## create TensoFlow Dataset from numpy arrays of spectrograms and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "50c89b6f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(train_x, tf.float32), train_y))\n",
    "train_aug_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(aug_x, tf.float32), aug_y))\n",
    "train_dataset = train_dataset.concatenate(train_aug_dataset)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(val_x, tf.float32),val_y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(test_x,tf.float32),test_y))\n",
    "\n",
    "train_ds = train_dataset.cache().shuffle(10000, seed=42).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_dataset.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_dataset.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8fc40f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 21:37:29.267569: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(682, 257), dtype=float32, numpy=\n",
       " array([[61.296875, 33.65625 ,  3.59375 , ...,  1.0625  ,  1.546875,\n",
       "          0.34375 ],\n",
       "        [63.8125  , 37.5     ,  7.84375 , ...,  1.046875,  1.484375,\n",
       "          0.734375],\n",
       "        [59.984375, 36.3125  , 14.375   , ...,  1.59375 ,  1.53125 ,\n",
       "          0.390625],\n",
       "        ...,\n",
       "        [39.84375 , 38.78125 , 15.5625  , ...,  0.796875,  0.765625,\n",
       "          0.46875 ],\n",
       "        [25.90625 , 33.734375, 26.21875 , ...,  1.09375 ,  1.375   ,\n",
       "          1.109375],\n",
       "        [14.640625, 18.078125, 42.578125, ...,  1.453125,  1.765625,\n",
       "          1.484375]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=uint8, numpy=43>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_ds))\n",
    "x[0],y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f197571",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test that this dataset is loading okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cc95bb2d",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " target = 10, \n",
      " spectrogram = \n",
      " [[14.6875   10.984375  8.125    ...  2.484375  2.765625  4.21875 ]\n",
      " [ 7.09375  10.1875   10.484375 ...  2.140625  1.921875  4.453125]\n",
      " [ 0.609375  9.265625 14.96875  ...  1.671875  1.859375  4.484375]\n",
      " ...\n",
      " [ 2.265625  4.453125  3.75     ...  5.6875    4.59375   0.40625 ]\n",
      " [ 1.1875    5.765625  6.4375   ...  5.3125    4.03125   2.484375]\n",
      " [ 4.453125  9.6875    7.171875 ...  3.203125  3.375     2.734375]]\n",
      "(682, 257, 1)\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(train_dataset.take(1)))\n",
    "print(f' target = {y}, \\n spectrogram = \\n {x}')\n",
    "input_shape = tf.expand_dims(x, axis=-1).shape\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e29a130a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[14.6875   10.984375  8.125    ...  2.484375  2.765625  4.21875 ]\n",
      " [ 7.09375  10.1875   10.484375 ...  2.140625  1.921875  4.453125]\n",
      " [ 0.609375  9.265625 14.96875  ...  1.671875  1.859375  4.484375]\n",
      " ...\n",
      " [ 2.265625  4.453125  3.75     ...  5.6875    4.59375   0.40625 ]\n",
      " [ 1.1875    5.765625  6.4375   ...  5.3125    4.03125   2.484375]\n",
      " [ 4.453125  9.6875    7.171875 ...  3.203125  3.375     2.734375]], shape=(682, 257), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a2159c4e",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "norm_layer.adapt(train_dataset.map(lambda x, y: tf.reshape(x, input_shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e68f71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# we have made an application and embeded wandb in the recording process\n",
    "this is to allow us to :\n",
    "> - capture new data from you device 🔊\n",
    "> - Version data using wandb Atefact 🏺\n",
    "> - Define your wakework 📢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "01e1429d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T10:47:21.788567Z",
     "start_time": "2023-01-28T10:47:21.587446Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdata\u001b[0m\r\n",
      "├── \u001b[01;34mbackground\u001b[0m\r\n",
      "├── \u001b[01;34mno\u001b[0m\r\n",
      "└── \u001b[01;34myes\u001b[0m\r\n",
      "\r\n",
      "3 directories\r\n"
     ]
    }
   ],
   "source": [
    "!tree data -d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c87b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create a new training run for our training 🏃🏻‍♀️ ♀️ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d733c973",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c43e1aafb8043f3a5ba07d34845087e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01673398819996995, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(config={\"bs\": 12},entity='tiny-ml',project = 'wake_word_detection', group='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00822475",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define our model 🏗️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "60cc0681",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "baseline_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Input(shape=input_shape),\n",
    "  tf.keras.layers.experimental.preprocessing.Resizing(32, 32, interpolation=\"nearest\"), \n",
    "  norm_layer,\n",
    "  tf.keras.layers.Conv2D(8, kernel_size=(8,8), strides=(2, 2), activation=\"relu\"),\n",
    "  tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dropout(0.25),\n",
    "  tf.keras.layers.Dense(50, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c5401882",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing_6 (Resizing)       (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " normalization_6 (Normalizat  (None, 32, 32, 1)        3         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 13, 13, 8)         520       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 6, 6, 8)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 288)               0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 288)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 50)                14450     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,973\n",
      "Trainable params: 14,970\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c008e0d1",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "METRICS = [\"accuracy\",]\n",
    "baseline_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=METRICS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "69ce5024",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    ''' a function to increase lr at start of trining\n",
    "    '''\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        # add somthing like np.linespace([0,-0.1])\n",
    "        return lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6418c1f4",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "# Initialize a new W&B run \n",
    "checkpoint_path = \"training_1/\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# Create a callback that saves the model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "04840fce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fc7a0b1e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(verbose=0, patience=25), \n",
    "    tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    ",cp_callback,WandbMetricsLogger(),WandbModelCheckpoint(checkpoint_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ffa1efd6",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/ex78039g?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x5f56f32e0>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "09c56d39",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "361/365 [============================>.] - ETA: 0s - loss: 3.4901 - accuracy: 0.1098\n",
      "Epoch 1: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 5s 12ms/step - loss: 3.4873 - accuracy: 0.1103 - val_loss: 3.1569 - val_accuracy: 0.1898 - lr: 0.0030\n",
      "Epoch 2/250\n",
      "361/365 [============================>.] - ETA: 0s - loss: 3.1311 - accuracy: 0.1892\n",
      "Epoch 2: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 3.1319 - accuracy: 0.1889 - val_loss: 2.9955 - val_accuracy: 0.2241 - lr: 0.0030\n",
      "Epoch 3/250\n",
      "355/365 [============================>.] - ETA: 0s - loss: 3.0014 - accuracy: 0.2175\n",
      "Epoch 3: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 3.0016 - accuracy: 0.2176 - val_loss: 2.9545 - val_accuracy: 0.2111 - lr: 0.0030\n",
      "Epoch 4/250\n",
      "359/365 [============================>.] - ETA: 0s - loss: 2.9265 - accuracy: 0.2339\n",
      "Epoch 4: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 7ms/step - loss: 2.9258 - accuracy: 0.2335 - val_loss: 2.9169 - val_accuracy: 0.2185 - lr: 0.0030\n",
      "Epoch 5/250\n",
      "356/365 [============================>.] - ETA: 0s - loss: 2.8675 - accuracy: 0.2444\n",
      "Epoch 5: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.8716 - accuracy: 0.2436 - val_loss: 2.9022 - val_accuracy: 0.2389 - lr: 0.0030\n",
      "Epoch 6/250\n",
      "355/365 [============================>.] - ETA: 0s - loss: 2.8204 - accuracy: 0.2621\n",
      "Epoch 6: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.8240 - accuracy: 0.2609 - val_loss: 2.9131 - val_accuracy: 0.2231 - lr: 0.0030\n",
      "Epoch 7/250\n",
      "359/365 [============================>.] - ETA: 0s - loss: 2.7854 - accuracy: 0.2676\n",
      "Epoch 7: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.7864 - accuracy: 0.2672 - val_loss: 2.8791 - val_accuracy: 0.2250 - lr: 0.0030\n",
      "Epoch 8/250\n",
      "354/365 [============================>.] - ETA: 0s - loss: 2.7573 - accuracy: 0.2730\n",
      "Epoch 8: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.7562 - accuracy: 0.2727 - val_loss: 2.8809 - val_accuracy: 0.2454 - lr: 0.0030\n",
      "Epoch 9/250\n",
      "351/365 [===========================>..] - ETA: 0s - loss: 2.7468 - accuracy: 0.2692\n",
      "Epoch 9: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 3s 9ms/step - loss: 2.7492 - accuracy: 0.2683 - val_loss: 2.8750 - val_accuracy: 0.2417 - lr: 0.0030\n",
      "Epoch 10/250\n",
      "358/365 [============================>.] - ETA: 0s - loss: 2.7280 - accuracy: 0.2810\n",
      "Epoch 10: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 3s 8ms/step - loss: 2.7285 - accuracy: 0.2814 - val_loss: 2.9062 - val_accuracy: 0.2463 - lr: 0.0030\n",
      "Epoch 11/250\n",
      "362/365 [============================>.] - ETA: 0s - loss: 2.6883 - accuracy: 0.2869\n",
      "Epoch 11: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.6872 - accuracy: 0.2876 - val_loss: 2.8924 - val_accuracy: 0.2500 - lr: 0.0027\n",
      "Epoch 12/250\n",
      "364/365 [============================>.] - ETA: 0s - loss: 2.6653 - accuracy: 0.2900\n",
      "Epoch 12: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 3s 7ms/step - loss: 2.6659 - accuracy: 0.2899 - val_loss: 2.8984 - val_accuracy: 0.2315 - lr: 0.0025\n",
      "Epoch 13/250\n",
      "359/365 [============================>.] - ETA: 0s - loss: 2.6474 - accuracy: 0.2966\n",
      "Epoch 13: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.6494 - accuracy: 0.2966 - val_loss: 2.8844 - val_accuracy: 0.2537 - lr: 0.0022\n",
      "Epoch 14/250\n",
      "358/365 [============================>.] - ETA: 0s - loss: 2.6226 - accuracy: 0.2971\n",
      "Epoch 14: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 3s 7ms/step - loss: 2.6239 - accuracy: 0.2970 - val_loss: 2.8872 - val_accuracy: 0.2556 - lr: 0.0020\n",
      "Epoch 15/250\n",
      "363/365 [============================>.] - ETA: 0s - loss: 2.6037 - accuracy: 0.3045\n",
      "Epoch 15: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.6038 - accuracy: 0.3044 - val_loss: 2.9127 - val_accuracy: 0.2361 - lr: 0.0018\n",
      "Epoch 16/250\n",
      "363/365 [============================>.] - ETA: 0s - loss: 2.5794 - accuracy: 0.3125\n",
      "Epoch 16: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.5785 - accuracy: 0.3125 - val_loss: 2.8810 - val_accuracy: 0.2491 - lr: 0.0016\n",
      "Epoch 17/250\n",
      "359/365 [============================>.] - ETA: 0s - loss: 2.5649 - accuracy: 0.3134\n",
      "Epoch 17: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.5628 - accuracy: 0.3131 - val_loss: 2.8698 - val_accuracy: 0.2481 - lr: 0.0015\n",
      "Epoch 18/250\n",
      "362/365 [============================>.] - ETA: 0s - loss: 2.5780 - accuracy: 0.3066\n",
      "Epoch 18: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 7ms/step - loss: 2.5792 - accuracy: 0.3066 - val_loss: 2.8820 - val_accuracy: 0.2417 - lr: 0.0013\n",
      "Epoch 19/250\n",
      "357/365 [============================>.] - ETA: 0s - loss: 2.5502 - accuracy: 0.3192\n",
      "Epoch 19: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 7ms/step - loss: 2.5481 - accuracy: 0.3196 - val_loss: 2.8615 - val_accuracy: 0.2389 - lr: 0.0012\n",
      "Epoch 20/250\n",
      "356/365 [============================>.] - ETA: 0s - loss: 2.5298 - accuracy: 0.3234\n",
      "Epoch 20: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.5328 - accuracy: 0.3225 - val_loss: 2.8773 - val_accuracy: 0.2435 - lr: 0.0011\n",
      "Epoch 21/250\n",
      "350/365 [===========================>..] - ETA: 0s - loss: 2.5348 - accuracy: 0.3190\n",
      "Epoch 21: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.5362 - accuracy: 0.3182 - val_loss: 2.8770 - val_accuracy: 0.2528 - lr: 9.9861e-04\n",
      "Epoch 22/250\n",
      "354/365 [============================>.] - ETA: 0s - loss: 2.5119 - accuracy: 0.3279\n",
      "Epoch 22: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 7ms/step - loss: 2.5106 - accuracy: 0.3273 - val_loss: 2.8672 - val_accuracy: 0.2537 - lr: 9.0358e-04\n",
      "Epoch 23/250\n",
      "354/365 [============================>.] - ETA: 0s - loss: 2.5045 - accuracy: 0.3262\n",
      "Epoch 23: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.5056 - accuracy: 0.3256 - val_loss: 2.8586 - val_accuracy: 0.2491 - lr: 8.1760e-04\n",
      "Epoch 24/250\n",
      "351/365 [===========================>..] - ETA: 0s - loss: 2.4993 - accuracy: 0.3305\n",
      "Epoch 24: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 3s 8ms/step - loss: 2.5033 - accuracy: 0.3289 - val_loss: 2.8786 - val_accuracy: 0.2528 - lr: 7.3979e-04\n",
      "Epoch 25/250\n",
      "356/365 [============================>.] - ETA: 0s - loss: 2.4929 - accuracy: 0.3315\n",
      "Epoch 25: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 3s 9ms/step - loss: 2.4886 - accuracy: 0.3316 - val_loss: 2.8707 - val_accuracy: 0.2574 - lr: 6.6939e-04\n",
      "Epoch 26/250\n",
      "353/365 [============================>.] - ETA: 0s - loss: 2.4841 - accuracy: 0.3290\n",
      "Epoch 26: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4820 - accuracy: 0.3290 - val_loss: 2.8492 - val_accuracy: 0.2528 - lr: 6.0569e-04\n",
      "Epoch 27/250\n",
      "362/365 [============================>.] - ETA: 0s - loss: 2.4752 - accuracy: 0.3314\n",
      "Epoch 27: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 7ms/step - loss: 2.4759 - accuracy: 0.3311 - val_loss: 2.8856 - val_accuracy: 0.2556 - lr: 5.4805e-04\n",
      "Epoch 28/250\n",
      "350/365 [===========================>..] - ETA: 0s - loss: 2.4678 - accuracy: 0.3333\n",
      "Epoch 28: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4715 - accuracy: 0.3325 - val_loss: 2.8709 - val_accuracy: 0.2565 - lr: 4.9590e-04\n",
      "Epoch 29/250\n",
      "349/365 [===========================>..] - ETA: 0s - loss: 2.4795 - accuracy: 0.3293\n",
      "Epoch 29: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4781 - accuracy: 0.3297 - val_loss: 2.8597 - val_accuracy: 0.2574 - lr: 4.4871e-04\n",
      "Epoch 30/250\n",
      "365/365 [==============================] - ETA: 0s - loss: 2.4680 - accuracy: 0.3344\n",
      "Epoch 30: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4680 - accuracy: 0.3344 - val_loss: 2.8667 - val_accuracy: 0.2565 - lr: 4.0601e-04\n",
      "Epoch 31/250\n",
      "365/365 [==============================] - ETA: 0s - loss: 2.4701 - accuracy: 0.3326\n",
      "Epoch 31: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4701 - accuracy: 0.3326 - val_loss: 2.8628 - val_accuracy: 0.2593 - lr: 3.6737e-04\n",
      "Epoch 32/250\n",
      "351/365 [===========================>..] - ETA: 0s - loss: 2.4746 - accuracy: 0.3317\n",
      "Epoch 32: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 7ms/step - loss: 2.4692 - accuracy: 0.3333 - val_loss: 2.8696 - val_accuracy: 0.2593 - lr: 3.3241e-04\n",
      "Epoch 33/250\n",
      "363/365 [============================>.] - ETA: 0s - loss: 2.4581 - accuracy: 0.3385\n",
      "Epoch 33: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4575 - accuracy: 0.3387 - val_loss: 2.8709 - val_accuracy: 0.2593 - lr: 3.0078e-04\n",
      "Epoch 34/250\n",
      "353/365 [============================>.] - ETA: 0s - loss: 2.4567 - accuracy: 0.3372\n",
      "Epoch 34: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 7ms/step - loss: 2.4561 - accuracy: 0.3374 - val_loss: 2.8658 - val_accuracy: 0.2574 - lr: 2.7215e-04\n",
      "Epoch 35/250\n",
      "350/365 [===========================>..] - ETA: 0s - loss: 2.4470 - accuracy: 0.3395\n",
      "Epoch 35: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4516 - accuracy: 0.3392 - val_loss: 2.8639 - val_accuracy: 0.2546 - lr: 2.4626e-04\n",
      "Epoch 36/250\n",
      "362/365 [============================>.] - ETA: 0s - loss: 2.4594 - accuracy: 0.3383\n",
      "Epoch 36: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4615 - accuracy: 0.3376 - val_loss: 2.8659 - val_accuracy: 0.2611 - lr: 2.2282e-04\n",
      "Epoch 37/250\n",
      "356/365 [============================>.] - ETA: 0s - loss: 2.4380 - accuracy: 0.3387\n",
      "Epoch 37: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 7ms/step - loss: 2.4405 - accuracy: 0.3386 - val_loss: 2.8647 - val_accuracy: 0.2574 - lr: 2.0162e-04\n",
      "Epoch 38/250\n",
      "354/365 [============================>.] - ETA: 0s - loss: 2.4453 - accuracy: 0.3383\n",
      "Epoch 38: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4411 - accuracy: 0.3400 - val_loss: 2.8672 - val_accuracy: 0.2574 - lr: 1.8243e-04\n",
      "Epoch 39/250\n",
      "362/365 [============================>.] - ETA: 0s - loss: 2.4499 - accuracy: 0.3400\n",
      "Epoch 39: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 7ms/step - loss: 2.4501 - accuracy: 0.3401 - val_loss: 2.8699 - val_accuracy: 0.2657 - lr: 1.6507e-04\n",
      "Epoch 40/250\n",
      "349/365 [===========================>..] - ETA: 0s - loss: 2.4422 - accuracy: 0.3424\n",
      "Epoch 40: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4382 - accuracy: 0.3430 - val_loss: 2.8671 - val_accuracy: 0.2574 - lr: 1.4936e-04\n",
      "Epoch 41/250\n",
      "349/365 [===========================>..] - ETA: 0s - loss: 2.4381 - accuracy: 0.3407\n",
      "Epoch 41: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4376 - accuracy: 0.3410 - val_loss: 2.8639 - val_accuracy: 0.2611 - lr: 1.3515e-04\n",
      "Epoch 42/250\n",
      "364/365 [============================>.] - ETA: 0s - loss: 2.4426 - accuracy: 0.3379\n",
      "Epoch 42: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 3s 8ms/step - loss: 2.4445 - accuracy: 0.3376 - val_loss: 2.8690 - val_accuracy: 0.2639 - lr: 1.2229e-04\n",
      "Epoch 43/250\n",
      "365/365 [==============================] - ETA: 0s - loss: 2.4370 - accuracy: 0.3416\n",
      "Epoch 43: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4370 - accuracy: 0.3416 - val_loss: 2.8655 - val_accuracy: 0.2620 - lr: 1.1065e-04\n",
      "Epoch 44/250\n",
      "362/365 [============================>.] - ETA: 0s - loss: 2.4429 - accuracy: 0.3377\n",
      "Epoch 44: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 5ms/step - loss: 2.4437 - accuracy: 0.3375 - val_loss: 2.8641 - val_accuracy: 0.2611 - lr: 1.0012e-04\n",
      "Epoch 45/250\n",
      "357/365 [============================>.] - ETA: 0s - loss: 2.4311 - accuracy: 0.3469\n",
      "Epoch 45: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4305 - accuracy: 0.3469 - val_loss: 2.8623 - val_accuracy: 0.2602 - lr: 9.0592e-05\n",
      "Epoch 46/250\n",
      "352/365 [===========================>..] - ETA: 0s - loss: 2.4338 - accuracy: 0.3453\n",
      "Epoch 46: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4371 - accuracy: 0.3450 - val_loss: 2.8669 - val_accuracy: 0.2593 - lr: 8.1971e-05\n",
      "Epoch 47/250\n",
      "362/365 [============================>.] - ETA: 0s - loss: 2.4433 - accuracy: 0.3432\n",
      "Epoch 47: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 6s 16ms/step - loss: 2.4431 - accuracy: 0.3432 - val_loss: 2.8701 - val_accuracy: 0.2583 - lr: 7.4171e-05\n",
      "Epoch 48/250\n",
      "363/365 [============================>.] - ETA: 0s - loss: 2.4303 - accuracy: 0.3454\n",
      "Epoch 48: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4293 - accuracy: 0.3457 - val_loss: 2.8657 - val_accuracy: 0.2611 - lr: 6.7112e-05\n",
      "Epoch 49/250\n",
      "352/365 [===========================>..] - ETA: 0s - loss: 2.4287 - accuracy: 0.3462\n",
      "Epoch 49: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4307 - accuracy: 0.3459 - val_loss: 2.8660 - val_accuracy: 0.2583 - lr: 6.0726e-05\n",
      "Epoch 50/250\n",
      "354/365 [============================>.] - ETA: 0s - loss: 2.4354 - accuracy: 0.3439\n",
      "Epoch 50: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4324 - accuracy: 0.3437 - val_loss: 2.8650 - val_accuracy: 0.2583 - lr: 5.4947e-05\n",
      "Epoch 51/250\n",
      "359/365 [============================>.] - ETA: 0s - loss: 2.4274 - accuracy: 0.3453\n",
      "Epoch 51: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "365/365 [==============================] - 2s 6ms/step - loss: 2.4270 - accuracy: 0.3455 - val_loss: 2.8636 - val_accuracy: 0.2602 - lr: 4.9718e-05\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 250\n",
    "history = baseline_model.fit(\n",
    "     train_ds, \n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9b6b1a4d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b6c064",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluate our locally saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2483cbb9",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 21:42:26.478528: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open training_1/.: FAILED_PRECONDITION: training_1; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2b41db970>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint('training_1')\n",
    "baseline_model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec365219",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Evaluate our Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "638700bf",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 - 5s - loss: 2.4582 - accuracy: 0.3518 - 5s/epoch - 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.4581873416900635, 0.3517705500125885]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.evaluate(test_ds, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed189bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Dowload model artifact/ our registered model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c0444ac4",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fb60627ca17c932d33d95875ef1c3b14 ['latest', 'epoch_50']\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "artifact = api.artifact(f'tiny-ml/wake_word_detection/run_{run.id}_model:v50', type='model')\n",
    "print(artifact.digest,artifact.aliases)\n",
    "file = artifact.download()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4217e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Load our pre trained tiny model from wandb artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "28eada02",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 - 2s - loss: 2.5362 - accuracy: 0.3197 - 2s/epoch - 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5361759662628174, 0.3197249174118042]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.load_weights(f'{file}/cp.ckpt')\n",
    "baseline_model.evaluate(test_ds, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4a280760",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 11.08it/s]\n",
      "100%|████████████████████████████████████| 410/410 [00:00<00:00, 2425479.04it/s]\n",
      "100%|████████████████████████████████████████| 500/500 [00:01<00:00, 455.76it/s]\n",
      "100%|████████████████████████████████████| 152/152 [00:00<00:00, 1880631.88it/s]\n",
      "100%|████████████████████████████████████████| 500/500 [00:01<00:00, 385.55it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 16000\n",
    "get_arm_spectrogram = Arm_spect().get_arm_spectrogram\n",
    "path = Path('./data/') \n",
    "paths = {pth.name:list(map(str,filter(lambda x:x.name != '.gitkeep',pth.iterdir()))) \n",
    "         for pth in path.iterdir() if pth.name in ['background','yes']}\n",
    "paths = {k:np.stack(list(filter(lambda x:len(x)==16384,map(read_wav,tqdm(v))))) \n",
    "         for (k,v) in tqdm(paths.items())}\n",
    "\n",
    "for k,v in paths.items():\n",
    "    filler_x = [augmenter(x,sample_rate) \n",
    "                for x in v[np.random.choice(v.shape[0], 500-len(v), replace=True)]]\n",
    "    filler_x = np.stack(list(map(lambda x:x[:16384], tqdm(filler_x))))\n",
    "    paths[k]=np.array(list(map(get_arm_spectrogram,tqdm(np.vstack([v,filler_x]))))).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ae8548cc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([array([[[1.07015625e+02, 6.92031250e+01, 1.55312500e+01, ...,\n",
       "         1.71875000e-01, 1.56250000e-01, 1.56250000e-01],\n",
       "        [1.22921875e+02, 6.72656250e+01, 4.59375000e+00, ...,\n",
       "         6.25000000e-02, 6.25000000e-02, 7.81250000e-02],\n",
       "        [1.27843750e+02, 6.41562500e+01, 1.71875000e-01, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.56250000e-02],\n",
       "        ...,\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[1.07015625e+02, 6.92031250e+01, 1.55312500e+01, ...,\n",
       "         1.71875000e-01, 1.56250000e-01, 1.56250000e-01],\n",
       "        [1.22921875e+02, 6.72656250e+01, 4.59375000e+00, ...,\n",
       "         6.25000000e-02, 6.25000000e-02, 7.81250000e-02],\n",
       "        [1.27843750e+02, 6.41562500e+01, 1.71875000e-01, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.56250000e-02],\n",
       "        ...,\n",
       "        [1.20250000e+02, 6.59843750e+01, 6.90625000e+00, ...,\n",
       "         2.34375000e-01, 9.37500000e-02, 3.12500000e-02],\n",
       "        [1.11859375e+02, 5.81406250e+01, 1.35000000e+01, ...,\n",
       "         4.37500000e-01, 1.56250000e-01, 1.56250000e-02],\n",
       "        [1.05609375e+02, 4.52812500e+01, 1.74062500e+01, ...,\n",
       "         8.12500000e-01, 4.37500000e-01, 2.81250000e-01]],\n",
       "\n",
       "       [[1.06062500e+02, 6.84843750e+01, 1.47656250e+01, ...,\n",
       "         9.53125000e-01, 8.43750000e-01, 7.81250000e-01],\n",
       "        [1.22250000e+02, 6.70625000e+01, 4.28125000e+00, ...,\n",
       "         6.71875000e-01, 6.25000000e-01, 5.93750000e-01],\n",
       "        [1.27546875e+02, 6.42812500e+01, 3.28125000e-01, ...,\n",
       "         2.96875000e-01, 2.96875000e-01, 2.81250000e-01],\n",
       "        ...,\n",
       "        [1.25968750e+02, 6.32343750e+01, 2.06250000e+00, ...,\n",
       "         6.71875000e-01, 6.71875000e-01, 6.71875000e-01],\n",
       "        [1.26984375e+02, 6.44531250e+01, 7.81250000e-01, ...,\n",
       "         3.43750000e-01, 2.96875000e-01, 2.50000000e-01],\n",
       "        [1.26968750e+02, 6.44218750e+01, 8.43750000e-01, ...,\n",
       "         3.43750000e-01, 3.28125000e-01, 2.81250000e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [1.27156250e+02, 6.38125000e+01, 7.65625000e-01, ...,\n",
       "         3.75000000e-01, 3.59375000e-01, 3.59375000e-01],\n",
       "        [1.26687500e+02, 6.29218750e+01, 1.18750000e+00, ...,\n",
       "         5.00000000e-01, 4.84375000e-01, 4.68750000e-01],\n",
       "        [1.26562500e+02, 6.25625000e+01, 1.29687500e+00, ...,\n",
       "         4.68750000e-01, 4.68750000e-01, 4.53125000e-01]],\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[1.25500000e+02, 6.39687500e+01, 1.53125000e+00, ...,\n",
       "         7.18750000e-01, 5.93750000e-01, 5.00000000e-01],\n",
       "        [1.27250000e+02, 6.43593750e+01, 7.81250000e-01, ...,\n",
       "         2.65625000e-01, 2.50000000e-01, 2.50000000e-01],\n",
       "        [1.27968750e+02, 6.40312500e+01, 3.12500000e-02, ...,\n",
       "         1.56250000e-02, 1.56250000e-02, 1.56250000e-02],\n",
       "        ...,\n",
       "        [1.27062500e+02, 6.40156250e+01, 9.68750000e-01, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.26359375e+02, 6.28593750e+01, 1.67187500e+00, ...,\n",
       "         1.56250000e-02, 1.56250000e-02, 0.00000000e+00],\n",
       "        [1.26015625e+02, 6.19843750e+01, 1.96875000e+00, ...,\n",
       "         3.12500000e-02, 0.00000000e+00, 0.00000000e+00]]], dtype=float32), array([[[  0.      ,   0.      ,   0.      , ...,   0.      ,\n",
       "           0.      ,   0.      ],\n",
       "        [  0.      ,   0.      ,   0.      , ...,   0.      ,\n",
       "           0.      ,   0.      ],\n",
       "        [  0.      ,   0.      ,   0.      , ...,   0.      ,\n",
       "           0.      ,   0.      ],\n",
       "        ...,\n",
       "        [  0.      ,   0.      ,   0.      , ...,   0.      ,\n",
       "           0.      ,   0.      ],\n",
       "        [  0.      ,   0.      ,   0.      , ...,   0.      ,\n",
       "           0.      ,   0.      ],\n",
       "        [  0.      ,   0.      ,   0.      , ...,   0.      ,\n",
       "           0.      ,   0.      ]],\n",
       "\n",
       "       [[ 13.859375,  19.390625,  27.765625, ...,   1.0625  ,\n",
       "           1.6875  ,   1.15625 ],\n",
       "        [ 15.34375 ,  17.28125 ,  32.453125, ...,   1.28125 ,\n",
       "           1.734375,   0.171875],\n",
       "        [ 12.59375 ,   9.03125 ,  30.90625 , ...,   1.859375,\n",
       "           1.625   ,   0.375   ],\n",
       "        ...,\n",
       "        [113.71875 ,  58.890625,   2.484375, ...,   1.5     ,\n",
       "           1.59375 ,   1.234375],\n",
       "        [116.046875,  58.84375 ,   2.      , ...,   1.640625,\n",
       "           1.453125,   1.171875],\n",
       "        [116.875   ,  58.34375 ,   1.375   , ...,   1.515625,\n",
       "           0.890625,   0.796875]],\n",
       "\n",
       "       [[ 89.96875 ,  64.421875,  22.359375, ...,   2.203125,\n",
       "           1.890625,   2.09375 ],\n",
       "        [ 87.4375  ,  57.375   ,  12.875   , ...,   3.25    ,\n",
       "           2.859375,   2.546875],\n",
       "        [ 69.59375 ,  42.9375  ,  10.78125 , ...,   3.640625,\n",
       "           3.53125 ,   1.609375],\n",
       "        ...,\n",
       "        [ 99.0625  ,  49.671875,   2.40625 , ...,   2.578125,\n",
       "           3.421875,   2.484375],\n",
       "        [ 99.78125 ,  47.8125  ,   1.328125, ...,   1.203125,\n",
       "           3.25    ,   2.328125],\n",
       "        [101.09375 ,  49.765625,   3.09375 , ...,   1.625   ,\n",
       "           3.09375 ,   0.5625  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 82.5     ,  43.390625,   1.78125 , ...,   6.234375,\n",
       "           8.703125,   7.515625],\n",
       "        [ 94.171875,  47.0625  ,   3.453125, ...,   0.953125,\n",
       "           5.578125,   8.34375 ],\n",
       "        [104.578125,  53.875   ,   5.515625, ...,   2.296875,\n",
       "           4.03125 ,   5.1875  ],\n",
       "        ...,\n",
       "        [ 48.359375,  19.578125,   4.34375 , ...,   2.9375  ,\n",
       "           6.25    ,  10.5     ],\n",
       "        [ 56.125   ,  26.953125,   7.65625 , ...,   4.109375,\n",
       "           6.34375 ,   8.25    ],\n",
       "        [ 67.828125,  37.21875 ,   7.640625, ...,   4.0625  ,\n",
       "           4.921875,   5.578125]],\n",
       "\n",
       "       [[  9.078125,  10.03125 ,  20.53125 , ...,   1.5625  ,\n",
       "           2.015625,   1.359375],\n",
       "        [  1.15625 ,   8.203125,  16.09375 , ...,   2.1875  ,\n",
       "           1.84375 ,   0.59375 ],\n",
       "        [ 11.375   ,  11.90625 ,  17.96875 , ...,   2.625   ,\n",
       "           1.703125,   0.46875 ],\n",
       "        ...,\n",
       "        [  8.859375,  12.65625 ,  47.1875  , ...,   1.265625,\n",
       "           1.71875 ,   2.      ],\n",
       "        [ 16.078125,   2.109375,  44.      , ...,   1.59375 ,\n",
       "           1.09375 ,   1.453125],\n",
       "        [ 16.078125,  11.34375 ,  42.1875  , ...,   1.59375 ,\n",
       "           0.265625,   1.203125]],\n",
       "\n",
       "       [[ 23.125   ,   5.453125,   6.640625, ...,   2.109375,\n",
       "           2.671875,   2.703125],\n",
       "        [ 18.5625  ,   8.78125 ,   6.40625 , ...,   3.90625 ,\n",
       "           3.296875,   0.640625],\n",
       "        [ 22.703125,  12.984375,   7.734375, ...,   4.859375,\n",
       "           3.46875 ,   1.296875],\n",
       "        ...,\n",
       "        [115.28125 ,  55.5625  ,   8.71875 , ...,   2.328125,\n",
       "           2.53125 ,   2.640625],\n",
       "        [117.359375,  64.25    ,   6.203125, ...,   1.5625  ,\n",
       "           1.609375,   1.703125],\n",
       "        [116.453125,  64.546875,   9.28125 , ...,   0.515625,\n",
       "           0.546875,   0.796875]]], dtype=float32)])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ee8cdee4",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y = np.concatenate([np.zeros((1,500)).astype(np.uint16)[0], np.ones((1,500)).astype(np.uint16)[0]])\n",
    "x = np.vstack(list(paths.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "80441b99",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_wake_data = tf.data.Dataset.from_tensor_slices((tf.cast(x,tf.float32), y))\n",
    "train_wake_loader = train_wake_data.cache().shuffle(2000, seed=42).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3ea53ccf",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " target = 0, \n",
      " spectrogram = \n",
      " [[1.07015625e+02 6.92031250e+01 1.55312500e+01 ... 1.71875000e-01\n",
      "  1.56250000e-01 1.56250000e-01]\n",
      " [1.22921875e+02 6.72656250e+01 4.59375000e+00 ... 6.25000000e-02\n",
      "  6.25000000e-02 7.81250000e-02]\n",
      " [1.27843750e+02 6.41562500e+01 1.71875000e-01 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.56250000e-02]\n",
      " ...\n",
      " [1.28031250e+02 6.39843750e+01 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.28031250e+02 6.39843750e+01 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.28031250e+02 6.39843750e+01 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(train_wake_data.take(1)))\n",
    "print(f' target = {y}, \\n spectrogram = \\n {x}')\n",
    "input_shape = tf.expand_dims(x, axis=-1).shape\n",
    "norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "norm_layer.adapt(train_wake_data.map(lambda x, y: tf.reshape(x, input_shape)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "539a8df6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249, 257, 1)\n"
     ]
    }
   ],
   "source": [
    "input_shape = tf.expand_dims(x, axis=-1).shape\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "be5316e1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Input(shape=input_shape),\n",
    "  tf.keras.layers.experimental.preprocessing.Resizing(32, 32, interpolation=\"nearest\"), \n",
    "  norm_layer,\n",
    "  tf.keras.layers.Conv2D(8, kernel_size=(8,8), strides=(2, 2), activation=\"relu\"),\n",
    "  tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dropout(0.25),\n",
    "  tf.keras.layers.Dense(50, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "47f5e99a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2b3fb4910>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.load_weights('./training_1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "860affaf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 249, 257, 1)]     0         \n",
      "                                                                 \n",
      " resizing_7 (Resizing)       (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " normalization_7 (Normalizat  (None, 32, 32, 1)        3         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 13, 13, 8)         520       \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 6, 6, 8)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 288)               0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 288)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 812\n",
      "Trainable params: 809\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_body = tf.keras.Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "classifier_head = tf.keras.layers.Dense(1, activation=\"sigmoid\")(model_body.output)\n",
    "wake_word_model = tf.keras.Model(model_body.input, classifier_head)\n",
    "wake_word_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8c48223b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for layer in wake_word_model.layers:\n",
    "    layer.trainable = False\n",
    "wake_word_model.layers[-1].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1d073e6d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_2\n"
     ]
    }
   ],
   "source": [
    "# Initialize a new W&B run\n",
    "run = wandb.init(config={\"bs\": 12},entity='tiny-ml',project = 'wake_word_detection', group='training',settings={'quiet':True,'silent':True,'show_warnings':False,'show_info':False})\n",
    "checkpoint_path = \"training_2/\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "print(checkpoint_dir)\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(verbose=0, patience=25), \n",
    "    tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    ",cp_callback,WandbMetricsLogger(),WandbModelCheckpoint(checkpoint_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d0615265",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wake_word_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=METRICS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f8697694",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6566 - accuracy: 0.6530WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.6566 - accuracy: 0.6530 - lr: 0.0030\n",
      "Epoch 2/100\n",
      "23/32 [====================>.........] - ETA: 0s - loss: 0.5169 - accuracy: 0.7812WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.5159 - accuracy: 0.7770 - lr: 0.0030\n",
      "Epoch 3/100\n",
      "22/32 [===================>..........] - ETA: 0s - loss: 0.4522 - accuracy: 0.8111WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 39ms/step - loss: 0.4473 - accuracy: 0.8200 - lr: 0.0030\n",
      "Epoch 4/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.4243 - accuracy: 0.8416WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.4284 - accuracy: 0.8380 - lr: 0.0030\n",
      "Epoch 5/100\n",
      "25/32 [======================>.......] - ETA: 0s - loss: 0.4068 - accuracy: 0.8450WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.4047 - accuracy: 0.8440 - lr: 0.0030\n",
      "Epoch 6/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.4131 - accuracy: 0.8498WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 47ms/step - loss: 0.4170 - accuracy: 0.8410 - lr: 0.0030\n",
      "Epoch 7/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.5567 - accuracy: 0.7812WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3917 - accuracy: 0.8480 - lr: 0.0030\n",
      "Epoch 8/100\n",
      "23/32 [====================>.........] - ETA: 0s - loss: 0.3752 - accuracy: 0.8655WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 38ms/step - loss: 0.3813 - accuracy: 0.8630 - lr: 0.0030\n",
      "Epoch 9/100\n",
      "25/32 [======================>.......] - ETA: 0s - loss: 0.3809 - accuracy: 0.8562WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3833 - accuracy: 0.8540 - lr: 0.0030\n",
      "Epoch 10/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3721 - accuracy: 0.8568WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3664 - accuracy: 0.8610 - lr: 0.0030\n",
      "Epoch 11/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3558 - accuracy: 0.8763WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 40ms/step - loss: 0.3573 - accuracy: 0.8700 - lr: 0.0027\n",
      "Epoch 12/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3631 - accuracy: 0.8579WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3636 - accuracy: 0.8580 - lr: 0.0025\n",
      "Epoch 13/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3645 - accuracy: 0.8627WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3602 - accuracy: 0.8650 - lr: 0.0022\n",
      "Epoch 14/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3542 - accuracy: 0.8685WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 39ms/step - loss: 0.3498 - accuracy: 0.8700 - lr: 0.0020\n",
      "Epoch 15/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3466 - accuracy: 0.8719WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3508 - accuracy: 0.8700 - lr: 0.0018\n",
      "Epoch 16/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3395 - accuracy: 0.8581WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3528 - accuracy: 0.8590 - lr: 0.0016\n",
      "Epoch 17/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3577 - accuracy: 0.8558WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 38ms/step - loss: 0.3562 - accuracy: 0.8570 - lr: 0.0015\n",
      "Epoch 18/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3357 - accuracy: 0.8762WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3483 - accuracy: 0.8640 - lr: 0.0013\n",
      "Epoch 19/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3405 - accuracy: 0.8656WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 28ms/step - loss: 0.3412 - accuracy: 0.8650 - lr: 0.0012\n",
      "Epoch 20/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3406 - accuracy: 0.8715WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 39ms/step - loss: 0.3470 - accuracy: 0.8670 - lr: 0.0011\n",
      "Epoch 21/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3439 - accuracy: 0.8649WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3463 - accuracy: 0.8630 - lr: 9.9861e-04\n",
      "Epoch 22/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3399 - accuracy: 0.8714WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3391 - accuracy: 0.8710 - lr: 9.0358e-04\n",
      "Epoch 23/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3469 - accuracy: 0.8646WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 37ms/step - loss: 0.3379 - accuracy: 0.8690 - lr: 8.1760e-04\n",
      "Epoch 24/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3526 - accuracy: 0.8567WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3477 - accuracy: 0.8630 - lr: 7.3979e-04\n",
      "Epoch 25/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3509 - accuracy: 0.8490WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 37ms/step - loss: 0.3430 - accuracy: 0.8590 - lr: 6.6939e-04\n",
      "Epoch 26/100\n",
      "22/32 [===================>..........] - ETA: 0s - loss: 0.3316 - accuracy: 0.8707WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3385 - accuracy: 0.8720 - lr: 6.0569e-04\n",
      "Epoch 27/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.4005 - accuracy: 0.8438WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3394 - accuracy: 0.8710 - lr: 5.4805e-04\n",
      "Epoch 28/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3288 - accuracy: 0.8763WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 38ms/step - loss: 0.3402 - accuracy: 0.8720 - lr: 4.9590e-04\n",
      "Epoch 29/100\n",
      "20/32 [=================>............] - ETA: 0s - loss: 0.3088 - accuracy: 0.8938WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3266 - accuracy: 0.8880 - lr: 4.4871e-04\n",
      "Epoch 30/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3318 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3349 - accuracy: 0.8710 - lr: 4.0601e-04\n",
      "Epoch 31/100\n",
      "23/32 [====================>.........] - ETA: 0s - loss: 0.3245 - accuracy: 0.8859WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 40ms/step - loss: 0.3337 - accuracy: 0.8800 - lr: 3.6737e-04\n",
      "Epoch 32/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3312 - accuracy: 0.8710WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3335 - accuracy: 0.8700 - lr: 3.3241e-04\n",
      "Epoch 33/100\n",
      "16/32 [==============>...............] - ETA: 0s - loss: 0.3434 - accuracy: 0.8574WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3298 - accuracy: 0.8730 - lr: 3.0078e-04\n",
      "Epoch 34/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3214 - accuracy: 0.8729WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 37ms/step - loss: 0.3199 - accuracy: 0.8740 - lr: 2.7215e-04\n",
      "Epoch 35/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3231 - accuracy: 0.8785WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3268 - accuracy: 0.8710 - lr: 2.4626e-04\n",
      "Epoch 36/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3302 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3243 - accuracy: 0.8790 - lr: 2.2282e-04\n",
      "Epoch 37/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3339 - accuracy: 0.8796WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 38ms/step - loss: 0.3257 - accuracy: 0.8820 - lr: 2.0162e-04\n",
      "Epoch 38/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3296 - accuracy: 0.8847WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3330 - accuracy: 0.8820 - lr: 1.8243e-04\n",
      "Epoch 39/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3367 - accuracy: 0.8690WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 28ms/step - loss: 0.3322 - accuracy: 0.8730 - lr: 1.6507e-04\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3359 - accuracy: 0.8650WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 36ms/step - loss: 0.3359 - accuracy: 0.8650 - lr: 1.4936e-04\n",
      "Epoch 41/100\n",
      "23/32 [====================>.........] - ETA: 0s - loss: 0.3289 - accuracy: 0.8709WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3282 - accuracy: 0.8750 - lr: 1.3515e-04\n",
      "Epoch 42/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3515 - accuracy: 0.8627WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3474 - accuracy: 0.8630 - lr: 1.2229e-04\n",
      "Epoch 43/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2205 - accuracy: 0.9688WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 37ms/step - loss: 0.3304 - accuracy: 0.8680 - lr: 1.1065e-04\n",
      "Epoch 44/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3243 - accuracy: 0.8762WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3370 - accuracy: 0.8660 - lr: 1.0012e-04\n",
      "Epoch 45/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3326 - accuracy: 0.8793WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 42ms/step - loss: 0.3350 - accuracy: 0.8780 - lr: 9.0592e-05\n",
      "Epoch 46/100\n",
      "22/32 [===================>..........] - ETA: 0s - loss: 0.3320 - accuracy: 0.8636WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3358 - accuracy: 0.8660 - lr: 8.1971e-05\n",
      "Epoch 47/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3343 - accuracy: 0.8687WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 36ms/step - loss: 0.3334 - accuracy: 0.8690 - lr: 7.4171e-05\n",
      "Epoch 48/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3156 - accuracy: 0.8894WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 37ms/step - loss: 0.3364 - accuracy: 0.8800 - lr: 6.7112e-05\n",
      "Epoch 49/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.4207 - accuracy: 0.8125WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3427 - accuracy: 0.8720 - lr: 6.0726e-05\n",
      "Epoch 50/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3292 - accuracy: 0.8679WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3295 - accuracy: 0.8680 - lr: 5.4947e-05\n",
      "Epoch 51/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3356 - accuracy: 0.8672WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 41ms/step - loss: 0.3268 - accuracy: 0.8720 - lr: 4.9718e-05\n",
      "Epoch 52/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3403 - accuracy: 0.8594WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3317 - accuracy: 0.8640 - lr: 4.4987e-05\n",
      "Epoch 53/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.3548 - accuracy: 0.8438WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3328 - accuracy: 0.8740 - lr: 4.0706e-05\n",
      "Epoch 54/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3430 - accuracy: 0.8672WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 37ms/step - loss: 0.3425 - accuracy: 0.8650 - lr: 3.6832e-05\n",
      "Epoch 55/100\n",
      "25/32 [======================>.......] - ETA: 0s - loss: 0.3363 - accuracy: 0.8600WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3342 - accuracy: 0.8660 - lr: 3.3327e-05\n",
      "Epoch 56/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3407 - accuracy: 0.8781WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3419 - accuracy: 0.8760 - lr: 3.0156e-05\n",
      "Epoch 57/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3292 - accuracy: 0.8785WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 37ms/step - loss: 0.3404 - accuracy: 0.8730 - lr: 2.7286e-05\n",
      "Epoch 58/100\n",
      "25/32 [======================>.......] - ETA: 0s - loss: 0.3311 - accuracy: 0.8737WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3240 - accuracy: 0.8780 - lr: 2.4689e-05\n",
      "Epoch 59/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3433 - accuracy: 0.8646WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3433 - accuracy: 0.8630 - lr: 2.2340e-05\n",
      "Epoch 60/100\n",
      "14/32 [============>.................] - ETA: 0s - loss: 0.3257 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 40ms/step - loss: 0.3419 - accuracy: 0.8610 - lr: 2.0214e-05\n",
      "Epoch 61/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3354 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3334 - accuracy: 0.8750 - lr: 1.8290e-05\n",
      "Epoch 62/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3428 - accuracy: 0.8635WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 35ms/step - loss: 0.3403 - accuracy: 0.8660 - lr: 1.6550e-05\n",
      "Epoch 63/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3208 - accuracy: 0.8858WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 39ms/step - loss: 0.3281 - accuracy: 0.8780 - lr: 1.4975e-05\n",
      "Epoch 64/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3441 - accuracy: 0.8708WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3493 - accuracy: 0.8660 - lr: 1.3550e-05\n",
      "Epoch 65/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3144 - accuracy: 0.8850WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 33ms/step - loss: 0.3283 - accuracy: 0.8740 - lr: 1.2260e-05\n",
      "Epoch 66/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3276 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 39ms/step - loss: 0.3242 - accuracy: 0.8780 - lr: 1.1094e-05\n",
      "Epoch 67/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3332 - accuracy: 0.8720WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 24ms/step - loss: 0.3344 - accuracy: 0.8710 - lr: 1.0038e-05\n",
      "Epoch 68/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2671 - accuracy: 0.9062WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 37ms/step - loss: 0.3123 - accuracy: 0.8780 - lr: 9.0827e-06\n",
      "Epoch 69/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3396 - accuracy: 0.8783WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 69: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3381 - accuracy: 0.8740 - lr: 8.2183e-06\n",
      "Epoch 70/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.3503 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3321 - accuracy: 0.8610 - lr: 7.4363e-06\n",
      "Epoch 71/100\n",
      "21/32 [==================>...........] - ETA: 0s - loss: 0.3264 - accuracy: 0.8780WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 52ms/step - loss: 0.3273 - accuracy: 0.8730 - lr: 6.7286e-06\n",
      "Epoch 72/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3277 - accuracy: 0.8694WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 28ms/step - loss: 0.3255 - accuracy: 0.8710 - lr: 6.0883e-06\n",
      "Epoch 73/100\n",
      "25/32 [======================>.......] - ETA: 0s - loss: 0.3368 - accuracy: 0.8662WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3320 - accuracy: 0.8730 - lr: 5.5089e-06\n",
      "Epoch 74/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3327 - accuracy: 0.8635WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 74: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 37ms/step - loss: 0.3310 - accuracy: 0.8650 - lr: 4.9847e-06\n",
      "Epoch 75/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3289 - accuracy: 0.8793WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3256 - accuracy: 0.8790 - lr: 4.5103e-06\n",
      "Epoch 76/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3377 - accuracy: 0.8690WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 29ms/step - loss: 0.3361 - accuracy: 0.8700 - lr: 4.0811e-06\n",
      "Epoch 77/100\n",
      "22/32 [===================>..........] - ETA: 0s - loss: 0.3163 - accuracy: 0.8622WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 39ms/step - loss: 0.3360 - accuracy: 0.8660 - lr: 3.6927e-06\n",
      "Epoch 78/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3339 - accuracy: 0.8687WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3348 - accuracy: 0.8680 - lr: 3.3413e-06\n",
      "Epoch 79/100\n",
      "25/32 [======================>.......] - ETA: 0s - loss: 0.3464 - accuracy: 0.8675WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3346 - accuracy: 0.8710 - lr: 3.0234e-06\n",
      "Epoch 80/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3421 - accuracy: 0.8679WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 40ms/step - loss: 0.3420 - accuracy: 0.8680 - lr: 2.7356e-06\n",
      "Epoch 81/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3298 - accuracy: 0.8672WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3320 - accuracy: 0.8670 - lr: 2.4753e-06\n",
      "Epoch 82/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3385 - accuracy: 0.8728WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3357 - accuracy: 0.8720 - lr: 2.2398e-06\n",
      "Epoch 83/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3169 - accuracy: 0.8930WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 83: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 41ms/step - loss: 0.3326 - accuracy: 0.8810 - lr: 2.0266e-06\n",
      "Epoch 84/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3244 - accuracy: 0.8773WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3309 - accuracy: 0.8730 - lr: 1.8338e-06\n",
      "Epoch 85/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3285 - accuracy: 0.8772WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 85: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3286 - accuracy: 0.8760 - lr: 1.6593e-06\n",
      "Epoch 86/100\n",
      "25/32 [======================>.......] - ETA: 0s - loss: 0.3317 - accuracy: 0.8712WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 42ms/step - loss: 0.3359 - accuracy: 0.8710 - lr: 1.5014e-06\n",
      "Epoch 87/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3221 - accuracy: 0.8831WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3308 - accuracy: 0.8780 - lr: 1.3585e-06\n",
      "Epoch 88/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.3459 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 38ms/step - loss: 0.3356 - accuracy: 0.8710 - lr: 1.2292e-06\n",
      "Epoch 89/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3375 - accuracy: 0.8681WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3331 - accuracy: 0.8730 - lr: 1.1122e-06\n",
      "Epoch 90/100\n",
      "11/32 [=========>....................] - ETA: 0s - loss: 0.3816 - accuracy: 0.8381WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 29ms/step - loss: 0.3262 - accuracy: 0.8770 - lr: 1.0064e-06\n",
      "Epoch 91/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3231 - accuracy: 0.8819WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 44ms/step - loss: 0.3293 - accuracy: 0.8730 - lr: 9.1062e-07\n",
      "Epoch 92/100\n",
      "21/32 [==================>...........] - ETA: 0s - loss: 0.3285 - accuracy: 0.8780WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3311 - accuracy: 0.8730 - lr: 8.2396e-07\n",
      "Epoch 93/100\n",
      "23/32 [====================>.........] - ETA: 0s - loss: 0.3129 - accuracy: 0.8899WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 28ms/step - loss: 0.3312 - accuracy: 0.8770 - lr: 7.4555e-07\n",
      "Epoch 94/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3413 - accuracy: 0.8646WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 38ms/step - loss: 0.3381 - accuracy: 0.8690 - lr: 6.7460e-07\n",
      "Epoch 95/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3412 - accuracy: 0.8739WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3292 - accuracy: 0.8810 - lr: 6.1041e-07\n",
      "Epoch 96/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3207 - accuracy: 0.8714WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 28ms/step - loss: 0.3150 - accuracy: 0.8730 - lr: 5.5232e-07\n",
      "Epoch 97/100\n",
      "20/32 [=================>............] - ETA: 0s - loss: 0.3303 - accuracy: 0.8734WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 39ms/step - loss: 0.3263 - accuracy: 0.8720 - lr: 4.9976e-07\n",
      "Epoch 98/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3380 - accuracy: 0.8763WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 28ms/step - loss: 0.3355 - accuracy: 0.8770 - lr: 4.5220e-07\n",
      "Epoch 99/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3552 - accuracy: 0.8620WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3343 - accuracy: 0.8740 - lr: 4.0917e-07\n",
      "Epoch 100/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3474 - accuracy: 0.8690WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.3422 - accuracy: 0.8690 - lr: 3.7023e-07\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "history = wake_word_model.fit(\n",
    "    train_wake_loader, \n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "78f7893a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for layer in wake_word_model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "38208ba5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 249, 257, 1)]     0         \n",
      "                                                                 \n",
      " resizing_7 (Resizing)       (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " normalization_7 (Normalizat  (None, 32, 32, 1)        3         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " quant_conv2d_7 (QuantizeWra  (None, 13, 13, 8)        539       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 6, 6, 8)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 288)               0         \n",
      "                                                                 \n",
      " quant_dropout_7 (QuantizeWr  (None, 288)              1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_11 (QuantizeWra  (None, 1)                294       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 837\n",
      "Trainable params: 809\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "def apply_qat_to_dense_and_cnn(layer):\n",
    "  if isinstance(layer, (tf.keras.layers.Dense, tf.keras.layers.Conv2D)):\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "  return layer\n",
    "\n",
    "annotated_model = tf.keras.models.clone_model(\n",
    "    wake_word_model,\n",
    "    clone_function=apply_qat_to_dense_and_cnn,\n",
    ")\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "51c5120e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 1s 3ms/step - loss: 0.3295 - accuracy: 0.8700\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2932 - accuracy: 0.8800\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3004 - accuracy: 0.8810\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2875 - accuracy: 0.8880\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3177 - accuracy: 0.8870\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2913 - accuracy: 0.8890\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2545 - accuracy: 0.8830\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2657 - accuracy: 0.9040\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2729 - accuracy: 0.8990\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2448 - accuracy: 0.9070\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2551 - accuracy: 0.8960\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2521 - accuracy: 0.8960\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2439 - accuracy: 0.9050\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2345 - accuracy: 0.9060\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.8990\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2560 - accuracy: 0.9040\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2104 - accuracy: 0.9220\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.9040\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2440 - accuracy: 0.9010\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2081 - accuracy: 0.9190\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2047 - accuracy: 0.9160\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2220 - accuracy: 0.9110\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.9090\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2317 - accuracy: 0.9050\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9220\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.9170\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 0.9130\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2121 - accuracy: 0.9140\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2112 - accuracy: 0.9220\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2074 - accuracy: 0.9190\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2049 - accuracy: 0.9210\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1982 - accuracy: 0.9150\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2113 - accuracy: 0.9150\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.9240\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2140 - accuracy: 0.9140\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1909 - accuracy: 0.9230\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.9180\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1968 - accuracy: 0.9200\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1919 - accuracy: 0.9260\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2012 - accuracy: 0.9200\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.9170\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1883 - accuracy: 0.9210\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1974 - accuracy: 0.9230\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.1991 - accuracy: 0.9170\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1987 - accuracy: 0.9240\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1903 - accuracy: 0.9240\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9310\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1984 - accuracy: 0.9270\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9190\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1762 - accuracy: 0.9260\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1887 - accuracy: 0.9240\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1944 - accuracy: 0.9090\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1796 - accuracy: 0.9300\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1756 - accuracy: 0.9320\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1776 - accuracy: 0.9260\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1815 - accuracy: 0.9280\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1793 - accuracy: 0.9300\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1642 - accuracy: 0.9360\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1929 - accuracy: 0.9300\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1892 - accuracy: 0.9190\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9300\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.9290\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1798 - accuracy: 0.9270\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1721 - accuracy: 0.9320\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1649 - accuracy: 0.9330\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1795 - accuracy: 0.9290\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1630 - accuracy: 0.9350\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1795 - accuracy: 0.9310\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1706 - accuracy: 0.9320\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1716 - accuracy: 0.9230\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1611 - accuracy: 0.9360\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1701 - accuracy: 0.9380\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1708 - accuracy: 0.9320\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1791 - accuracy: 0.9210\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1568 - accuracy: 0.9390\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1683 - accuracy: 0.9300\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1646 - accuracy: 0.9330\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1658 - accuracy: 0.9320\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1694 - accuracy: 0.9320\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1761 - accuracy: 0.9270\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1745 - accuracy: 0.9320\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.9330\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1619 - accuracy: 0.9320\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1634 - accuracy: 0.9350\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1693 - accuracy: 0.9330\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1794 - accuracy: 0.9300\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1690 - accuracy: 0.9350\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1643 - accuracy: 0.9350\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1597 - accuracy: 0.9320\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1420 - accuracy: 0.9420\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1490 - accuracy: 0.9430\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1544 - accuracy: 0.9390\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1467 - accuracy: 0.9460\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1689 - accuracy: 0.9330\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9310\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1574 - accuracy: 0.9340\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1712 - accuracy: 0.9300\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1386 - accuracy: 0.9430\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1566 - accuracy: 0.9360\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9320\n"
     ]
    }
   ],
   "source": [
    "quant_aware_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=METRICS,\n",
    ")\n",
    "\n",
    "EPOCHS=100\n",
    "quant_aware_history = quant_aware_model.fit(\n",
    "    train_wake_loader,  \n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b4d4d4c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, dropout_7_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/tmp4_1be5sa/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/tmp4_1be5sa/assets\n",
      "/Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-01-29 22:08:23.786690: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-01-29 22:08:23.786702: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-01-29 22:08:23.786789: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/tmp4_1be5sa\n",
      "2023-01-29 22:08:23.788279: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-01-29 22:08:23.788285: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/tmp4_1be5sa\n",
      "2023-01-29 22:08:23.792562: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-01-29 22:08:23.815996: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/tmp4_1be5sa\n",
      "2023-01-29 22:08:23.822177: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 35389 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "596831a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6016"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_models_dir = Path(\"tf_lite_model\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "tflite_model_file = tflite_models_dir/\"wake_word.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "59546720",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"alignas(8) const unsigned char tflite_model[] = {\" > tf_lite_model/wake_word.tflite.h\n",
    "cat tf_lite_model/wake_word.tflite | xxd -i                        >> tf_lite_model/wake_word.tflite.h\n",
    "echo \"};\"                                               >> tf_lite_model/wake_word.tflite.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f54793df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wake_word.tflite   wake_word.tflite.h\r\n"
     ]
    }
   ],
   "source": [
    "!ls tf_lite_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daba4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "rise": {
   "backimage": "wandb_bg.png",
   "centre": false,
   "controls": false,
   "embedded": false,
   "enable_chalkboard": false,
   "height": "99%",
   "margin": 0.09,
   "progress": true,
   "scroll": false,
   "slideNumber": false,
   "start_slideshow_at": "selected",
   "theme": "black",
   "transition": "convex",
   "width": "100%"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
