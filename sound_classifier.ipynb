{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f7d868",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/wandb/tiny-ml.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "754196eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T11:27:36.482912Z",
     "start_time": "2023-01-28T11:27:32.170223Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import wave\n",
    "from pvrecorder import PvRecorder\n",
    "import time \n",
    "import recorder\n",
    "from utils.data_processing import augmenter,log_wandb_artifact,plot_spectrogram,segment,read_wav,Arm_spect\n",
    "from utils.create_sweep import create_sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9543b088",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# some utilities that weve made in \n",
    "import recorder\n",
    "from utils.data_processing import augmenter,log_wandb_artifact,plot_spectrogram,segment,read_wav,Arm_spect\n",
    "from utils.create_sweep import create_sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d9f5e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load utils/data_processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff20e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T11:30:00.672579Z",
     "start_time": "2023-01-28T11:30:00.292579Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](slides/tiny_ml_cover.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b9b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](slides/ml_morphology.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25943d74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](slides/mlops_graph.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df11f7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](slides/edge_devices.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1d7fb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    " <tr>\n",
    "    <td> <img src=\"./slides/spark_micro_mod_pico_1.jpg\" alt=\"Drawing\" style=\"height: 100%;width:100%;\" align=\"right\"/> </td>\n",
    "    <td> <img src=\"./slides/micro_mod_pico_2.jpg\" alt=\"Drawing\" style=\"height: 60%; width:100%\" align=\"left\"/> </td>\n",
    "    </tr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da54139b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T11:27:48.771672Z",
     "start_time": "2023-01-28T11:27:46.173182Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tqdm_args = dict(leave=True,position=0)\n",
    "tqdm_colours = (f'#{r:02x}{g:02x}{b:02x}'for r,g,b in [(int(255*s), 3, int(255*s)) for s in np.linspace(1,0,100)])\n",
    "run = wandb.init(entity='tiny-ml',project = 'wake_word_detection', group='Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39dc1a45",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording\n"
     ]
    }
   ],
   "source": [
    "recorder = PvRecorder(device_index=0, frame_length=512)\n",
    "sound = []\n",
    "recorder.start()\n",
    "print('recording')\n",
    "t_0 = time.time()\n",
    "while time.time()-t_0<4:\n",
    "    frame = recorder.read()\n",
    "    sound.extend(frame)\n",
    "recorder.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f171c251",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data = np.array(sound).astype(np.int16)\n",
    "t_step = np.arange(0, len(data))\n",
    "fig = px.line(x=t_step, y=data, title='a sound wave')\n",
    "run.log({'sound_wave': fig})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0a0f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Showing a Run in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0cdd1dc",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/rqdqmyf1?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x29a9bdb10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b7237b4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/rqdqmyf1?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x29a9bdb10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './data/yes/yes_record 102.wav'\n",
    "run.log({'test_sound':wandb.Audio(path)})\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba972c38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The basis of our signal processing is using Fast Fourier Transform (FFT)\n",
    "> - [here](https://www.youtube.com/watch?v=spUNpyF58BY) is an amazing video on how FFT works. \n",
    "> - quickly go through getting from FFT to a Spectrogram. \n",
    ">  ![](./slides/200px-Fourier_in_his_coat_of_prefect.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e0272c9",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/ipykernel_7942/660930182.py:1: ComplexWarning:\n",
      "\n",
      "Casting complex values to real discards the imaginary part\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fft = np.fft.fft(data).astype(np.int16)\n",
    "t_steps = np.arange(0,len(fft))\n",
    "fft = np.stack([fft,t_step]).T\n",
    "df = pd.DataFrame(fft, columns = ['Faurrier_Value','Time_Step'])\n",
    "fig = px.line(df,x='Time_Step', y='Faurrier_Value', title='Fourrier Transform')\n",
    "run.log({'Fouriere Transform':fig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e47afaf7",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/rqdqmyf1?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x29a9bdb10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f788ee0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## FFT can also be inverted, we can get sound back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43d25b76",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/ipykernel_7942/1994990191.py:3: ComplexWarning:\n",
      "\n",
      "Casting complex values to real discards the imaginary part\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fft = np.fft.fft(data)\n",
    "roll = np.roll(fft,200)\n",
    "ifft = np.fft.ifft(roll).astype(np.int16)\n",
    "t_step = np.arange(len(ifft))\n",
    "ifft = np.stack([ifft,t_step]).T\n",
    "df = pd.DataFrame(ifft, columns = ['Frequecy_Value','Time_Step'])\n",
    "fig = px.line(df,x='Time_Step', \n",
    "              y='Frequecy_Value', \n",
    "              title='Inverse Fourrier Transform to Get sound Back')\n",
    "run.log({'Inverset Fouriere':fig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9df0e8e",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/rqdqmyf1?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x29a9bdb10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2507bf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Down Sample using FFT as an example of Signal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eab43216",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/ipykernel_7942/3482508054.py:4: ComplexWarning:\n",
      "\n",
      "Casting complex values to real discards the imaginary part\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# and examper of downsampling using FFT\n",
    "roll = np.roll(fft,15)\n",
    "ifft_ten = np.fft.ifft(roll)\n",
    "ifft_ten = ((2**(16-4)) * ifft_ten/ifft_ten.max()).astype(np.int16)\n",
    "ifft_ten = np.stack([ifft_ten,t_step]).T\n",
    "df = pd.DataFrame(ifft_ten, columns = ['Frequecy_Value','Time_Step'])\n",
    "fig = px.line(df,x='Time_Step', y='Frequecy_Value', title='Dowsampling using Fast Fourrier Transform (FFT)')\n",
    "run.log({'Down_Sampled_Sound':fig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc2fa951",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/rqdqmyf1?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x29a9bdb10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "340af87a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()\n",
    "run = wandb.init(id=run.id,\n",
    "                 entity='tiny-ml',\n",
    "                 project = 'wake_word_detection ',\n",
    "                 group='Data',\n",
    "                 resume='must')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a04b458",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ARM spectrogram using FFT ðŸ¦¾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "605df5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5     , 0.3125  , 0.125   , ..., 0.      , 0.      , 0.      ],\n",
       "       [0.671875, 0.375   , 0.0625  , ..., 0.015625, 0.      , 0.      ],\n",
       "       [0.78125 , 0.390625, 0.03125 , ..., 0.      , 0.      , 0.      ],\n",
       "       ...,\n",
       "       [1.109375, 0.53125 , 0.125   , ..., 0.015625, 0.      , 0.      ],\n",
       "       [0.96875 , 0.375   , 0.03125 , ..., 0.015625, 0.      , 0.      ],\n",
       "       [0.921875, 0.34375 , 0.0625  , ..., 0.      , 0.      , 0.      ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'yes'\n",
    "idx = 10\n",
    "sound = read_wav(f'./data/{category}/{category}_record {idx}.wav')\n",
    "sound = sound.astype(np.float32, order='F') / 32768.0\n",
    "get_arm_spectrogram = Arm_spect().get_arm_spectrogram\n",
    "get_arm_spectrogram(sound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79663ee8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Showing output of custom function optimized for arm processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "953d6351",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0 ... -591 -643 -631]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/ipykernel_7942/3992365391.py:8: ComplexWarning:\n",
      "\n",
      "Casting complex values to real discards the imaginary part\n",
      "\n"
     ]
    }
   ],
   "source": [
    "category = 'yes'\n",
    "sound = read_wav(f'./data/{category}/yes_record 119.wav')\n",
    "print(sound)\n",
    "sound = sound.astype(np.float32, order='F') / 32768.0\n",
    "# what we are using \n",
    "arm_spct = get_arm_spectrogram(sound)\n",
    "# what were not using \n",
    "tf_spect = tf.signal.stft(sound, frame_length=512, frame_step=128).numpy().astype(np.float32)\n",
    "fig = px.imshow(np.array(arm_spct))\n",
    "run.log({f'spectrogram_{category}':fig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "442d339f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719bd660",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adding spectrogram plot to wandb table ðŸŒŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb1fb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# logging some media outside of a table to workspace\n",
    "This is for:\n",
    "- cheching what our network see in signal domain;\n",
    "- to spot patterns;\n",
    "- gain intuitions about process and raw signal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cb23d2f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;252;3;252mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 9/9 [00:02<00:00,  3.69it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 16000\n",
    "category = 'yes'\n",
    "n_samples = 10\n",
    "for idx in tqdm(range(1,n_samples), colour=next(tqdm_colours)):\n",
    "    sound = read_wav(f'./data/{category}/{category}_record {idx}.wav')\n",
    "    sound = sound.astype(np.float32, order='F') / 32768.0\n",
    "    arm_spect = get_arm_spectrogram(sound)\n",
    "    img = plot_spectrogram(arm_spect,sample_rate=sample_rate)\n",
    "    img = wandb.Image(img)\n",
    "    run.log({f'spectrogram_image{category}':img})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e78af2b",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/rqdqmyf1?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x29b9a3250>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.finish()\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e330931e",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m artifact\u001b[38;5;241m.\u001b[39madd_reference(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/karoldvl/ESC-50/archive/master.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m artifact \u001b[38;5;241m=\u001b[39m run\u001b[38;5;241m.\u001b[39muse_artifact(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtiny-ml/wake_word_detection/metadata:v0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mESC-50-master\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m artifact_dir \u001b[38;5;241m=\u001b[39m \u001b[43martifact\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmkdir datasets\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munzip ./artifacts/metadata:v0/master.zip -d /Users/fridadesigley/pico/tiny-ml/datasets/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/wandb/apis/public.py:4739\u001b[0m, in \u001b[0;36mArtifact.download\u001b[0;34m(self, root, recursive)\u001b[0m\n\u001b[1;32m   4736\u001b[0m download_logger \u001b[38;5;241m=\u001b[39m _ArtifactDownloadLogger(nfiles\u001b[38;5;241m=\u001b[39mnfiles)\n\u001b[1;32m   4738\u001b[0m pool \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mdummy\u001b[38;5;241m.\u001b[39mPool(\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m-> 4739\u001b[0m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_logger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_logger\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanifest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4742\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recursive:\n\u001b[1;32m   4744\u001b[0m     pool\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m artifact: artifact\u001b[38;5;241m.\u001b[39mdownload(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dependent_artifacts)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!rm -rf datasets\n",
    "artifact = wandb.Artifact(name='metadata', type='ESC-50-master')\n",
    "artifact.add_reference('https://github.com/karoldvl/ESC-50/archive/master.zip')\n",
    "artifact = run.use_artifact('tiny-ml/wake_word_detection/metadata:v0', type='ESC-50-master')\n",
    "artifact_dir = artifact.download()\n",
    "!mkdir datasets\n",
    "!unzip ./artifacts/metadata:v0/master.zip -d /Users/fridadesigley/pico/tiny-ml/datasets/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff62315",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. This Part covers pre training our model on ESC 50 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed107e5e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Reading our pre-traning data to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4af1b618",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c3a826fbe143fbb51567da3ada428f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016751931249988652, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/ipykernel_7942/1036862475.py 1 <module>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 1108, in init\n",
      "    run = wi.init()\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 742, in init\n",
      "    raise error\n",
      "wandb.errors.CommError: Error communicating with wandb process, exiting...\n",
      "For more info see: https://docs.wandb.ai/library/init#init-start-error\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "problem",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1108\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1108\u001b[0m     run \u001b[38;5;241m=\u001b[39m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1109\u001b[0m     except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:742\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[0;32m--> 742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mCommError\u001b[0m: Error communicating with wandb process, exiting...\nFor more info see: https://docs.wandb.ai/library/init#init-start-error",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mentity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtiny-ml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwake_word_detection\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmust\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m esc50_csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./datasets/ESC-50-master/meta/esc50.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(esc50_csv)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1145\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m except_exit:\n\u001b[1;32m   1144\u001b[0m             os\u001b[38;5;241m.\u001b[39m_exit(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproblem\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror_seen\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run\n",
      "\u001b[0;31mException\u001b[0m: problem"
     ]
    }
   ],
   "source": [
    "run = wandb.init(id=run.id,\n",
    "                 entity='tiny-ml',\n",
    "                 project = 'wake_word_detection', \n",
    "                 group='Data',\n",
    "                 resume='must')\n",
    "esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'\n",
    "df = pd.read_csv(esc50_csv)\n",
    "initial_data_table = wandb.Table(data=df, columns=list(df.columns))\n",
    "run.log({'initial_table':initial_data_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc78f8b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Here we are training on 1 second sound clips and our ESC 50 Dataset is 4 seconds per class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85a58fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### testing our function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b25bb",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_file = 'datasets/ESC-50-master/audio/1-100032-A-0.wav'\n",
    "data = segment(fid=test_file,chunk=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34bed9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### reading 1 second clips writing to a new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9518a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "in_paranet = Path('./datasets/ESC-50-master/audio/')\n",
    "out_data_dir = Path('ESC-50')\n",
    "out_data_dir.mkdir(exist_ok=True)\n",
    "all_out = [ ]\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "    out_dir = out_data_dir/row.category\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    data, vals = segment(fid = str(in_paranet/row.filename),\n",
    "       chunk=1)\n",
    "    chans, samp_width , rate = vals\n",
    "    out_fids = [ ]\n",
    "    for idx,sound in enumerate(data):\n",
    "        out_fid = out_dir/f'{idx}_{row.filename}'\n",
    "        out_fids.append(out_fid)\n",
    "        with wave.open(str(out_fid), 'w') as outfile:\n",
    "            outfile.setnchannels(chans)\n",
    "            outfile.setsampwidth(samp_width)\n",
    "            outfile.setframerate(rate)\n",
    "            outfile.setnframes(int(len(sound) /  samp_width))\n",
    "            outfile.writeframes(sound)\n",
    "    all_out.append(out_fids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b34806",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building an Artifact of 1 second sound clips broken down into class/category name\n",
    "This:\n",
    "> - creates a type which is here a parent directory\n",
    "> - names an artfact for each class\n",
    "> - creates a list of artfacts which are then itterated through to add a dirctory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8aaf6c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Our artifacts are going to follow this pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4080ab6d",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;252;3;252mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 2/2 [00:00<00:00,  3.19it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "run = wandb.init(id=run.id,entity='tiny-ml',project = 'wake_word_detection', group='Data',\n",
    "                resume='must')\n",
    "\n",
    "esc50_csv = Path('./datasets/ESC-50-master/meta/esc50.csv')\n",
    "path = Path('ESC-50/')\n",
    "meta_data = path/'meta_data'\n",
    "meta_data.mkdir(exist_ok=True)\n",
    "shutil.copyfile(esc50_csv,meta_data/esc50_csv.name)\n",
    "esc_artifacts = [pth for pth in path.iterdir()]\n",
    "for pth in tqdm(esc_artifacts[-2:],**tqdm_args,colour=next(tqdm_colours)):\n",
    "    log_wandb_artifact(run=run,path=pth)\n",
    "    \n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a544c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### adding our one second sound file paths to our data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "49b5e83a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>esc10</th>\n",
       "      <th>src_file</th>\n",
       "      <th>take</th>\n",
       "      <th>0_1_sec</th>\n",
       "      <th>1_2_sec</th>\n",
       "      <th>2_3_sec</th>\n",
       "      <th>3_4_sec</th>\n",
       "      <th>4_5_sec</th>\n",
       "      <th>5_6_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/dog/0_1-100032-A-0.wav</td>\n",
       "      <td>ESC-50/dog/1_1-100032-A-0.wav</td>\n",
       "      <td>ESC-50/dog/2_1-100032-A-0.wav</td>\n",
       "      <td>ESC-50/dog/3_1-100032-A-0.wav</td>\n",
       "      <td>ESC-50/dog/4_1-100032-A-0.wav</td>\n",
       "      <td>ESC-50/dog/5_1-100032-A-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100038-A-14.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>chirping_birds</td>\n",
       "      <td>False</td>\n",
       "      <td>100038</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/chirping_birds/0_1-100038-A-14.wav</td>\n",
       "      <td>ESC-50/chirping_birds/1_1-100038-A-14.wav</td>\n",
       "      <td>ESC-50/chirping_birds/2_1-100038-A-14.wav</td>\n",
       "      <td>ESC-50/chirping_birds/3_1-100038-A-14.wav</td>\n",
       "      <td>ESC-50/chirping_birds/4_1-100038-A-14.wav</td>\n",
       "      <td>ESC-50/chirping_birds/5_1-100038-A-14.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100210-A-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/vacuum_cleaner/0_1-100210-A-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/1_1-100210-A-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/2_1-100210-A-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/3_1-100210-A-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/4_1-100210-A-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/5_1-100210-A-36.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100210-B-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>B</td>\n",
       "      <td>ESC-50/vacuum_cleaner/0_1-100210-B-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/1_1-100210-B-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/2_1-100210-B-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/3_1-100210-B-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/4_1-100210-B-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/5_1-100210-B-36.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101296-A-19.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>False</td>\n",
       "      <td>101296</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/thunderstorm/0_1-101296-A-19.wav</td>\n",
       "      <td>ESC-50/thunderstorm/1_1-101296-A-19.wav</td>\n",
       "      <td>ESC-50/thunderstorm/2_1-101296-A-19.wav</td>\n",
       "      <td>ESC-50/thunderstorm/3_1-101296-A-19.wav</td>\n",
       "      <td>ESC-50/thunderstorm/4_1-101296-A-19.wav</td>\n",
       "      <td>ESC-50/thunderstorm/5_1-101296-A-19.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>5-263831-B-6.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>hen</td>\n",
       "      <td>False</td>\n",
       "      <td>263831</td>\n",
       "      <td>B</td>\n",
       "      <td>ESC-50/hen/0_5-263831-B-6.wav</td>\n",
       "      <td>ESC-50/hen/1_5-263831-B-6.wav</td>\n",
       "      <td>ESC-50/hen/2_5-263831-B-6.wav</td>\n",
       "      <td>ESC-50/hen/3_5-263831-B-6.wav</td>\n",
       "      <td>ESC-50/hen/4_5-263831-B-6.wav</td>\n",
       "      <td>ESC-50/hen/5_5-263831-B-6.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>5-263902-A-36.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>263902</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/vacuum_cleaner/0_5-263902-A-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/1_5-263902-A-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/2_5-263902-A-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/3_5-263902-A-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/4_5-263902-A-36.wav</td>\n",
       "      <td>ESC-50/vacuum_cleaner/5_5-263902-A-36.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>5-51149-A-25.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>footsteps</td>\n",
       "      <td>False</td>\n",
       "      <td>51149</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/footsteps/0_5-51149-A-25.wav</td>\n",
       "      <td>ESC-50/footsteps/1_5-51149-A-25.wav</td>\n",
       "      <td>ESC-50/footsteps/2_5-51149-A-25.wav</td>\n",
       "      <td>ESC-50/footsteps/3_5-51149-A-25.wav</td>\n",
       "      <td>ESC-50/footsteps/4_5-51149-A-25.wav</td>\n",
       "      <td>ESC-50/footsteps/5_5-51149-A-25.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>5-61635-A-8.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>sheep</td>\n",
       "      <td>False</td>\n",
       "      <td>61635</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/sheep/0_5-61635-A-8.wav</td>\n",
       "      <td>ESC-50/sheep/1_5-61635-A-8.wav</td>\n",
       "      <td>ESC-50/sheep/2_5-61635-A-8.wav</td>\n",
       "      <td>ESC-50/sheep/3_5-61635-A-8.wav</td>\n",
       "      <td>ESC-50/sheep/4_5-61635-A-8.wav</td>\n",
       "      <td>ESC-50/sheep/5_5-61635-A-8.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>5-9032-A-0.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>9032</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/dog/0_5-9032-A-0.wav</td>\n",
       "      <td>ESC-50/dog/1_5-9032-A-0.wav</td>\n",
       "      <td>ESC-50/dog/2_5-9032-A-0.wav</td>\n",
       "      <td>ESC-50/dog/3_5-9032-A-0.wav</td>\n",
       "      <td>ESC-50/dog/4_5-9032-A-0.wav</td>\n",
       "      <td>ESC-50/dog/5_5-9032-A-0.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename  fold  target        category  esc10  src_file take  \\\n",
       "0      1-100032-A-0.wav     1       0             dog   True    100032    A   \n",
       "1     1-100038-A-14.wav     1      14  chirping_birds  False    100038    A   \n",
       "2     1-100210-A-36.wav     1      36  vacuum_cleaner  False    100210    A   \n",
       "3     1-100210-B-36.wav     1      36  vacuum_cleaner  False    100210    B   \n",
       "4     1-101296-A-19.wav     1      19    thunderstorm  False    101296    A   \n",
       "...                 ...   ...     ...             ...    ...       ...  ...   \n",
       "1995   5-263831-B-6.wav     5       6             hen  False    263831    B   \n",
       "1996  5-263902-A-36.wav     5      36  vacuum_cleaner  False    263902    A   \n",
       "1997   5-51149-A-25.wav     5      25       footsteps  False     51149    A   \n",
       "1998    5-61635-A-8.wav     5       8           sheep  False     61635    A   \n",
       "1999     5-9032-A-0.wav     5       0             dog   True      9032    A   \n",
       "\n",
       "                                        0_1_sec  \\\n",
       "0                 ESC-50/dog/0_1-100032-A-0.wav   \n",
       "1     ESC-50/chirping_birds/0_1-100038-A-14.wav   \n",
       "2     ESC-50/vacuum_cleaner/0_1-100210-A-36.wav   \n",
       "3     ESC-50/vacuum_cleaner/0_1-100210-B-36.wav   \n",
       "4       ESC-50/thunderstorm/0_1-101296-A-19.wav   \n",
       "...                                         ...   \n",
       "1995              ESC-50/hen/0_5-263831-B-6.wav   \n",
       "1996  ESC-50/vacuum_cleaner/0_5-263902-A-36.wav   \n",
       "1997        ESC-50/footsteps/0_5-51149-A-25.wav   \n",
       "1998             ESC-50/sheep/0_5-61635-A-8.wav   \n",
       "1999                ESC-50/dog/0_5-9032-A-0.wav   \n",
       "\n",
       "                                        1_2_sec  \\\n",
       "0                 ESC-50/dog/1_1-100032-A-0.wav   \n",
       "1     ESC-50/chirping_birds/1_1-100038-A-14.wav   \n",
       "2     ESC-50/vacuum_cleaner/1_1-100210-A-36.wav   \n",
       "3     ESC-50/vacuum_cleaner/1_1-100210-B-36.wav   \n",
       "4       ESC-50/thunderstorm/1_1-101296-A-19.wav   \n",
       "...                                         ...   \n",
       "1995              ESC-50/hen/1_5-263831-B-6.wav   \n",
       "1996  ESC-50/vacuum_cleaner/1_5-263902-A-36.wav   \n",
       "1997        ESC-50/footsteps/1_5-51149-A-25.wav   \n",
       "1998             ESC-50/sheep/1_5-61635-A-8.wav   \n",
       "1999                ESC-50/dog/1_5-9032-A-0.wav   \n",
       "\n",
       "                                        2_3_sec  \\\n",
       "0                 ESC-50/dog/2_1-100032-A-0.wav   \n",
       "1     ESC-50/chirping_birds/2_1-100038-A-14.wav   \n",
       "2     ESC-50/vacuum_cleaner/2_1-100210-A-36.wav   \n",
       "3     ESC-50/vacuum_cleaner/2_1-100210-B-36.wav   \n",
       "4       ESC-50/thunderstorm/2_1-101296-A-19.wav   \n",
       "...                                         ...   \n",
       "1995              ESC-50/hen/2_5-263831-B-6.wav   \n",
       "1996  ESC-50/vacuum_cleaner/2_5-263902-A-36.wav   \n",
       "1997        ESC-50/footsteps/2_5-51149-A-25.wav   \n",
       "1998             ESC-50/sheep/2_5-61635-A-8.wav   \n",
       "1999                ESC-50/dog/2_5-9032-A-0.wav   \n",
       "\n",
       "                                        3_4_sec  \\\n",
       "0                 ESC-50/dog/3_1-100032-A-0.wav   \n",
       "1     ESC-50/chirping_birds/3_1-100038-A-14.wav   \n",
       "2     ESC-50/vacuum_cleaner/3_1-100210-A-36.wav   \n",
       "3     ESC-50/vacuum_cleaner/3_1-100210-B-36.wav   \n",
       "4       ESC-50/thunderstorm/3_1-101296-A-19.wav   \n",
       "...                                         ...   \n",
       "1995              ESC-50/hen/3_5-263831-B-6.wav   \n",
       "1996  ESC-50/vacuum_cleaner/3_5-263902-A-36.wav   \n",
       "1997        ESC-50/footsteps/3_5-51149-A-25.wav   \n",
       "1998             ESC-50/sheep/3_5-61635-A-8.wav   \n",
       "1999                ESC-50/dog/3_5-9032-A-0.wav   \n",
       "\n",
       "                                        4_5_sec  \\\n",
       "0                 ESC-50/dog/4_1-100032-A-0.wav   \n",
       "1     ESC-50/chirping_birds/4_1-100038-A-14.wav   \n",
       "2     ESC-50/vacuum_cleaner/4_1-100210-A-36.wav   \n",
       "3     ESC-50/vacuum_cleaner/4_1-100210-B-36.wav   \n",
       "4       ESC-50/thunderstorm/4_1-101296-A-19.wav   \n",
       "...                                         ...   \n",
       "1995              ESC-50/hen/4_5-263831-B-6.wav   \n",
       "1996  ESC-50/vacuum_cleaner/4_5-263902-A-36.wav   \n",
       "1997        ESC-50/footsteps/4_5-51149-A-25.wav   \n",
       "1998             ESC-50/sheep/4_5-61635-A-8.wav   \n",
       "1999                ESC-50/dog/4_5-9032-A-0.wav   \n",
       "\n",
       "                                        5_6_sec  \n",
       "0                 ESC-50/dog/5_1-100032-A-0.wav  \n",
       "1     ESC-50/chirping_birds/5_1-100038-A-14.wav  \n",
       "2     ESC-50/vacuum_cleaner/5_1-100210-A-36.wav  \n",
       "3     ESC-50/vacuum_cleaner/5_1-100210-B-36.wav  \n",
       "4       ESC-50/thunderstorm/5_1-101296-A-19.wav  \n",
       "...                                         ...  \n",
       "1995              ESC-50/hen/5_5-263831-B-6.wav  \n",
       "1996  ESC-50/vacuum_cleaner/5_5-263902-A-36.wav  \n",
       "1997        ESC-50/footsteps/5_5-51149-A-25.wav  \n",
       "1998             ESC-50/sheep/5_5-61635-A-8.wav  \n",
       "1999                ESC-50/dog/5_5-9032-A-0.wav  \n",
       "\n",
       "[2000 rows x 13 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = wandb.init(id=run.id,entity='tiny-ml',project = 'wake_word_detection', group='Data',\n",
    "                resume='must')\n",
    "df = pd.read_csv(esc50_csv)\n",
    "out_esc50_csv = meta_data/esc50_csv.name\n",
    "sec_files = np.array(all_out).astype(str)\n",
    "for files in range(sec_files.shape[-1]):\n",
    "    df[f'{files}_{files+1}_sec']=sec_files[...,files].astype(str)\n",
    "df.to_csv(out_esc50_csv)\n",
    "log_wandb_artifact(run=run,path=out_esc50_csv)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6406535a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/u164yrde?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b713ffd0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c02048d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "meta_data_table = wandb.Table(data=df,columns=list(df.columns))\n",
    "run.log({'meta_data_without_media':meta_data_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4c383",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Data table for sound Wandb table with \n",
    "\n",
    "> 1. ouriginal sound string\n",
    "> 2. playable sound file\n",
    "> 3. 4 * 1 second clips files\n",
    "> 4  4 * 1 second clips files\n",
    "\n",
    "# Sound and Spectrogram Table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47880190",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adding Sound and Spectrograms to WandB Table  ðŸ”‰ ðŸª„ ðŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "67d34470",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4_5_sec: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2120.37it/s]\n",
      "5_6_sec: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2807.43it/s]\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(id=run.id,entity='tiny-ml',project = 'wake_word_detection', group='Data',\n",
    "                resume='must')\n",
    "n_examples = 2\n",
    "n_samples = 10\n",
    "s_df = df.sample(n=n_samples, random_state=2)\n",
    "columns = list(df.columns)\n",
    "table_with_media = wandb.Table(data=s_df,columns=columns)\n",
    "for colmn in df.columns[-n_examples:]:\n",
    "    # create wandb sound objects\n",
    "    sounds = [wandb.Audio(fid) for fid in tqdm(s_df[colmn].values,**tqdm_args,desc=colmn)]\n",
    "    # add these as a column to our wandb table\n",
    "    table_with_media.add_column(name=f'sound_{colmn}',data=sounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36010449",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adding Spectrograms to wandb Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d4bd84c4",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4_5_sec: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.15it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.01it/s]\n",
      "5_6_sec: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.97it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns[-n_examples:]:\n",
    "    # read wavs \n",
    "    wavs = map(read_wav,tqdm(s_df[col].values,**tqdm_args,desc=col))\n",
    "    # generate spectrograms\n",
    "    spects = map(get_arm_spectrogram,wavs)\n",
    "    sample_rates = iter(np.full((1,len(df)),1600)[0])\n",
    "    # get numpy arrays of save .jpg files\n",
    "    ims = list(map(plot_spectrogram,spects,sample_rates))\n",
    "    # create a list of wandb images\n",
    "    spects = list(map(wandb.Image,tqdm(ims)))\n",
    "    # add these to a table \n",
    "    table_with_media.add_column(name=f'image_{col}',data=spects)\n",
    "run.log({'sound_spectrogram_table':table_with_media})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521245b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stack our pandas df by 1 second Files ðŸ§±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "12952f33",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>esc10</th>\n",
       "      <th>src_file</th>\n",
       "      <th>take</th>\n",
       "      <th>all_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/dog/0_1-100032-A-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/dog/1_1-100032-A-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/dog/2_1-100032-A-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/dog/3_1-100032-A-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/dog/4_1-100032-A-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>5-9032-A-0.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>9032</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/dog/1_5-9032-A-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>5-9032-A-0.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>9032</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/dog/2_5-9032-A-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>5-9032-A-0.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>9032</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/dog/3_5-9032-A-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>5-9032-A-0.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>9032</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/dog/4_5-9032-A-0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>5-9032-A-0.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>9032</td>\n",
       "      <td>A</td>\n",
       "      <td>ESC-50/dog/5_5-9032-A-0.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename fold target category esc10 src_file take  \\\n",
       "0      1-100032-A-0.wav    1      0      dog  True   100032    A   \n",
       "1      1-100032-A-0.wav    1      0      dog  True   100032    A   \n",
       "2      1-100032-A-0.wav    1      0      dog  True   100032    A   \n",
       "3      1-100032-A-0.wav    1      0      dog  True   100032    A   \n",
       "4      1-100032-A-0.wav    1      0      dog  True   100032    A   \n",
       "...                 ...  ...    ...      ...   ...      ...  ...   \n",
       "11995    5-9032-A-0.wav    5      0      dog  True     9032    A   \n",
       "11996    5-9032-A-0.wav    5      0      dog  True     9032    A   \n",
       "11997    5-9032-A-0.wav    5      0      dog  True     9032    A   \n",
       "11998    5-9032-A-0.wav    5      0      dog  True     9032    A   \n",
       "11999    5-9032-A-0.wav    5      0      dog  True     9032    A   \n",
       "\n",
       "                           all_files  \n",
       "0      ESC-50/dog/0_1-100032-A-0.wav  \n",
       "1      ESC-50/dog/1_1-100032-A-0.wav  \n",
       "2      ESC-50/dog/2_1-100032-A-0.wav  \n",
       "3      ESC-50/dog/3_1-100032-A-0.wav  \n",
       "4      ESC-50/dog/4_1-100032-A-0.wav  \n",
       "...                              ...  \n",
       "11995    ESC-50/dog/1_5-9032-A-0.wav  \n",
       "11996    ESC-50/dog/2_5-9032-A-0.wav  \n",
       "11997    ESC-50/dog/3_5-9032-A-0.wav  \n",
       "11998    ESC-50/dog/4_5-9032-A-0.wav  \n",
       "11999    ESC-50/dog/5_5-9032-A-0.wav  \n",
       "\n",
       "[12000 rows x 8 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp_df = pd.DataFrame(np.repeat(df.values, 6, axis=0))\n",
    "rp_df.columns=df.columns\n",
    "a,b,c,d,e,f = np.array([df[col].values for col in df.columns[-6:]])\n",
    "all_files =np.vstack((a,b,c,d,e,f)).reshape((-1,),order='F')\n",
    "rp_df = rp_df[df.columns[:-6]]\n",
    "rp_df['all_files']=all_files\n",
    "df = rp_df  \n",
    "df.to_csv(out_esc50_csv)\n",
    "log_wandb_artifact(run,out_esc50_csv)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c82182",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define Test,Train, Val using pandas & add sets column to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9e14e26c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sets = { }\n",
    "train_val = df.sample(frac=0.9,random_state=42)\n",
    "sets['test']=df.drop(train_val.index)\n",
    "sets['train']=train_val.sample(frac=0.9,random_state=42)\n",
    "sets['val']=train_val.drop(sets['train'].index)\n",
    "for name,subset in sets.items():\n",
    "        subset = subset.reset_index(drop=True)\n",
    "        subset['set']=[name for va in range(len(subset))]\n",
    "        sets[name]=subset\n",
    "df.groupby(\"target\", group_keys=False).apply(lambda x: x)\n",
    "df = pd.concat(sets.values()).reset_index(drop=True)\n",
    "df.to_csv(out_esc50_csv)\n",
    "log_wandb_artifact(run,out_esc50_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8cb5024d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/u164yrde?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2c1988940>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd012a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# remove silence by randomly sampling from non silent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f2343c49",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "run = wandb.init(id=run.id,entity='tiny-ml',project = 'wake_word_detection', group='Data',\n",
    "                resume='must')\n",
    "is_sound = [not np.std(read_wav(x)) < 0.1 for x in df.all_files.values]\n",
    "df['is_sound']=is_sound\n",
    "df.to_csv(out_esc50_csv)\n",
    "log_wandb_artifact(run,out_esc50_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "79ff83db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "silence = df[df.is_sound==False]\n",
    "final_meta_table = wandb.Table(data=silence,columns=list(df.columns))\n",
    "wandb_audio = list(map(wandb.Audio,silence.all_files.values))\n",
    "final_meta_table.add_column(name='audio',data=wandb_audio)\n",
    "run.log({'silence_tabel':final_meta_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2bf86940",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/u164yrde?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2c1988940>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5cc303",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building a wandb Artifact for our pre processed data (numpy array spectrograms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1be18f6e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/fridadesigley/pico/tiny-ml/processed/train.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/train_data.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/test.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/val.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/train_aug.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x2c199dff0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = wandb.init(id=run.id,entity='tiny-ml',project = 'wake_word_detection', group='Data',\n",
    "                resume='must')\n",
    "artifact = wandb.Artifact(type='pre_processed_sound_data',name='npz-esc-50-files')\n",
    "path = Path('./processed/')\n",
    "# create references for our large pre processed npz files\n",
    "for fid_path in path.iterdir():\n",
    "    print(f'file://{fid_path.resolve()}')  \n",
    "    artifact.add_reference(f'file://{fid_path.resolve()}')\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "852643f5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;249;3;249mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 51/51 [00:16<00:00,  3.09it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for pth in df[df.is_sound==False].all_files.values:\n",
    "    path = Path(pth)\n",
    "    try:\n",
    "        path.unlink()\n",
    "    except FileNotFoundError:\n",
    "        print('file alredady deleted')\n",
    "for path in tqdm(esc_artifacts,**tqdm_args,colour=next(tqdm_colours)):\n",
    "    log_wandb_artifact(run=run,path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09631f66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Save 3D arrays of Spectrograms using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "25907320",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def save_npz(x,y,name,augmenter=None):\n",
    "    wavs = list(map(read_wav,tqdm(x,f'creating {name}')))\n",
    "    if augmenter:\n",
    "        samples = np.full((1,len(y)), 16000)[0]\n",
    "        wavs = list(map(augmenter,wavs,tqdm(samples,f'augmenting {name}')))\n",
    "        x_data = np.array(list(map(get_arm_spectrogram,tqdm(wavs,f'creating {name} augspects'))))\n",
    "    else:\n",
    "        x_data = np.array(list(map(get_arm_spectrogram,tqdm(wavs,f'creating {name} spects'))))\n",
    "    for array in x_data:\n",
    "        assert array.shape==(682, 257)\n",
    "    np.savez(f'{name}.npz',x_data=x_data,y_data=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d785ac4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loop over files and save as npz, this is our pre-processed training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6068b6e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "contains_sound = df[df.is_sound==True]\n",
    "subset =  contains_sound[contains_sound.set=='train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a865c649",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating processed/test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1149/1149 [00:00<00:00, 5700.45it/s]\n",
      "creating processed/test spects: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1149/1149 [00:09<00:00, 123.06it/s]\n",
      "creating processed/train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9194/9194 [00:02<00:00, 3544.44it/s]\n",
      "creating processed/train spects: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9194/9194 [01:24<00:00, 108.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmenting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating processed/train_aug: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1838/1838 [00:00<00:00, 2010.63it/s]\n",
      "augmenting processed/train_aug: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1837/1838 [03:45<00:00,  8.16it/s]\n",
      "creating processed/train_aug augspects: 100%|â–ˆ| 1838/1838 [00:15<00:00, 120.38it\n",
      "creating processed/val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1030/1030 [00:00<00:00, 3070.56it/s]\n",
      "creating processed/val spects: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1030/1030 [00:08<00:00, 119.39it/s]\n"
     ]
    }
   ],
   "source": [
    "pre_process=True\n",
    "contains_sound = df[df.is_sound==True]\n",
    "if pre_process:\n",
    "    for name in df.set.unique():\n",
    "        subset =  contains_sound[contains_sound.set==name]\n",
    "        x_data = subset['all_files'].values\n",
    "        y_data = subset['target'].values\n",
    "        save_npz(x_data,y_data,f'processed/{name}')\n",
    "        if name=='train':\n",
    "            print('augmenting')\n",
    "            save_npz(x_data[:len(x_data)//5],y_data[:len(y_data)//5],f'processed/{name}_aug',augmenter=augmenter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7637c344",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building a wandb Artifact for our pre processed data (numpy array spectrograms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "617cf77a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/fridadesigley/pico/tiny-ml/processed/train.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/train_data.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/test.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/val.npz\n",
      "file:///Users/fridadesigley/pico/tiny-ml/processed/train_aug.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x2c45f4670>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact = wandb.Artifact(type='pre_processed_sound_data',name='npz-esc-50-files')\n",
    "path = Path('./processed/')\n",
    "# create references for our large pre processed npz files\n",
    "for fid_path in path.iterdir():\n",
    "    print(f'file://{fid_path.resolve()}')  \n",
    "    artifact.add_reference(f'file://{fid_path.resolve()}')\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f758cf5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### To finish our data run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "657f0228",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/u164yrde?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2c1988940>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.finish()\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097b848",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ece38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training load training dataset which has already been pre processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac742071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import wave\n",
    "from pvrecorder import PvRecorder\n",
    "import time \n",
    "from utils.data_processing import log_wandb_artifact,plot_spectrogram,segment,read_wav,Arm_spect\n",
    "from utils.create_sweep import create_sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feaccb8e",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with np.load('processed/train.npz',allow_pickle=True) as data:\n",
    "    train_x = data['x_data'].astype(np.float32)\n",
    "    train_y = data['y_data'].astype(np.uint8)\n",
    "with np.load('processed/train_aug.npz',allow_pickle=True) as data:\n",
    "    aug_x = data['x_data'].astype(np.float32)\n",
    "    aug_y = data['y_data'].astype(np.uint8)\n",
    "with np.load('processed/test.npz',allow_pickle=True) as data:\n",
    "    test_x = data['x_data'].astype(np.float32)\n",
    "    test_y = data['y_data'].astype(np.uint8)\n",
    "with np.load('processed/val.npz',allow_pickle=True) as data:\n",
    "    val_x = data['x_data'].astype(np.float32)\n",
    "    val_y = data['y_data'].astype(np.uint8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4832936a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd03eb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## create TensoFlow Dataset from numpy arrays of spectrograms and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50c89b6f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(train_x, tf.float32), train_y))\n",
    "train_aug_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(aug_x, tf.float32), aug_y))\n",
    "train_dataset = train_dataset.concatenate(train_aug_dataset)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(val_x, tf.float32),val_y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(test_x,tf.float32),test_y))\n",
    "\n",
    "train_ds = train_dataset.cache().shuffle(100, seed=42).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_dataset.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_dataset.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc40f20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 20:42:45.077465: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(682, 257), dtype=float32, numpy=\n",
       " array([[ 7.65625 ,  6.71875 , 22.296875, ...,  1.875   ,  2.765625,\n",
       "          3.328125],\n",
       "        [12.28125 , 10.046875, 21.640625, ...,  2.921875,  3.046875,\n",
       "          2.5625  ],\n",
       "        [ 9.      , 19.953125, 30.546875, ...,  2.75    ,  3.03125 ,\n",
       "          0.796875],\n",
       "        ...,\n",
       "        [ 8.171875, 10.484375, 25.4375  , ...,  1.125   ,  0.875   ,\n",
       "          0.421875],\n",
       "        [ 1.765625, 18.71875 , 24.359375, ...,  1.25    ,  1.890625,\n",
       "          0.703125],\n",
       "        [ 6.4375  , 25.96875 , 31.125   , ...,  2.40625 ,  3.171875,\n",
       "          2.390625]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=uint8, numpy=28>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_ds))\n",
    "x[0],y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f197571",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test that this dataset is loading okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc95bb2d",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x,y = next(iter(train_dataset.take(1)))\n",
    "#print(f' target = {y}, \\n spectrogram = \\n {x}')\n",
    "input_shape = tf.expand_dims(x, axis=-1).shape\n",
    "#print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e29a130a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  4.328125   5.046875   5.609375 ...   1.609375   1.203125   0.59375 ]\n",
      " [  8.015625   2.5        7.03125  ...   1.59375    2.46875    1.859375]\n",
      " [ 15.921875   9.109375  15.5625   ...   2.015625   3.5625     3.5625  ]\n",
      " ...\n",
      " [ 88.390625  59.9375    12.6875   ...   2.75       2.6875     0.8125  ]\n",
      " [100.078125  61.8125    14.1875   ...   2.71875    3.125      0.6875  ]\n",
      " [ 88.875     63.03125    5.09375  ...   3.         3.0625     1.84375 ]], shape=(682, 257), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2159c4e",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 20:42:54.297941: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "norm_layer.adapt(train_dataset.map(lambda x, y: tf.reshape(x, input_shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e68f71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# we have made an application and embeded wandb in the recording process\n",
    "this is to allow us to :\n",
    "> - capture new data from you device ðŸ”Š\n",
    "> - Version data using wandb Atefact ðŸº\n",
    "> - Define your wakework ðŸ“¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01e1429d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T10:47:21.788567Z",
     "start_time": "2023-01-28T10:47:21.587446Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdata\u001b[0m\r\n",
      "â”œâ”€â”€ \u001b[01;34mbackground\u001b[0m\r\n",
      "â”œâ”€â”€ \u001b[01;34mno\u001b[0m\r\n",
      "â””â”€â”€ \u001b[01;34myes\u001b[0m\r\n",
      "\r\n",
      "3 directories\r\n"
     ]
    }
   ],
   "source": [
    "!tree data -d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c87b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create a new training run for our training ðŸƒðŸ»â€â™€ï¸ â™€ï¸ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d733c973",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrds\u001b[0m (\u001b[33mtiny-ml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230130_204300-fr6wwl37</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/fr6wwl37\" target=\"_blank\">lunar-wish-28</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/fr6wwl37\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/fr6wwl37</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(entity='tiny-ml',project = 'wake_word_detection', group='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00822475",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define our model ðŸ—ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60cc0681",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "baseline_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Input(shape=input_shape),\n",
    "  tf.keras.layers.experimental.preprocessing.Resizing(32, 32, interpolation=\"nearest\"), \n",
    "  norm_layer,\n",
    "  tf.keras.layers.Conv2D(8, kernel_size=(8,8), strides=(2, 2), activation=\"relu\"),\n",
    "  tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dropout(0.25),\n",
    "  tf.keras.layers.Dense(50, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5401882",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing (Resizing)         (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 32, 32, 1)        3         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 13, 13, 8)         520       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 6, 6, 8)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 288)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 288)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                14450     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,973\n",
      "Trainable params: 14,970\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c008e0d1",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "METRICS = [\"accuracy\",]\n",
    "baseline_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=METRICS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69ce5024",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    ''' a function to increase lr at start of trining\n",
    "    '''\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        # add somthing like np.linespace([0,-0.1])\n",
    "        return lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6418c1f4",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "# Initialize a new W&B run \n",
    "checkpoint_path = \"training_1/\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# Create a callback that saves the model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04840fce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc7a0b1e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(verbose=0, patience=25), \n",
    "    tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    ",cp_callback,WandbMetricsLogger(),WandbModelCheckpoint(checkpoint_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffa1efd6",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/fr6wwl37?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2bc0f1360>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09c56d39",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "329/345 [===========================>..] - ETA: 0s - loss: 3.5054 - accuracy: 0.0954\n",
      "Epoch 1: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 3s 7ms/step - loss: 3.5017 - accuracy: 0.0956 - val_loss: 3.2672 - val_accuracy: 0.1379 - lr: 0.0030\n",
      "Epoch 2/250\n",
      "340/345 [============================>.] - ETA: 0s - loss: 3.1688 - accuracy: 0.1669\n",
      "Epoch 2: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 3.1689 - accuracy: 0.1666 - val_loss: 3.1763 - val_accuracy: 0.1680 - lr: 0.0030\n",
      "Epoch 3/250\n",
      "327/345 [===========================>..] - ETA: 0s - loss: 3.0316 - accuracy: 0.1966\n",
      "Epoch 3: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 3.0378 - accuracy: 0.1943 - val_loss: 3.1432 - val_accuracy: 0.1592 - lr: 0.0030\n",
      "Epoch 4/250\n",
      "333/345 [===========================>..] - ETA: 0s - loss: 2.9566 - accuracy: 0.2059\n",
      "Epoch 4: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.9575 - accuracy: 0.2063 - val_loss: 3.1350 - val_accuracy: 0.1903 - lr: 0.0030\n",
      "Epoch 5/250\n",
      "330/345 [===========================>..] - ETA: 0s - loss: 2.8951 - accuracy: 0.2213\n",
      "Epoch 5: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.9002 - accuracy: 0.2189 - val_loss: 3.0797 - val_accuracy: 0.1845 - lr: 0.0030\n",
      "Epoch 6/250\n",
      "331/345 [===========================>..] - ETA: 0s - loss: 2.8491 - accuracy: 0.2268\n",
      "Epoch 6: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 5ms/step - loss: 2.8527 - accuracy: 0.2263 - val_loss: 3.0853 - val_accuracy: 0.1845 - lr: 0.0030\n",
      "Epoch 7/250\n",
      "337/345 [============================>.] - ETA: 0s - loss: 2.8005 - accuracy: 0.2414\n",
      "Epoch 7: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.8011 - accuracy: 0.2414 - val_loss: 3.0828 - val_accuracy: 0.1874 - lr: 0.0030\n",
      "Epoch 8/250\n",
      "329/345 [===========================>..] - ETA: 0s - loss: 2.7797 - accuracy: 0.2452\n",
      "Epoch 8: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.7831 - accuracy: 0.2445 - val_loss: 3.0747 - val_accuracy: 0.1806 - lr: 0.0030\n",
      "Epoch 9/250\n",
      "334/345 [============================>.] - ETA: 0s - loss: 2.7544 - accuracy: 0.2532\n",
      "Epoch 9: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.7559 - accuracy: 0.2531 - val_loss: 3.0770 - val_accuracy: 0.1913 - lr: 0.0030\n",
      "Epoch 10/250\n",
      "335/345 [============================>.] - ETA: 0s - loss: 2.7458 - accuracy: 0.2548\n",
      "Epoch 10: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.7467 - accuracy: 0.2543 - val_loss: 3.0699 - val_accuracy: 0.1845 - lr: 0.0030\n",
      "Epoch 11/250\n",
      "343/345 [============================>.] - ETA: 0s - loss: 2.7143 - accuracy: 0.2585\n",
      "Epoch 11: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 5ms/step - loss: 2.7133 - accuracy: 0.2586 - val_loss: 3.0600 - val_accuracy: 0.1990 - lr: 0.0027\n",
      "Epoch 12/250\n",
      "340/345 [============================>.] - ETA: 0s - loss: 2.6942 - accuracy: 0.2637\n",
      "Epoch 12: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.6920 - accuracy: 0.2637 - val_loss: 3.0523 - val_accuracy: 0.2039 - lr: 0.0025\n",
      "Epoch 13/250\n",
      "332/345 [===========================>..] - ETA: 0s - loss: 2.6678 - accuracy: 0.2704\n",
      "Epoch 13: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.6697 - accuracy: 0.2699 - val_loss: 3.0649 - val_accuracy: 0.1971 - lr: 0.0022\n",
      "Epoch 14/250\n",
      "332/345 [===========================>..] - ETA: 0s - loss: 2.6456 - accuracy: 0.2764\n",
      "Epoch 14: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.6449 - accuracy: 0.2769 - val_loss: 3.0540 - val_accuracy: 0.2029 - lr: 0.0020\n",
      "Epoch 15/250\n",
      "343/345 [============================>.] - ETA: 0s - loss: 2.6244 - accuracy: 0.2761\n",
      "Epoch 15: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 5ms/step - loss: 2.6239 - accuracy: 0.2763 - val_loss: 3.0657 - val_accuracy: 0.2049 - lr: 0.0018\n",
      "Epoch 16/250\n",
      "336/345 [============================>.] - ETA: 0s - loss: 2.6074 - accuracy: 0.2864\n",
      "Epoch 16: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.6059 - accuracy: 0.2863 - val_loss: 3.0565 - val_accuracy: 0.1951 - lr: 0.0016\n",
      "Epoch 17/250\n",
      "330/345 [===========================>..] - ETA: 0s - loss: 2.6006 - accuracy: 0.2883\n",
      "Epoch 17: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.6028 - accuracy: 0.2877 - val_loss: 3.0837 - val_accuracy: 0.2126 - lr: 0.0015\n",
      "Epoch 18/250\n",
      "340/345 [============================>.] - ETA: 0s - loss: 2.5875 - accuracy: 0.2917\n",
      "Epoch 18: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.5871 - accuracy: 0.2914 - val_loss: 3.0728 - val_accuracy: 0.2117 - lr: 0.0013\n",
      "Epoch 19/250\n",
      "344/345 [============================>.] - ETA: 0s - loss: 2.5662 - accuracy: 0.2948\n",
      "Epoch 19: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.5652 - accuracy: 0.2951 - val_loss: 3.0849 - val_accuracy: 0.2097 - lr: 0.0012\n",
      "Epoch 20/250\n",
      "344/345 [============================>.] - ETA: 0s - loss: 2.5593 - accuracy: 0.2930\n",
      "Epoch 20: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 5ms/step - loss: 2.5586 - accuracy: 0.2931 - val_loss: 3.0909 - val_accuracy: 0.2087 - lr: 0.0011\n",
      "Epoch 21/250\n",
      "344/345 [============================>.] - ETA: 0s - loss: 2.5431 - accuracy: 0.2966\n",
      "Epoch 21: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.5422 - accuracy: 0.2967 - val_loss: 3.0764 - val_accuracy: 0.2039 - lr: 9.9861e-04\n",
      "Epoch 22/250\n",
      "344/345 [============================>.] - ETA: 0s - loss: 2.5498 - accuracy: 0.2979\n",
      "Epoch 22: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.5500 - accuracy: 0.2979 - val_loss: 3.0844 - val_accuracy: 0.2107 - lr: 9.0358e-04\n",
      "Epoch 23/250\n",
      "340/345 [============================>.] - ETA: 0s - loss: 2.5392 - accuracy: 0.2992\n",
      "Epoch 23: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.5403 - accuracy: 0.2988 - val_loss: 3.0977 - val_accuracy: 0.2107 - lr: 8.1760e-04\n",
      "Epoch 24/250\n",
      "344/345 [============================>.] - ETA: 0s - loss: 2.5347 - accuracy: 0.3021\n",
      "Epoch 24: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 5ms/step - loss: 2.5358 - accuracy: 0.3022 - val_loss: 3.0977 - val_accuracy: 0.2049 - lr: 7.3979e-04\n",
      "Epoch 25/250\n",
      "342/345 [============================>.] - ETA: 0s - loss: 2.5306 - accuracy: 0.3073\n",
      "Epoch 25: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.5289 - accuracy: 0.3073 - val_loss: 3.0980 - val_accuracy: 0.2117 - lr: 6.6939e-04\n",
      "Epoch 26/250\n",
      "339/345 [============================>.] - ETA: 0s - loss: 2.5234 - accuracy: 0.3016\n",
      "Epoch 26: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.5225 - accuracy: 0.3023 - val_loss: 3.0877 - val_accuracy: 0.2097 - lr: 6.0569e-04\n",
      "Epoch 27/250\n",
      "341/345 [============================>.] - ETA: 0s - loss: 2.5084 - accuracy: 0.3109\n",
      "Epoch 27: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.5083 - accuracy: 0.3106 - val_loss: 3.1076 - val_accuracy: 0.2117 - lr: 5.4805e-04\n",
      "Epoch 28/250\n",
      "332/345 [===========================>..] - ETA: 0s - loss: 2.5081 - accuracy: 0.3091\n",
      "Epoch 28: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.5100 - accuracy: 0.3092 - val_loss: 3.0904 - val_accuracy: 0.2087 - lr: 4.9590e-04\n",
      "Epoch 29/250\n",
      "339/345 [============================>.] - ETA: 0s - loss: 2.5048 - accuracy: 0.3117\n",
      "Epoch 29: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 5ms/step - loss: 2.5033 - accuracy: 0.3118 - val_loss: 3.0846 - val_accuracy: 0.2126 - lr: 4.4871e-04\n",
      "Epoch 30/250\n",
      "333/345 [===========================>..] - ETA: 0s - loss: 2.5110 - accuracy: 0.3116\n",
      "Epoch 30: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.5117 - accuracy: 0.3107 - val_loss: 3.0870 - val_accuracy: 0.2146 - lr: 4.0601e-04\n",
      "Epoch 31/250\n",
      "330/345 [===========================>..] - ETA: 0s - loss: 2.5048 - accuracy: 0.3087\n",
      "Epoch 31: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.5046 - accuracy: 0.3096 - val_loss: 3.0845 - val_accuracy: 0.2117 - lr: 3.6737e-04\n",
      "Epoch 32/250\n",
      "341/345 [============================>.] - ETA: 0s - loss: 2.4819 - accuracy: 0.3142\n",
      "Epoch 32: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.4833 - accuracy: 0.3139 - val_loss: 3.0811 - val_accuracy: 0.2165 - lr: 3.3241e-04\n",
      "Epoch 33/250\n",
      "343/345 [============================>.] - ETA: 0s - loss: 2.4935 - accuracy: 0.3121\n",
      "Epoch 33: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.4920 - accuracy: 0.3129 - val_loss: 3.0866 - val_accuracy: 0.2194 - lr: 3.0078e-04\n",
      "Epoch 34/250\n",
      "332/345 [===========================>..] - ETA: 0s - loss: 2.4968 - accuracy: 0.3083\n",
      "Epoch 34: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.4950 - accuracy: 0.3091 - val_loss: 3.0829 - val_accuracy: 0.2155 - lr: 2.7215e-04\n",
      "Epoch 35/250\n",
      "337/345 [============================>.] - ETA: 0s - loss: 2.4810 - accuracy: 0.3159\n",
      "Epoch 35: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.4795 - accuracy: 0.3160 - val_loss: 3.0794 - val_accuracy: 0.2194 - lr: 2.4626e-04\n",
      "Epoch 36/250\n",
      "335/345 [============================>.] - ETA: 0s - loss: 2.4910 - accuracy: 0.3108\n",
      "Epoch 36: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 6ms/step - loss: 2.4894 - accuracy: 0.3108 - val_loss: 3.0803 - val_accuracy: 0.2252 - lr: 2.2282e-04\n",
      "Epoch 37/250\n",
      "334/345 [============================>.] - ETA: 0s - loss: 2.4776 - accuracy: 0.3201\n",
      "Epoch 37: saving model to training_1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_1/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training_1)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "345/345 [==============================] - 2s 6ms/step - loss: 2.4760 - accuracy: 0.3194 - val_loss: 3.0774 - val_accuracy: 0.2155 - lr: 2.0162e-04\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 250\n",
    "history = baseline_model.fit(\n",
    "     train_ds, \n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e73e2f68",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>â–â–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch/epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–</td></tr><tr><td>epoch/loss</td><td>â–ˆâ–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/lr</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–</td></tr><tr><td>epoch/val_accuracy</td><td>â–â–ƒâ–ƒâ–…â–…â–…â–…â–„â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡</td></tr><tr><td>epoch/val_loss</td><td>â–ˆâ–…â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.31943</td></tr><tr><td>epoch/epoch</td><td>36</td></tr><tr><td>epoch/learning_rate</td><td>0.0002</td></tr><tr><td>epoch/loss</td><td>2.47604</td></tr><tr><td>epoch/lr</td><td>0.0002</td></tr><tr><td>epoch/val_accuracy</td><td>0.21553</td></tr><tr><td>epoch/val_loss</td><td>3.07742</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-wish-28</strong> at: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/fr6wwl37\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/fr6wwl37</a><br/>Synced 6 W&B file(s), 0 media file(s), 300 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230130_204300-fr6wwl37/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b44b9",
   "metadata": {},
   "source": [
    "## WandB Sweep ðŸ§¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99dbc7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wake_word_sweep_mp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wake_word_sweep_mp.py\n",
    "import os\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, Nadam\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils.create_sweep import create_sweep\n",
    "import multiprocessing as mp\n",
    "\n",
    "def sub_sweep(swp_id):\n",
    "    def pre_train(config = None):\n",
    "\n",
    "        run = wandb.init(config=config)\n",
    "\n",
    "        EPOCHS = run.config.epochs or 100\n",
    "        LEARNING_RATE = run.config.learning_rate or 0.005\n",
    "        KERNEL_SIZE = run.config.kernel_size\n",
    "        DROPOUT_P = run.config.dropout\n",
    "        ACTIVATION = run.config.activation\n",
    "        BATCH_SIZE = run.config.batch_size\n",
    "        BETA_1 = run.config.beta_1\n",
    "        BETA_2 = run.config.beta_2\n",
    "        EPSILON = run.config.epsilon\n",
    "\n",
    "        with np.load('processed/train.npz',allow_pickle=True) as data:\n",
    "            train_x = data['x_data'].astype(np.float32)\n",
    "            train_y = data['y_data'].astype(np.uint8)\n",
    "        with np.load('processed/train_aug.npz',allow_pickle=True) as data:\n",
    "            aug_x = data['x_data'].astype(np.float32)\n",
    "            aug_y = data['y_data'].astype(np.uint8)\n",
    "        with np.load('processed/val.npz',allow_pickle=True) as data:\n",
    "            val_x = data['x_data'].astype(np.float32)\n",
    "            val_y = data['y_data'].astype(np.uint8)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(train_x, tf.float32), train_y))\n",
    "        train_aug_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(aug_x, tf.float32), aug_y))\n",
    "        train_dataset = train_dataset.concatenate(train_aug_dataset)\n",
    "\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(val_x, tf.float32),val_y))\n",
    "\n",
    "        train_ds = train_dataset.cache().shuffle(1000, seed=42).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        val_ds = val_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "        x,y = next(iter(train_dataset.take(1)))\n",
    "        #print(f' target = {y}, \\n spectrogram = \\n {x}')\n",
    "        input_shape = tf.expand_dims(x, axis=-1).shape\n",
    "        #print(input_shape)\n",
    "        norm_layer.adapt(train_dataset.map(lambda x, y: tf.reshape(x, input_shape)))\n",
    "\n",
    "        # Initialize a new W&B run \n",
    "        checkpoint_path = \"training_1/\"\n",
    "        checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "        # Create a callback that saves the model's weights\n",
    "        baseline_model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=input_shape),\n",
    "            tf.keras.layers.experimental.preprocessing.Resizing(32, 32, interpolation=\"nearest\"), \n",
    "            norm_layer,\n",
    "            tf.keras.layers.Conv2D(8, kernel_size=(KERNEL_SIZE,KERNEL_SIZE), strides=(2, 2), activation=ACTIVATION),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dropout(DROPOUT_P),\n",
    "            tf.keras.layers.Dense(50, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        if run.config.optimizer == \"adam\":\n",
    "            opt = Adam(\n",
    "                learning_rate=LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON\n",
    "                )\n",
    "        elif run.config.optimizer == \"adamax\":\n",
    "            opt = Adamax(\n",
    "                learning_rate=LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON\n",
    "            )\n",
    "        elif run.config.optimizer == \"nadam\":\n",
    "            opt = Nadam(\n",
    "                learning_rate=LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON\n",
    "            )\n",
    "\n",
    "        METRICS = [\"accuracy\",]\n",
    "        baseline_model.compile(\n",
    "            optimizer=opt,\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            metrics=METRICS\n",
    "        )\n",
    "        def scheduler(epoch, lr):\n",
    "            ''' a function to increase lr at start of trining\n",
    "            '''\n",
    "            if epoch < 10:\n",
    "                return lr\n",
    "            else:\n",
    "                # add somthing like np.linespace([0,-0.1])\n",
    "                return lr * tf.math.exp(-0.1)\n",
    "\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        verbose=1\n",
    "        )\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(verbose=0, patience=25), \n",
    "            tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "            ,cp_callback,WandbMetricsLogger()]\n",
    "        history = baseline_model.fit(\n",
    "             train_ds, \n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=val_ds\n",
    "        )\n",
    "        run.finish()\n",
    "wandb.agent(swp_id, function=pre_train,count=10)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aae4fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wake_word_sweep_mp import main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57c11e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: jg8rqet3\n",
      "Sweep URL: https://wandb.ai/tiny-ml/wake_word_detection/sweeps/jg8rqet3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: m0i6gsnb with config:\n",
      "wandb: \tactivation: gelu\n",
      "wandb: \tbatch_size: 40\n",
      "wandb: \tbeta_1: 0.9\n",
      "wandb: \tbeta_2: 0.987\n",
      "wandb: \tdropout: 0.04\n",
      "wandb: \tepochs: 100\n",
      "wandb: \tepsilon: 1e-07\n",
      "wandb: \tkernel_size: 9\n",
      "wandb: \tlearning_rate: 0.0007\n",
      "wandb: \toptimizer: adam\n",
      "wandb: Agent Starting Run: m0i2hwdh with config:\n",
      "wandb: \tactivation: selu\n",
      "wandb: \tbatch_size: 40\n",
      "wandb: \tbeta_1: 0.5625\n",
      "wandb: \tbeta_2: 0.987\n",
      "wandb: \tdropout: 0.05\n",
      "wandb: \tepochs: 100\n",
      "wandb: \tepsilon: 1e-07\n",
      "wandb: \tkernel_size: 11\n",
      "wandb: \tlearning_rate: 0.00030000000000000003\n",
      "wandb: \toptimizer: adam\n",
      "wandb: Currently logged in as: frds (tiny-ml). Use `wandb login --relogin` to force relogin\n",
      "wandb: Currently logged in as: frds (tiny-ml). Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.13.9\n",
      "wandb: Run data is saved locally in /Users/fridadesigley/pico/tiny-ml/wandb/run-20230130_204715-m0i2hwdh\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run helpful-sweep-2\n",
      "wandb: â­ï¸ View project at https://wandb.ai/tiny-ml/wake_word_detection\n",
      "wandb: ðŸ§¹ View sweep at https://wandb.ai/tiny-ml/wake_word_detection/sweeps/jg8rqet3\n",
      "wandb: ðŸš€ View run at https://wandb.ai/tiny-ml/wake_word_detection/runs/m0i2hwdh\n",
      "wandb: Tracking run with wandb version 0.13.9\n",
      "wandb: Run data is saved locally in /Users/fridadesigley/pico/tiny-ml/wandb/run-20230130_204715-m0i6gsnb\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run ancient-sweep-1\n",
      "wandb: â­ï¸ View project at https://wandb.ai/tiny-ml/wake_word_detection\n",
      "wandb: ðŸ§¹ View sweep at https://wandb.ai/tiny-ml/wake_word_detection/sweeps/jg8rqet3\n",
      "wandb: ðŸš€ View run at https://wandb.ai/tiny-ml/wake_word_detection/runs/m0i6gsnb\n",
      "WARNING:tensorflow:From /Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From /Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2023-01-30 20:48:32.311901: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-30 20:48:32.315790: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "274/276 [============================>.] - ETA: 0s - loss: 3.8460 - accuracy: 0.0423\n",
      "Epoch 1: saving model to training_1/\n",
      "\n",
      "Epoch 1: saving model to training_1/\n",
      "276/276 [==============================] - 19s 65ms/step - loss: 3.6830 - accuracy: 0.0624 - val_loss: 3.5095 - val_accuracy: 0.0903 - lr: 7.0000e-04\n",
      "Epoch 2/100\n",
      "  2/276 [..............................] - ETA: 29s - loss: 3.4483 - accuracy: 0.0875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 20:49:25.099924: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at save_restore_v2_ops.cc:286 : NOT_FOUND: training_1/_temp/part-00000-of-00001.data-00000-of-00001; No such file or directory\n",
      "wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15/276 [>.............................] - ETA: 34s - loss: 3.4304 - accuracy: 0.1000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: - 0.321 MB of 0.321 MB uploaded (0.000 MB deduped)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26/276 [=>............................] - ETA: 29s - loss: 3.4552 - accuracy: 0.0913"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: \\ 0.321 MB of 0.324 MB uploaded (0.000 MB deduped)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37/276 [===>..........................] - ETA: 25s - loss: 3.4397 - accuracy: 0.0980"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: | 0.321 MB of 0.324 MB uploaded (0.000 MB deduped)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 58/276 [=====>........................] - ETA: 21s - loss: 3.4293 - accuracy: 0.1078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ðŸš€ View run helpful-sweep-2 at: https://wandb.ai/tiny-ml/wake_word_detection/runs/m0i2hwdh\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230130_204715-m0i2hwdh/logs\n",
      "Run m0i2hwdh errored: NotFoundError()\n",
      "wandb: ERROR Run m0i2hwdh errored: NotFoundError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/276 [============>.................] - ETA: 13s - loss: 3.3912 - accuracy: 0.1110"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: nlj4f6xx with config:\n",
      "wandb: \tactivation: tanh\n",
      "wandb: \tbatch_size: 120\n",
      "wandb: \tbeta_1: 1.125\n",
      "wandb: \tbeta_2: 0.99\n",
      "wandb: \tdropout: 0.41\n",
      "wandb: \tepochs: 100\n",
      "wandb: \tepsilon: 7.5e-08\n",
      "wandb: \tkernel_size: 11\n",
      "wandb: \tlearning_rate: 0.0009000000000000001\n",
      "wandb: \toptimizer: adam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/276 [===================>..........] - ETA: 7s - loss: 3.3628 - accuracy: 0.1193"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Tracking run with wandb version 0.13.9\n",
      "wandb: Run data is saved locally in /Users/fridadesigley/pico/tiny-ml/wandb/run-20230130_204939-nlj4f6xx\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run earthy-sweep-3\n",
      "wandb: â­ï¸ View project at https://wandb.ai/tiny-ml/wake_word_detection\n",
      "wandb: ðŸ§¹ View sweep at https://wandb.ai/tiny-ml/wake_word_detection/sweeps/jg8rqet3\n",
      "wandb: ðŸš€ View run at https://wandb.ai/tiny-ml/wake_word_detection/runs/nlj4f6xx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - ETA: 0s - loss: 3.3651 - accuracy: 0.1197\n",
      "Epoch 2: saving model to training_1/\n",
      "276/276 [==============================] - 28s 103ms/step - loss: 3.3651 - accuracy: 0.1197 - val_loss: 3.2926 - val_accuracy: 0.1272 - lr: 7.0000e-04\n",
      "Epoch 3/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 3.2144 - accuracy: 0.1567\n",
      "Epoch 3: saving model to training_1/\n",
      "276/276 [==============================] - 22s 79ms/step - loss: 3.2144 - accuracy: 0.1567 - val_loss: 3.2121 - val_accuracy: 0.1718 - lr: 7.0000e-04\n",
      "Epoch 4/100\n",
      "273/276 [============================>.] - ETA: 0s - loss: 3.1253 - accuracy: 0.1772\n",
      "Epoch 4: saving model to training_1/\n",
      "276/276 [==============================] - 3s 12ms/step - loss: 3.1268 - accuracy: 0.1774 - val_loss: 3.1520 - val_accuracy: 0.1845 - lr: 7.0000e-04\n",
      "Epoch 5/100\n",
      "269/276 [============================>.] - ETA: 0s - loss: 3.0580 - accuracy: 0.1959\n",
      "Epoch 5: saving model to training_1/\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 3.0609 - accuracy: 0.1953 - val_loss: 3.1139 - val_accuracy: 0.1845 - lr: 7.0000e-04\n",
      "Epoch 6/100\n",
      "272/276 [============================>.] - ETA: 0s - loss: 3.0046 - accuracy: 0.2009\n",
      "Epoch 6: saving model to training_1/\n",
      "276/276 [==============================] - 4s 14ms/step - loss: 3.0038 - accuracy: 0.2014 - val_loss: 3.0926 - val_accuracy: 0.2087 - lr: 7.0000e-04\n",
      "Epoch 7/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 2.9622 - accuracy: 0.2122\n",
      "Epoch 7: saving model to training_1/\n",
      "276/276 [==============================] - 4s 13ms/step - loss: 2.9622 - accuracy: 0.2122 - val_loss: 3.0568 - val_accuracy: 0.1961 - lr: 7.0000e-04\n",
      "Epoch 8/100\n",
      "269/276 [============================>.] - ETA: 0s - loss: 2.9159 - accuracy: 0.2250\n",
      "Epoch 8: saving model to training_1/\n",
      "276/276 [==============================] - 4s 14ms/step - loss: 2.9179 - accuracy: 0.2244 - val_loss: 3.0374 - val_accuracy: 0.1971 - lr: 7.0000e-04\n",
      "Epoch 9/100\n",
      "274/276 [============================>.] - ETA: 0s - loss: 2.8748 - accuracy: 0.2358\n",
      "Epoch 9: saving model to training_1/\n",
      "276/276 [==============================] - 4s 15ms/step - loss: 2.8768 - accuracy: 0.2354 - val_loss: 3.0507 - val_accuracy: 0.2049 - lr: 7.0000e-04\n",
      "Epoch 10/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 2.8517 - accuracy: 0.2383\n",
      "Epoch 10: saving model to training_1/\n",
      "276/276 [==============================] - 3s 12ms/step - loss: 2.8517 - accuracy: 0.2383 - val_loss: 3.0160 - val_accuracy: 0.2078 - lr: 7.0000e-04\n",
      "Epoch 11/100\n",
      "271/276 [============================>.] - ETA: 0s - loss: 2.8166 - accuracy: 0.2466\n",
      "Epoch 11: saving model to training_1/\n",
      "276/276 [==============================] - 5s 17ms/step - loss: 2.8187 - accuracy: 0.2457 - val_loss: 3.0128 - val_accuracy: 0.2049 - lr: 6.3339e-04\n",
      "Epoch 12/100\n",
      "272/276 [============================>.] - ETA: 0s - loss: 2.7814 - accuracy: 0.2553\n",
      "Epoch 12: saving model to training_1/\n",
      "276/276 [==============================] - 4s 14ms/step - loss: 2.7816 - accuracy: 0.2553 - val_loss: 3.0011 - val_accuracy: 0.2155 - lr: 5.7311e-04\n",
      "Epoch 13/100\n",
      "271/276 [============================>.] - ETA: 0s - loss: 2.7632 - accuracy: 0.2605\n",
      "Epoch 13: saving model to training_1/\n",
      "276/276 [==============================] - 4s 15ms/step - loss: 2.7634 - accuracy: 0.2605 - val_loss: 2.9996 - val_accuracy: 0.2175 - lr: 5.1857e-04\n",
      "Epoch 14/100\n",
      "275/276 [============================>.] - ETA: 0s - loss: 2.7399 - accuracy: 0.2643\n",
      "Epoch 14: saving model to training_1/\n",
      "276/276 [==============================] - 4s 14ms/step - loss: 2.7402 - accuracy: 0.2644 - val_loss: 2.9997 - val_accuracy: 0.2194 - lr: 4.6922e-04\n",
      "Epoch 15/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 2.7212 - accuracy: 0.2673\n",
      "Epoch 15: saving model to training_1/\n",
      "276/276 [==============================] - 4s 14ms/step - loss: 2.7212 - accuracy: 0.2673 - val_loss: 2.9902 - val_accuracy: 0.2136 - lr: 4.2457e-04\n",
      "Epoch 16/100\n",
      "270/276 [============================>.] - ETA: 0s - loss: 2.6989 - accuracy: 0.2738\n",
      "Epoch 16: saving model to training_1/\n",
      "276/276 [==============================] - 4s 16ms/step - loss: 2.7040 - accuracy: 0.2728 - val_loss: 2.9883 - val_accuracy: 0.2155 - lr: 3.8417e-04\n",
      "Epoch 17/100\n",
      "273/276 [============================>.] - ETA: 0s - loss: 2.6854 - accuracy: 0.2796\n",
      "Epoch 17: saving model to training_1/\n",
      "276/276 [==============================] - 4s 16ms/step - loss: 2.6867 - accuracy: 0.2793 - val_loss: 2.9898 - val_accuracy: 0.2126 - lr: 3.4761e-04\n",
      "Epoch 18/100\n",
      "272/276 [============================>.] - ETA: 0s - loss: 2.6813 - accuracy: 0.2848\n",
      "Epoch 18: saving model to training_1/\n",
      "276/276 [==============================] - 2s 7ms/step - loss: 2.6817 - accuracy: 0.2846 - val_loss: 2.9776 - val_accuracy: 0.2184 - lr: 3.1453e-04\n",
      "Epoch 19/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 2.6627 - accuracy: 0.2837\n",
      "Epoch 19: saving model to training_1/\n",
      "276/276 [==============================] - 14s 52ms/step - loss: 2.6627 - accuracy: 0.2837 - val_loss: 2.9731 - val_accuracy: 0.2146 - lr: 2.8460e-04\n",
      "Epoch 20/100\n",
      "272/276 [============================>.] - ETA: 0s - loss: 2.6483 - accuracy: 0.2858\n",
      "Epoch 20: saving model to training_1/\n",
      "276/276 [==============================] - 17s 61ms/step - loss: 2.6497 - accuracy: 0.2859 - val_loss: 2.9805 - val_accuracy: 0.2165 - lr: 2.5752e-04\n",
      "Epoch 21/100\n",
      "270/276 [============================>.] - ETA: 0s - loss: 2.6432 - accuracy: 0.2851\n",
      "Epoch 21: saving model to training_1/\n",
      "276/276 [==============================] - 2s 7ms/step - loss: 2.6438 - accuracy: 0.2851 - val_loss: 2.9790 - val_accuracy: 0.2155 - lr: 2.3301e-04\n",
      "Epoch 22/100\n",
      "269/276 [============================>.] - ETA: 0s - loss: 2.6283 - accuracy: 0.2923\n",
      "Epoch 22: saving model to training_1/\n",
      "276/276 [==============================] - 5s 16ms/step - loss: 2.6301 - accuracy: 0.2915 - val_loss: 2.9675 - val_accuracy: 0.2194 - lr: 2.1084e-04\n",
      "Epoch 23/100\n",
      "272/276 [============================>.] - ETA: 0s - loss: 2.6186 - accuracy: 0.2943\n",
      "Epoch 23: saving model to training_1/\n",
      "276/276 [==============================] - 5s 17ms/step - loss: 2.6205 - accuracy: 0.2943 - val_loss: 2.9685 - val_accuracy: 0.2175 - lr: 1.9077e-04\n",
      "Epoch 24/100\n",
      "275/276 [============================>.] - ETA: 0s - loss: 2.6095 - accuracy: 0.2994\n",
      "Epoch 24: saving model to training_1/\n",
      "276/276 [==============================] - 4s 14ms/step - loss: 2.6096 - accuracy: 0.2993 - val_loss: 2.9714 - val_accuracy: 0.2175 - lr: 1.7262e-04\n",
      "Epoch 25/100\n",
      "273/276 [============================>.] - ETA: 0s - loss: 2.6088 - accuracy: 0.2997\n",
      "Epoch 25: saving model to training_1/\n",
      "276/276 [==============================] - 4s 16ms/step - loss: 2.6098 - accuracy: 0.2991 - val_loss: 2.9666 - val_accuracy: 0.2117 - lr: 1.5619e-04\n",
      "Epoch 26/100\n",
      "272/276 [============================>.] - ETA: 0s - loss: 2.6057 - accuracy: 0.3018\n",
      "Epoch 26: saving model to training_1/\n",
      "276/276 [==============================] - 4s 15ms/step - loss: 2.6052 - accuracy: 0.3026 - val_loss: 2.9665 - val_accuracy: 0.2146 - lr: 1.4133e-04\n",
      "Epoch 27/100\n",
      "265/276 [===========================>..] - ETA: 0s - loss: 2.5967 - accuracy: 0.2992\n",
      "Epoch 27: saving model to training_1/\n",
      "276/276 [==============================] - 4s 15ms/step - loss: 2.5996 - accuracy: 0.2987 - val_loss: 2.9643 - val_accuracy: 0.2126 - lr: 1.2788e-04\n",
      "Epoch 28/100\n",
      "270/276 [============================>.] - ETA: 0s - loss: 2.5961 - accuracy: 0.2975\n",
      "Epoch 28: saving model to training_1/\n",
      "276/276 [==============================] - 4s 14ms/step - loss: 2.5982 - accuracy: 0.2973 - val_loss: 2.9616 - val_accuracy: 0.2097 - lr: 1.1571e-04\n",
      "Epoch 29/100\n",
      "268/276 [============================>.] - ETA: 0s - loss: 2.5898 - accuracy: 0.3032\n",
      "Epoch 29: saving model to training_1/\n",
      "276/276 [==============================] - 5s 18ms/step - loss: 2.5933 - accuracy: 0.3017 - val_loss: 2.9651 - val_accuracy: 0.2107 - lr: 1.0470e-04\n",
      "Epoch 30/100\n",
      "177/276 [==================>...........] - ETA: 7s - loss: 2.5470 - accuracy: 0.3133Epoch 1/100\n",
      "35/92 [==========>...................] - ETA: 4s - loss: 3.9653 - accuracy: 0.0212\n",
      "Epoch 30: saving model to training_1/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 17s 62ms/step - loss: 2.5912 - accuracy: 0.3038 - val_loss: 2.9629 - val_accuracy: 0.2097 - lr: 9.4735e-05\n",
      "37/92 [===========>..................] - ETA: 4s - loss: 3.9664 - accuracy: 0.0216Epoch 31/100\n",
      "133/276 [=============>................] - ETA: 20s - loss: 2.5281 - accuracy: 0.3126\n",
      "Epoch 1: saving model to training_1/\n",
      "92/92 [==============================] - 24s 251ms/step - loss: 4.0199 - accuracy: 0.0220 - val_loss: 4.1368 - val_accuracy: 0.0223 - lr: 9.0000e-04\n",
      "Epoch 2/100\n",
      "35/92 [==========>...................] - ETA: 26s - loss: 4.2708 - accuracy: 0.0143\n",
      "Epoch 31: saving model to training_1/\n",
      "276/276 [==============================] - 36s 130ms/step - loss: 2.5851 - accuracy: 0.3010 - val_loss: 2.9610 - val_accuracy: 0.2097 - lr: 8.5720e-05\n",
      "36/92 [==========>...................] - ETA: 25s - loss: 4.2724 - accuracy: 0.0150Epoch 32/100\n",
      "126/276 [============>.................] - ETA: 20s - loss: 2.5324 - accuracy: 0.3183\n",
      "Epoch 2: saving model to training_1/\n",
      "92/92 [==============================] - 34s 372ms/step - loss: 4.4499 - accuracy: 0.0151 - val_loss: 4.6289 - val_accuracy: 0.0194 - lr: 9.0000e-04\n",
      "Epoch 3/100\n",
      "41/92 [============>.................] - ETA: 23s - loss: 4.8811 - accuracy: 0.0159\n",
      "Epoch 32: saving model to training_1/\n",
      "276/276 [==============================] - 37s 134ms/step - loss: 2.5797 - accuracy: 0.3057 - val_loss: 2.9618 - val_accuracy: 0.2126 - lr: 7.7562e-05\n",
      "Epoch 33/100\n",
      "124/276 [============>.................] - ETA: 14s - loss: 2.5222 - accuracy: 0.3238\n",
      "Epoch 3: saving model to training_1/\n",
      "92/92 [==============================] - 31s 334ms/step - loss: 5.0294 - accuracy: 0.0161 - val_loss: 5.1318 - val_accuracy: 0.0194 - lr: 9.0000e-04\n",
      "Epoch 4/100\n",
      "47/92 [==============>...............] - ETA: 17s - loss: 5.4505 - accuracy: 0.0133\n",
      "Epoch 33: saving model to training_1/\n",
      "276/276 [==============================] - 31s 111ms/step - loss: 2.5776 - accuracy: 0.3093 - val_loss: 2.9603 - val_accuracy: 0.2117 - lr: 7.0181e-05\n",
      "Epoch 34/100\n",
      "121/276 [============>.................] - ETA: 24s - loss: 2.5322 - accuracy: 0.3178\n",
      "Epoch 4: saving model to training_1/\n",
      "92/92 [==============================] - 38s 410ms/step - loss: 5.5611 - accuracy: 0.0144 - val_loss: 5.5961 - val_accuracy: 0.0175 - lr: 9.0000e-04\n",
      "Epoch 5/100\n",
      "46/92 [==============>...............] - ETA: 13s - loss: 5.9462 - accuracy: 0.0163\n",
      "Epoch 34: saving model to training_1/\n",
      "276/276 [==============================] - 32s 117ms/step - loss: 2.5760 - accuracy: 0.3056 - val_loss: 2.9605 - val_accuracy: 0.2126 - lr: 6.3503e-05\n",
      "Epoch 35/100\n",
      "121/276 [============>.................] - ETA: 14s - loss: 2.5222 - accuracy: 0.3118\n",
      "Epoch 5: saving model to training_1/\n",
      "92/92 [==============================] - 25s 271ms/step - loss: 6.0474 - accuracy: 0.0156 - val_loss: 6.0396 - val_accuracy: 0.0175 - lr: 9.0000e-04\n",
      "Epoch 6/100\n",
      "49/92 [==============>...............] - ETA: 14s - loss: 6.4449 - accuracy: 0.0172\n",
      "Epoch 35: saving model to training_1/\n",
      "276/276 [==============================] - 28s 103ms/step - loss: 2.5773 - accuracy: 0.3044 - val_loss: 2.9611 - val_accuracy: 0.2126 - lr: 5.7460e-05\n",
      "Epoch 36/100\n",
      "135/276 [=============>................] - ETA: 13s - loss: 2.5276 - accuracy: 0.3217\n",
      "Epoch 6: saving model to training_1/\n",
      "92/92 [==============================] - 30s 323ms/step - loss: 6.5246 - accuracy: 0.0163 - val_loss: 6.4738 - val_accuracy: 0.0184 - lr: 9.0000e-04\n",
      "Epoch 7/100\n",
      "49/92 [==============>...............] - ETA: 10s - loss: 6.9472 - accuracy: 0.0155\n",
      "Epoch 36: saving model to training_1/\n",
      "276/276 [==============================] - 26s 93ms/step - loss: 2.5711 - accuracy: 0.3099 - val_loss: 2.9588 - val_accuracy: 0.2117 - lr: 5.1992e-05\n",
      "Epoch 37/100\n",
      "133/276 [=============>................] - ETA: 17s - loss: 2.5123 - accuracy: 0.3244\n",
      "Epoch 7: saving model to training_1/\n",
      "92/92 [==============================] - 30s 320ms/step - loss: 6.9896 - accuracy: 0.0170 - val_loss: 6.9011 - val_accuracy: 0.0165 - lr: 9.0000e-04\n",
      "Epoch 8/100\n",
      "51/92 [===============>..............] - ETA: 14s - loss: 7.3778 - accuracy: 0.0137\n",
      "Epoch 37: saving model to training_1/\n",
      "276/276 [==============================] - 36s 130ms/step - loss: 2.5692 - accuracy: 0.3121 - val_loss: 2.9590 - val_accuracy: 0.2126 - lr: 4.7044e-05\n",
      "Epoch 38/100\n",
      "127/276 [============>.................] - ETA: 17s - loss: 2.5189 - accuracy: 0.3220\n",
      "Epoch 8: saving model to training_1/\n",
      "92/92 [==============================] - 34s 368ms/step - loss: 7.3733 - accuracy: 0.0153 - val_loss: 7.0712 - val_accuracy: 0.0175 - lr: 9.0000e-04\n",
      "Epoch 9/100\n",
      "19/92 [=====>........................] - ETA: 28s - loss: 7.5613 - accuracy: 0.01454"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Ctrl + C detected. Stopping sweep.\n",
      "wandb: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/92 [=======>......................] - ETA: 25s - loss: 7.5004 - accuracy: 0.0157"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-1:\n",
      "Process SpawnPoolWorker-2:\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/fridadesigley/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "204/276 [=====================>........] - ETA: 8s - loss: 2.5460 - accuracy: 0.3146"
     ]
    }
   ],
   "source": [
    "main('sweep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ff9ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d2ithiyt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta_1: 0.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta_2: 0.9855\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon: 7.5e-08\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006000000000000001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamax\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrds\u001b[0m (\u001b[33mtiny-ml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230130_142518-d2ithiyt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/d2ithiyt\" target=\"_blank\">bumbling-sweep-1</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/d2ithiyt\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/d2ithiyt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 14:25:41.812236: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "106/107 [============================>.] - ETA: 0s - loss: 3.8661 - accuracy: 0.0346\n",
      "Epoch 1: saving model to training_1/\n",
      "107/107 [==============================] - 3s 26ms/step - loss: 3.8660 - accuracy: 0.0345 - val_loss: 3.7551 - val_accuracy: 0.0524 - lr: 6.0000e-04\n",
      "Epoch 2/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.7217 - accuracy: 0.0605\n",
      "Epoch 2: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.7199 - accuracy: 0.0615 - val_loss: 3.6813 - val_accuracy: 0.0709 - lr: 6.0000e-04\n",
      "Epoch 3/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.6553 - accuracy: 0.0745\n",
      "Epoch 3: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.6584 - accuracy: 0.0741 - val_loss: 3.6276 - val_accuracy: 0.0816 - lr: 6.0000e-04\n",
      "Epoch 4/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.5978 - accuracy: 0.0904\n",
      "Epoch 4: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.5994 - accuracy: 0.0906 - val_loss: 3.5718 - val_accuracy: 0.1000 - lr: 6.0000e-04\n",
      "Epoch 5/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.5390 - accuracy: 0.1057\n",
      "Epoch 5: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.5391 - accuracy: 0.1054 - val_loss: 3.5162 - val_accuracy: 0.1146 - lr: 6.0000e-04\n",
      "Epoch 6/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.4758 - accuracy: 0.1184\n",
      "Epoch 6: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.4812 - accuracy: 0.1169 - val_loss: 3.4611 - val_accuracy: 0.1223 - lr: 6.0000e-04\n",
      "Epoch 7/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.4274 - accuracy: 0.1304\n",
      "Epoch 7: saving model to training_1/\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 3.4266 - accuracy: 0.1307 - val_loss: 3.4159 - val_accuracy: 0.1311 - lr: 6.0000e-04\n",
      "Epoch 8/100\n",
      "100/107 [===========================>..] - ETA: 0s - loss: 3.3829 - accuracy: 0.1376\n",
      "Epoch 8: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.3849 - accuracy: 0.1378 - val_loss: 3.3745 - val_accuracy: 0.1369 - lr: 6.0000e-04\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - ETA: 0s - loss: 3.3389 - accuracy: 0.1485\n",
      "Epoch 9: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.3389 - accuracy: 0.1485 - val_loss: 3.3417 - val_accuracy: 0.1485 - lr: 6.0000e-04\n",
      "Epoch 10/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.2999 - accuracy: 0.1526\n",
      "Epoch 10: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.3024 - accuracy: 0.1520 - val_loss: 3.3145 - val_accuracy: 0.1524 - lr: 6.0000e-04\n",
      "Epoch 11/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.2759 - accuracy: 0.1613\n",
      "Epoch 11: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.2773 - accuracy: 0.1610 - val_loss: 3.2919 - val_accuracy: 0.1650 - lr: 5.4290e-04\n",
      "Epoch 12/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.2534 - accuracy: 0.1603\n",
      "Epoch 12: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.2546 - accuracy: 0.1602 - val_loss: 3.2746 - val_accuracy: 0.1621 - lr: 4.9124e-04\n",
      "Epoch 13/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.2323 - accuracy: 0.1673\n",
      "Epoch 13: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.2336 - accuracy: 0.1671 - val_loss: 3.2610 - val_accuracy: 0.1641 - lr: 4.4449e-04\n",
      "Epoch 14/100\n",
      "106/107 [============================>.] - ETA: 0s - loss: 3.2200 - accuracy: 0.1686\n",
      "Epoch 14: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.2199 - accuracy: 0.1687 - val_loss: 3.2469 - val_accuracy: 0.1699 - lr: 4.0219e-04\n",
      "Epoch 15/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.2024 - accuracy: 0.1734\n",
      "Epoch 15: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.2037 - accuracy: 0.1734 - val_loss: 3.2389 - val_accuracy: 0.1670 - lr: 3.6392e-04\n",
      "Epoch 16/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1871 - accuracy: 0.1770\n",
      "Epoch 16: saving model to training_1/\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 3.1958 - accuracy: 0.1740 - val_loss: 3.2285 - val_accuracy: 0.1738 - lr: 3.2929e-04\n",
      "Epoch 17/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.1833 - accuracy: 0.1829\n",
      "Epoch 17: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1851 - accuracy: 0.1819 - val_loss: 3.2238 - val_accuracy: 0.1718 - lr: 2.9795e-04\n",
      "Epoch 18/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1766 - accuracy: 0.1795\n",
      "Epoch 18: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1796 - accuracy: 0.1797 - val_loss: 3.2177 - val_accuracy: 0.1699 - lr: 2.6960e-04\n",
      "Epoch 19/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.1659 - accuracy: 0.1808\n",
      "Epoch 19: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1676 - accuracy: 0.1805 - val_loss: 3.2120 - val_accuracy: 0.1757 - lr: 2.4394e-04\n",
      "Epoch 20/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.1608 - accuracy: 0.1781\n",
      "Epoch 20: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1633 - accuracy: 0.1778 - val_loss: 3.2071 - val_accuracy: 0.1738 - lr: 2.2073e-04\n",
      "Epoch 21/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1473 - accuracy: 0.1851\n",
      "Epoch 21: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1522 - accuracy: 0.1833 - val_loss: 3.2043 - val_accuracy: 0.1767 - lr: 1.9972e-04\n",
      "Epoch 22/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1501 - accuracy: 0.1833\n",
      "Epoch 22: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1526 - accuracy: 0.1827 - val_loss: 3.1985 - val_accuracy: 0.1786 - lr: 1.8072e-04\n",
      "Epoch 23/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.1453 - accuracy: 0.1861\n",
      "Epoch 23: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1474 - accuracy: 0.1859 - val_loss: 3.1958 - val_accuracy: 0.1777 - lr: 1.6352e-04\n",
      "Epoch 24/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1406 - accuracy: 0.1856\n",
      "Epoch 24: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1445 - accuracy: 0.1848 - val_loss: 3.1928 - val_accuracy: 0.1777 - lr: 1.4796e-04\n",
      "Epoch 25/100\n",
      "106/107 [============================>.] - ETA: 0s - loss: 3.1380 - accuracy: 0.1888\n",
      "Epoch 25: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1380 - accuracy: 0.1886 - val_loss: 3.1908 - val_accuracy: 0.1786 - lr: 1.3388e-04\n",
      "Epoch 26/100\n",
      "100/107 [===========================>..] - ETA: 0s - loss: 3.1314 - accuracy: 0.1882\n",
      "Epoch 26: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1367 - accuracy: 0.1873 - val_loss: 3.1884 - val_accuracy: 0.1748 - lr: 1.2114e-04\n",
      "Epoch 27/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1250 - accuracy: 0.1867\n",
      "Epoch 27: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1310 - accuracy: 0.1860 - val_loss: 3.1865 - val_accuracy: 0.1796 - lr: 1.0961e-04\n",
      "Epoch 28/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1237 - accuracy: 0.1868\n",
      "Epoch 28: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1261 - accuracy: 0.1868 - val_loss: 3.1842 - val_accuracy: 0.1786 - lr: 9.9179e-05\n",
      "Epoch 29/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1276 - accuracy: 0.1935\n",
      "Epoch 29: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1308 - accuracy: 0.1930 - val_loss: 3.1828 - val_accuracy: 0.1796 - lr: 8.9741e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.1240 - accuracy: 0.1881\n",
      "Epoch 30: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1293 - accuracy: 0.1868 - val_loss: 3.1815 - val_accuracy: 0.1806 - lr: 8.1201e-05\n",
      "Epoch 31/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.1248 - accuracy: 0.1925\n",
      "Epoch 31: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1258 - accuracy: 0.1922 - val_loss: 3.1798 - val_accuracy: 0.1816 - lr: 7.3474e-05\n",
      "Epoch 32/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.1142 - accuracy: 0.1926\n",
      "Epoch 32: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1184 - accuracy: 0.1910 - val_loss: 3.1788 - val_accuracy: 0.1825 - lr: 6.6482e-05\n",
      "Epoch 33/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.1176 - accuracy: 0.1889\n",
      "Epoch 33: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1206 - accuracy: 0.1888 - val_loss: 3.1775 - val_accuracy: 0.1835 - lr: 6.0155e-05\n",
      "Epoch 34/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1121 - accuracy: 0.1950\n",
      "Epoch 34: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1187 - accuracy: 0.1938 - val_loss: 3.1767 - val_accuracy: 0.1835 - lr: 5.4431e-05\n",
      "Epoch 35/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.1174 - accuracy: 0.1878\n",
      "Epoch 35: saving model to training_1/\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 3.1211 - accuracy: 0.1874 - val_loss: 3.1759 - val_accuracy: 0.1845 - lr: 4.9251e-05\n",
      "Epoch 36/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1114 - accuracy: 0.1960\n",
      "Epoch 36: saving model to training_1/\n",
      "107/107 [==============================] - 4s 33ms/step - loss: 3.1143 - accuracy: 0.1961 - val_loss: 3.1749 - val_accuracy: 0.1845 - lr: 4.4564e-05\n",
      "Epoch 37/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.1152 - accuracy: 0.1933\n",
      "Epoch 37: saving model to training_1/\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 3.1209 - accuracy: 0.1936 - val_loss: 3.1745 - val_accuracy: 0.1835 - lr: 4.0323e-05\n",
      "Epoch 38/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.1132 - accuracy: 0.1927\n",
      "Epoch 38: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1146 - accuracy: 0.1933 - val_loss: 3.1740 - val_accuracy: 0.1835 - lr: 3.6486e-05\n",
      "Epoch 39/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.1113 - accuracy: 0.1921\n",
      "Epoch 39: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1119 - accuracy: 0.1921 - val_loss: 3.1735 - val_accuracy: 0.1845 - lr: 3.3014e-05\n",
      "Epoch 40/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.1131 - accuracy: 0.1917\n",
      "Epoch 40: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1154 - accuracy: 0.1913 - val_loss: 3.1731 - val_accuracy: 0.1845 - lr: 2.9872e-05\n",
      "Epoch 41/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1084 - accuracy: 0.1955\n",
      "Epoch 41: saving model to training_1/\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 3.1145 - accuracy: 0.1939 - val_loss: 3.1727 - val_accuracy: 0.1835 - lr: 2.7030e-05\n",
      "Epoch 42/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1120 - accuracy: 0.1930\n",
      "Epoch 42: saving model to training_1/\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 3.1130 - accuracy: 0.1930 - val_loss: 3.1724 - val_accuracy: 0.1825 - lr: 2.4457e-05\n",
      "Epoch 43/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.1080 - accuracy: 0.1944\n",
      "Epoch 43: saving model to training_1/\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 3.1086 - accuracy: 0.1943 - val_loss: 3.1720 - val_accuracy: 0.1825 - lr: 2.2130e-05\n",
      "Epoch 44/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1052 - accuracy: 0.1953\n",
      "Epoch 44: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1083 - accuracy: 0.1942 - val_loss: 3.1717 - val_accuracy: 0.1816 - lr: 2.0024e-05\n",
      "Epoch 45/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1065 - accuracy: 0.1927\n",
      "Epoch 45: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1104 - accuracy: 0.1917 - val_loss: 3.1716 - val_accuracy: 0.1806 - lr: 1.8118e-05\n",
      "Epoch 46/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.1106 - accuracy: 0.1911\n",
      "Epoch 46: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1114 - accuracy: 0.1920 - val_loss: 3.1713 - val_accuracy: 0.1806 - lr: 1.6394e-05\n",
      "Epoch 47/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1115 - accuracy: 0.1943\n",
      "Epoch 47: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1108 - accuracy: 0.1945 - val_loss: 3.1710 - val_accuracy: 0.1806 - lr: 1.4834e-05\n",
      "Epoch 48/100\n",
      "106/107 [============================>.] - ETA: 0s - loss: 3.1068 - accuracy: 0.1966\n",
      "Epoch 48: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1071 - accuracy: 0.1965 - val_loss: 3.1708 - val_accuracy: 0.1816 - lr: 1.3422e-05\n",
      "Epoch 49/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.1057 - accuracy: 0.1961\n",
      "Epoch 49: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1074 - accuracy: 0.1961 - val_loss: 3.1706 - val_accuracy: 0.1816 - lr: 1.2145e-05\n",
      "Epoch 50/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1080 - accuracy: 0.1951\n",
      "Epoch 50: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1100 - accuracy: 0.1944 - val_loss: 3.1704 - val_accuracy: 0.1816 - lr: 1.0989e-05\n",
      "Epoch 51/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.1042 - accuracy: 0.1954\n",
      "Epoch 51: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1062 - accuracy: 0.1952 - val_loss: 3.1703 - val_accuracy: 0.1806 - lr: 9.9436e-06\n",
      "Epoch 52/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1016 - accuracy: 0.1954\n",
      "Epoch 52: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1088 - accuracy: 0.1953 - val_loss: 3.1702 - val_accuracy: 0.1806 - lr: 8.9974e-06\n",
      "Epoch 53/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1067 - accuracy: 0.1975\n",
      "Epoch 53: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1122 - accuracy: 0.1969 - val_loss: 3.1701 - val_accuracy: 0.1816 - lr: 8.1411e-06\n",
      "Epoch 54/100\n",
      "106/107 [============================>.] - ETA: 0s - loss: 3.1079 - accuracy: 0.1962\n",
      "Epoch 54: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1080 - accuracy: 0.1962 - val_loss: 3.1700 - val_accuracy: 0.1806 - lr: 7.3664e-06\n",
      "Epoch 55/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.1029 - accuracy: 0.1932\n",
      "Epoch 55: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1069 - accuracy: 0.1928 - val_loss: 3.1699 - val_accuracy: 0.1806 - lr: 6.6654e-06\n",
      "Epoch 56/100\n",
      "106/107 [============================>.] - ETA: 0s - loss: 3.1045 - accuracy: 0.1927\n",
      "Epoch 56: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1047 - accuracy: 0.1925 - val_loss: 3.1698 - val_accuracy: 0.1806 - lr: 6.0311e-06\n",
      "Epoch 57/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.1009 - accuracy: 0.1975\n",
      "Epoch 57: saving model to training_1/\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 3.1038 - accuracy: 0.1965 - val_loss: 3.1697 - val_accuracy: 0.1806 - lr: 5.4572e-06\n",
      "Epoch 58/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1031 - accuracy: 0.1939\n",
      "Epoch 58: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1042 - accuracy: 0.1939 - val_loss: 3.1697 - val_accuracy: 0.1806 - lr: 4.9379e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "106/107 [============================>.] - ETA: 0s - loss: 3.1111 - accuracy: 0.1918\n",
      "Epoch 59: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1111 - accuracy: 0.1916 - val_loss: 3.1696 - val_accuracy: 0.1806 - lr: 4.4680e-06\n",
      "Epoch 60/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.1028 - accuracy: 0.1893\n",
      "Epoch 60: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1096 - accuracy: 0.1880 - val_loss: 3.1695 - val_accuracy: 0.1806 - lr: 4.0428e-06\n",
      "Epoch 61/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.1069 - accuracy: 0.1960\n",
      "Epoch 61: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1082 - accuracy: 0.1957 - val_loss: 3.1695 - val_accuracy: 0.1806 - lr: 3.6581e-06\n",
      "Epoch 62/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1028 - accuracy: 0.1943\n",
      "Epoch 62: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1066 - accuracy: 0.1938 - val_loss: 3.1694 - val_accuracy: 0.1806 - lr: 3.3099e-06\n",
      "Epoch 63/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.1090 - accuracy: 0.1923\n",
      "Epoch 63: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1087 - accuracy: 0.1924 - val_loss: 3.1693 - val_accuracy: 0.1816 - lr: 2.9950e-06\n",
      "Epoch 64/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1043 - accuracy: 0.1943\n",
      "Epoch 64: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1067 - accuracy: 0.1940 - val_loss: 3.1693 - val_accuracy: 0.1806 - lr: 2.7100e-06\n",
      "Epoch 65/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.0980 - accuracy: 0.1962\n",
      "Epoch 65: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1062 - accuracy: 0.1951 - val_loss: 3.1692 - val_accuracy: 0.1806 - lr: 2.4521e-06\n",
      "Epoch 66/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.0998 - accuracy: 0.1991\n",
      "Epoch 66: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1044 - accuracy: 0.1982 - val_loss: 3.1692 - val_accuracy: 0.1806 - lr: 2.2187e-06\n",
      "Epoch 67/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.1064 - accuracy: 0.1944\n",
      "Epoch 67: saving model to training_1/\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 3.1080 - accuracy: 0.1947 - val_loss: 3.1692 - val_accuracy: 0.1806 - lr: 2.0076e-06\n",
      "Epoch 68/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.1029 - accuracy: 0.1930\n",
      "Epoch 68: saving model to training_1/\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 3.1056 - accuracy: 0.1927 - val_loss: 3.1692 - val_accuracy: 0.1806 - lr: 1.8165e-06\n",
      "Epoch 69/100\n",
      "106/107 [============================>.] - ETA: 0s - loss: 3.1076 - accuracy: 0.1884\n",
      "Epoch 69: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1072 - accuracy: 0.1885 - val_loss: 3.1691 - val_accuracy: 0.1806 - lr: 1.6437e-06\n",
      "Epoch 70/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.0982 - accuracy: 0.1976\n",
      "Epoch 70: saving model to training_1/\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 3.1031 - accuracy: 0.1970 - val_loss: 3.1691 - val_accuracy: 0.1806 - lr: 1.4873e-06\n",
      "Epoch 71/100\n",
      "107/107 [==============================] - ETA: 0s - loss: 3.1046 - accuracy: 0.1945\n",
      "Epoch 71: saving model to training_1/\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 3.1046 - accuracy: 0.1945 - val_loss: 3.1691 - val_accuracy: 0.1806 - lr: 1.3457e-06\n",
      "Epoch 72/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.1039 - accuracy: 0.1947\n",
      "Epoch 72: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1076 - accuracy: 0.1942 - val_loss: 3.1691 - val_accuracy: 0.1806 - lr: 1.2177e-06\n",
      "Epoch 73/100\n",
      "106/107 [============================>.] - ETA: 0s - loss: 3.1071 - accuracy: 0.1938\n",
      "Epoch 73: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1073 - accuracy: 0.1939 - val_loss: 3.1691 - val_accuracy: 0.1806 - lr: 1.1018e-06\n",
      "Epoch 74/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.1015 - accuracy: 0.1920\n",
      "Epoch 74: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1032 - accuracy: 0.1923 - val_loss: 3.1691 - val_accuracy: 0.1806 - lr: 9.9694e-07\n",
      "Epoch 75/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1024 - accuracy: 0.1882\n",
      "Epoch 75: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1055 - accuracy: 0.1873 - val_loss: 3.1691 - val_accuracy: 0.1806 - lr: 9.0206e-07\n",
      "Epoch 76/100\n",
      "106/107 [============================>.] - ETA: 0s - loss: 3.1077 - accuracy: 0.1920\n",
      "Epoch 76: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1080 - accuracy: 0.1920 - val_loss: 3.1691 - val_accuracy: 0.1806 - lr: 8.1622e-07\n",
      "Epoch 77/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.1043 - accuracy: 0.1947\n",
      "Epoch 77: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1106 - accuracy: 0.1936 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 7.3855e-07\n",
      "Epoch 78/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.1095 - accuracy: 0.1918\n",
      "Epoch 78: saving model to training_1/\n",
      "107/107 [==============================] - 3s 24ms/step - loss: 3.1092 - accuracy: 0.1920 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 6.6827e-07\n",
      "Epoch 79/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.1058 - accuracy: 0.1949\n",
      "Epoch 79: saving model to training_1/\n",
      "107/107 [==============================] - 3s 28ms/step - loss: 3.1071 - accuracy: 0.1940 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 6.0467e-07\n",
      "Epoch 80/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.1034 - accuracy: 0.1970\n",
      "Epoch 80: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1075 - accuracy: 0.1962 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 5.4713e-07\n",
      "Epoch 81/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.1029 - accuracy: 0.1959\n",
      "Epoch 81: saving model to training_1/\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 3.1077 - accuracy: 0.1953 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 4.9506e-07\n",
      "Epoch 82/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1022 - accuracy: 0.1972\n",
      "Epoch 82: saving model to training_1/\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 3.1054 - accuracy: 0.1964 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 4.4795e-07\n",
      "Epoch 83/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.1073 - accuracy: 0.1927\n",
      "Epoch 83: saving model to training_1/\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 3.1100 - accuracy: 0.1918 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 4.0532e-07\n",
      "Epoch 84/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1055 - accuracy: 0.1979\n",
      "Epoch 84: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1079 - accuracy: 0.1972 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 3.6675e-07\n",
      "Epoch 85/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1003 - accuracy: 0.1937\n",
      "Epoch 85: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1075 - accuracy: 0.1915 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 3.3185e-07\n",
      "Epoch 86/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.0985 - accuracy: 0.1982\n",
      "Epoch 86: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1070 - accuracy: 0.1953 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 3.0027e-07\n",
      "Epoch 87/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.1081 - accuracy: 0.1932\n",
      "Epoch 87: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1080 - accuracy: 0.1933 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 2.7170e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.1057 - accuracy: 0.1946\n",
      "Epoch 88: saving model to training_1/\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 3.1067 - accuracy: 0.1948 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 2.4584e-07\n",
      "Epoch 89/100\n",
      "100/107 [===========================>..] - ETA: 0s - loss: 3.1023 - accuracy: 0.1939\n",
      "Epoch 89: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1086 - accuracy: 0.1930 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 2.2245e-07\n",
      "Epoch 90/100\n",
      "105/107 [============================>.] - ETA: 0s - loss: 3.1060 - accuracy: 0.1943\n",
      "Epoch 90: saving model to training_1/\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 3.1061 - accuracy: 0.1943 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 2.0128e-07\n",
      "Epoch 91/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.0976 - accuracy: 0.1967\n",
      "Epoch 91: saving model to training_1/\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 3.1015 - accuracy: 0.1957 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 1.8212e-07\n",
      "Epoch 92/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1059 - accuracy: 0.1948\n",
      "Epoch 92: saving model to training_1/\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 3.1076 - accuracy: 0.1947 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 1.6479e-07\n",
      "Epoch 93/100\n",
      "101/107 [===========================>..] - ETA: 0s - loss: 3.1007 - accuracy: 0.1955\n",
      "Epoch 93: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1027 - accuracy: 0.1956 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 1.4911e-07\n",
      "Epoch 94/100\n",
      "106/107 [============================>.] - ETA: 0s - loss: 3.1057 - accuracy: 0.1939\n",
      "Epoch 94: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1057 - accuracy: 0.1941 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 1.3492e-07\n",
      "Epoch 95/100\n",
      "106/107 [============================>.] - ETA: 0s - loss: 3.1079 - accuracy: 0.1983\n",
      "Epoch 95: saving model to training_1/\n",
      "107/107 [==============================] - 5s 46ms/step - loss: 3.1082 - accuracy: 0.1982 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 1.2208e-07\n",
      "Epoch 96/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1068 - accuracy: 0.1947\n",
      "Epoch 96: saving model to training_1/\n",
      "107/107 [==============================] - 3s 26ms/step - loss: 3.1068 - accuracy: 0.1942 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 1.1046e-07\n",
      "Epoch 97/100\n",
      "106/107 [============================>.] - ETA: 0s - loss: 3.1070 - accuracy: 0.1970\n",
      "Epoch 97: saving model to training_1/\n",
      "107/107 [==============================] - 2s 20ms/step - loss: 3.1071 - accuracy: 0.1971 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 9.9952e-08\n",
      "Epoch 98/100\n",
      "104/107 [============================>.] - ETA: 0s - loss: 3.1056 - accuracy: 0.1933\n",
      "Epoch 98: saving model to training_1/\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.1064 - accuracy: 0.1926 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 9.0440e-08\n",
      "Epoch 99/100\n",
      "103/107 [===========================>..] - ETA: 0s - loss: 3.1076 - accuracy: 0.1965\n",
      "Epoch 99: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1098 - accuracy: 0.1950 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 8.1833e-08\n",
      "Epoch 100/100\n",
      "102/107 [===========================>..] - ETA: 0s - loss: 3.1021 - accuracy: 0.1931\n",
      "Epoch 100: saving model to training_1/\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 3.1086 - accuracy: 0.1933 - val_loss: 3.1690 - val_accuracy: 0.1806 - lr: 7.4046e-08\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>â–â–ƒâ–…â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/loss</td><td>â–ˆâ–†â–„â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/lr</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/val_accuracy</td><td>â–â–ƒâ–…â–…â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch/val_loss</td><td>â–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.19326</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0</td></tr><tr><td>epoch/loss</td><td>3.10855</td></tr><tr><td>epoch/lr</td><td>0.0</td></tr><tr><td>epoch/val_accuracy</td><td>0.18058</td></tr><tr><td>epoch/val_loss</td><td>3.16896</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bumbling-sweep-1</strong> at: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/d2ithiyt\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/d2ithiyt</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230130_142518-d2ithiyt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gweszqk6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: selu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta_1: 1.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta_2: 0.984\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon: 6.25e-08\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230130_142812-gweszqk6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/gweszqk6\" target=\"_blank\">floral-sweep-2</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/gweszqk6\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/gweszqk6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "270/276 [============================>.] - ETA: 0s - loss: 3.9925 - accuracy: 0.0354\n",
      "Epoch 1: saving model to training_1/\n",
      "276/276 [==============================] - 4s 13ms/step - loss: 4.0021 - accuracy: 0.0354 - val_loss: 4.3978 - val_accuracy: 0.0272 - lr: 5.0000e-04\n",
      "Epoch 2/100\n",
      "271/276 [============================>.] - ETA: 0s - loss: 4.8182 - accuracy: 0.0338\n",
      "Epoch 2: saving model to training_1/\n",
      "276/276 [==============================] - 2s 6ms/step - loss: 4.8230 - accuracy: 0.0341 - val_loss: 5.5301 - val_accuracy: 0.0291 - lr: 5.0000e-04\n",
      "Epoch 3/100\n",
      "272/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0328\n",
      "Epoch 3: saving model to training_1/\n",
      "276/276 [==============================] - 2s 6ms/step - loss: nan - accuracy: 0.0328 - val_loss: nan - val_accuracy: 0.0262 - lr: 5.0000e-04\n",
      "Epoch 4/100\n",
      "264/276 [===========================>..] - ETA: 0s - loss: nan - accuracy: 0.0187\n",
      "Epoch 4: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 5.0000e-04\n",
      "Epoch 5/100\n",
      "268/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0187\n",
      "Epoch 5: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 5.0000e-04\n",
      "Epoch 6/100\n",
      "272/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0189\n",
      "Epoch 6: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 5.0000e-04\n",
      "Epoch 7/100\n",
      "270/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0186\n",
      "Epoch 7: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "267/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0189\n",
      "Epoch 8: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "267/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0185\n",
      "Epoch 9: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "271/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0185\n",
      "Epoch 10: saving model to training_1/\n",
      "276/276 [==============================] - 4s 13ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "264/276 [===========================>..] - ETA: 0s - loss: nan - accuracy: 0.0187\n",
      "Epoch 11: saving model to training_1/\n",
      "276/276 [==============================] - 3s 11ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 4.5242e-04\n",
      "Epoch 12/100\n",
      "273/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0186\n",
      "Epoch 12: saving model to training_1/\n",
      "276/276 [==============================] - 2s 6ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 4.0937e-04\n",
      "Epoch 13/100\n",
      "274/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0188\n",
      "Epoch 13: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 3.7041e-04\n",
      "Epoch 14/100\n",
      "271/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0187\n",
      "Epoch 14: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 3.3516e-04\n",
      "Epoch 15/100\n",
      "269/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0187\n",
      "Epoch 15: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 3.0327e-04\n",
      "Epoch 16/100\n",
      "268/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0187\n",
      "Epoch 16: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 2.7441e-04\n",
      "Epoch 17/100\n",
      "266/276 [===========================>..] - ETA: 0s - loss: nan - accuracy: 0.0183\n",
      "Epoch 17: saving model to training_1/\n",
      "276/276 [==============================] - 2s 6ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 2.4829e-04\n",
      "Epoch 18/100\n",
      "270/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0183\n",
      "Epoch 18: saving model to training_1/\n",
      "276/276 [==============================] - 2s 6ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 2.2466e-04\n",
      "Epoch 19/100\n",
      "268/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0186\n",
      "Epoch 19: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 2.0328e-04\n",
      "Epoch 20/100\n",
      "266/276 [===========================>..] - ETA: 0s - loss: nan - accuracy: 0.0184\n",
      "Epoch 20: saving model to training_1/\n",
      "276/276 [==============================] - 2s 6ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 1.8394e-04\n",
      "Epoch 21/100\n",
      "268/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0187\n",
      "Epoch 21: saving model to training_1/\n",
      "276/276 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 1.6644e-04\n",
      "Epoch 22/100\n",
      "268/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0185\n",
      "Epoch 22: saving model to training_1/\n",
      "276/276 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 1.5060e-04\n",
      "Epoch 23/100\n",
      "275/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0187\n",
      "Epoch 23: saving model to training_1/\n",
      "276/276 [==============================] - 2s 6ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 1.3627e-04\n",
      "Epoch 24/100\n",
      "275/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0187\n",
      "Epoch 24: saving model to training_1/\n",
      "276/276 [==============================] - 2s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 1.2330e-04\n",
      "Epoch 25/100\n",
      "272/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0187\n",
      "Epoch 25: saving model to training_1/\n",
      "276/276 [==============================] - 2s 7ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 1.1157e-04\n",
      "Epoch 26/100\n",
      "271/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0188\n",
      "Epoch 26: saving model to training_1/\n",
      "276/276 [==============================] - 2s 6ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 1.0095e-04\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>â–ˆâ–‡â–‡â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/epoch</td><td>â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>epoch/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–…â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>epoch/loss</td><td>â–â–ˆ                        </td></tr><tr><td>epoch/lr</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–…â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>epoch/val_accuracy</td><td>â–ƒâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/val_loss</td><td>â–â–ˆ                        </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.01867</td></tr><tr><td>epoch/epoch</td><td>25</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>nan</td></tr><tr><td>epoch/lr</td><td>0.0001</td></tr><tr><td>epoch/val_accuracy</td><td>0.02621</td></tr><tr><td>epoch/val_loss</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">floral-sweep-2</strong> at: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/gweszqk6\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/gweszqk6</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230130_142812-gweszqk6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s40qjd21 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: selu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta_1: 0.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta_2: 0.996\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon: 1.125e-07\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230130_142956-s40qjd21</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/s40qjd21\" target=\"_blank\">driven-sweep-3</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/s40qjd21\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/s40qjd21</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.9613 - accuracy: 0.0274\n",
      "Epoch 1: saving model to training_1/\n",
      "173/173 [==============================] - 4s 18ms/step - loss: 3.9608 - accuracy: 0.0275 - val_loss: 3.8974 - val_accuracy: 0.0252 - lr: 2.0000e-04\n",
      "Epoch 2/100\n",
      "169/173 [============================>.] - ETA: 0s - loss: 3.8595 - accuracy: 0.0388\n",
      "Epoch 2: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.8584 - accuracy: 0.0393 - val_loss: 3.8248 - val_accuracy: 0.0350 - lr: 2.0000e-04\n",
      "Epoch 3/100\n",
      "170/173 [============================>.] - ETA: 0s - loss: 3.7878 - accuracy: 0.0514\n",
      "Epoch 3: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.7874 - accuracy: 0.0512 - val_loss: 3.7611 - val_accuracy: 0.0398 - lr: 2.0000e-04\n",
      "Epoch 4/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.7254 - accuracy: 0.0600\n",
      "Epoch 4: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.7254 - accuracy: 0.0598 - val_loss: 3.7051 - val_accuracy: 0.0573 - lr: 2.0000e-04\n",
      "Epoch 5/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.6684 - accuracy: 0.0730\n",
      "Epoch 5: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.6682 - accuracy: 0.0732 - val_loss: 3.6586 - val_accuracy: 0.0641 - lr: 2.0000e-04\n",
      "Epoch 6/100\n",
      "169/173 [============================>.] - ETA: 0s - loss: 3.6198 - accuracy: 0.0819\n",
      "Epoch 6: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.6197 - accuracy: 0.0819 - val_loss: 3.6172 - val_accuracy: 0.0757 - lr: 2.0000e-04\n",
      "Epoch 7/100\n",
      "172/173 [============================>.] - ETA: 0s - loss: 3.5801 - accuracy: 0.0875\n",
      "Epoch 7: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.5798 - accuracy: 0.0875 - val_loss: 3.5821 - val_accuracy: 0.0806 - lr: 2.0000e-04\n",
      "Epoch 8/100\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 3.5426 - accuracy: 0.0979\n",
      "Epoch 8: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.5438 - accuracy: 0.0977 - val_loss: 3.5504 - val_accuracy: 0.0883 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.5067 - accuracy: 0.1003\n",
      "Epoch 9: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.5082 - accuracy: 0.1005 - val_loss: 3.5222 - val_accuracy: 0.0854 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.4728 - accuracy: 0.1057\n",
      "Epoch 10: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.4779 - accuracy: 0.1043 - val_loss: 3.4966 - val_accuracy: 0.0903 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "164/173 [===========================>..] - ETA: 0s - loss: 3.4471 - accuracy: 0.1185\n",
      "Epoch 11: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.4498 - accuracy: 0.1189 - val_loss: 3.4736 - val_accuracy: 0.0990 - lr: 1.8097e-04\n",
      "Epoch 12/100\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 3.4244 - accuracy: 0.1215\n",
      "Epoch 12: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.4257 - accuracy: 0.1227 - val_loss: 3.4556 - val_accuracy: 0.0990 - lr: 1.6375e-04\n",
      "Epoch 13/100\n",
      "169/173 [============================>.] - ETA: 0s - loss: 3.4037 - accuracy: 0.1215\n",
      "Epoch 13: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.4052 - accuracy: 0.1216 - val_loss: 3.4389 - val_accuracy: 0.1039 - lr: 1.4816e-04\n",
      "Epoch 14/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.3870 - accuracy: 0.1269\n",
      "Epoch 14: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.3874 - accuracy: 0.1265 - val_loss: 3.4253 - val_accuracy: 0.1078 - lr: 1.3406e-04\n",
      "Epoch 15/100\n",
      "168/173 [============================>.] - ETA: 0s - loss: 3.3693 - accuracy: 0.1301\n",
      "Epoch 15: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.3717 - accuracy: 0.1303 - val_loss: 3.4140 - val_accuracy: 0.1097 - lr: 1.2131e-04\n",
      "Epoch 16/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.3605 - accuracy: 0.1337\n",
      "Epoch 16: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.3615 - accuracy: 0.1334 - val_loss: 3.4033 - val_accuracy: 0.1146 - lr: 1.0976e-04\n",
      "Epoch 17/100\n",
      "172/173 [============================>.] - ETA: 0s - loss: 3.3487 - accuracy: 0.1367\n",
      "Epoch 17: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.3488 - accuracy: 0.1366 - val_loss: 3.3947 - val_accuracy: 0.1136 - lr: 9.9317e-05\n",
      "Epoch 18/100\n",
      "172/173 [============================>.] - ETA: 0s - loss: 3.3389 - accuracy: 0.1369\n",
      "Epoch 18: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.3396 - accuracy: 0.1369 - val_loss: 3.3872 - val_accuracy: 0.1175 - lr: 8.9866e-05\n",
      "Epoch 19/100\n",
      "168/173 [============================>.] - ETA: 0s - loss: 3.3292 - accuracy: 0.1370\n",
      "Epoch 19: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.3297 - accuracy: 0.1377 - val_loss: 3.3803 - val_accuracy: 0.1165 - lr: 8.1314e-05\n",
      "Epoch 20/100\n",
      "170/173 [============================>.] - ETA: 0s - loss: 3.3209 - accuracy: 0.1381\n",
      "Epoch 20: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.3225 - accuracy: 0.1386 - val_loss: 3.3745 - val_accuracy: 0.1184 - lr: 7.3576e-05\n",
      "Epoch 21/100\n",
      "173/173 [==============================] - ETA: 0s - loss: 3.3178 - accuracy: 0.1408\n",
      "Epoch 21: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.3178 - accuracy: 0.1408 - val_loss: 3.3697 - val_accuracy: 0.1165 - lr: 6.6574e-05\n",
      "Epoch 22/100\n",
      "168/173 [============================>.] - ETA: 0s - loss: 3.3080 - accuracy: 0.1416\n",
      "Epoch 22: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.3088 - accuracy: 0.1415 - val_loss: 3.3650 - val_accuracy: 0.1165 - lr: 6.0239e-05\n",
      "Epoch 23/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.2974 - accuracy: 0.1467\n",
      "Epoch 23: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.3025 - accuracy: 0.1466 - val_loss: 3.3609 - val_accuracy: 0.1165 - lr: 5.4506e-05\n",
      "Epoch 24/100\n",
      "173/173 [==============================] - ETA: 0s - loss: 3.2992 - accuracy: 0.1429\n",
      "Epoch 24: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2992 - accuracy: 0.1429 - val_loss: 3.3574 - val_accuracy: 0.1184 - lr: 4.9319e-05\n",
      "Epoch 25/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.2927 - accuracy: 0.1418\n",
      "Epoch 25: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2957 - accuracy: 0.1414 - val_loss: 3.3541 - val_accuracy: 0.1204 - lr: 4.4626e-05\n",
      "Epoch 26/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.2940 - accuracy: 0.1434\n",
      "Epoch 26: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2942 - accuracy: 0.1434 - val_loss: 3.3512 - val_accuracy: 0.1204 - lr: 4.0379e-05\n",
      "Epoch 27/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.2840 - accuracy: 0.1473\n",
      "Epoch 27: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2886 - accuracy: 0.1478 - val_loss: 3.3485 - val_accuracy: 0.1223 - lr: 3.6537e-05\n",
      "Epoch 28/100\n",
      "173/173 [==============================] - ETA: 0s - loss: 3.2852 - accuracy: 0.1438\n",
      "Epoch 28: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2852 - accuracy: 0.1438 - val_loss: 3.3462 - val_accuracy: 0.1223 - lr: 3.3060e-05\n",
      "Epoch 29/100\n",
      "170/173 [============================>.] - ETA: 0s - loss: 3.2814 - accuracy: 0.1460\n",
      "Epoch 29: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2832 - accuracy: 0.1457 - val_loss: 3.3442 - val_accuracy: 0.1214 - lr: 2.9914e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.2773 - accuracy: 0.1456\n",
      "Epoch 30: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2802 - accuracy: 0.1454 - val_loss: 3.3425 - val_accuracy: 0.1233 - lr: 2.7067e-05\n",
      "Epoch 31/100\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 3.2721 - accuracy: 0.1503\n",
      "Epoch 31: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2765 - accuracy: 0.1498 - val_loss: 3.3408 - val_accuracy: 0.1223 - lr: 2.4491e-05\n",
      "Epoch 32/100\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 3.2732 - accuracy: 0.1476\n",
      "Epoch 32: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2777 - accuracy: 0.1469 - val_loss: 3.3393 - val_accuracy: 0.1243 - lr: 2.2161e-05\n",
      "Epoch 33/100\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 3.2687 - accuracy: 0.1475\n",
      "Epoch 33: saving model to training_1/\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 3.2734 - accuracy: 0.1476 - val_loss: 3.3380 - val_accuracy: 0.1233 - lr: 2.0052e-05\n",
      "Epoch 34/100\n",
      "170/173 [============================>.] - ETA: 0s - loss: 3.2716 - accuracy: 0.1476\n",
      "Epoch 34: saving model to training_1/\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 3.2740 - accuracy: 0.1474 - val_loss: 3.3369 - val_accuracy: 0.1223 - lr: 1.8144e-05\n",
      "Epoch 35/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.2695 - accuracy: 0.1534\n",
      "Epoch 35: saving model to training_1/\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 3.2705 - accuracy: 0.1534 - val_loss: 3.3359 - val_accuracy: 0.1214 - lr: 1.6417e-05\n",
      "Epoch 36/100\n",
      "172/173 [============================>.] - ETA: 0s - loss: 3.2711 - accuracy: 0.1533\n",
      "Epoch 36: saving model to training_1/\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 3.2708 - accuracy: 0.1532 - val_loss: 3.3349 - val_accuracy: 0.1233 - lr: 1.4855e-05\n",
      "Epoch 37/100\n",
      "168/173 [============================>.] - ETA: 0s - loss: 3.2655 - accuracy: 0.1495\n",
      "Epoch 37: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2679 - accuracy: 0.1489 - val_loss: 3.3341 - val_accuracy: 0.1223 - lr: 1.3441e-05\n",
      "Epoch 38/100\n",
      "164/173 [===========================>..] - ETA: 0s - loss: 3.2629 - accuracy: 0.1487\n",
      "Epoch 38: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2683 - accuracy: 0.1478 - val_loss: 3.3335 - val_accuracy: 0.1223 - lr: 1.2162e-05\n",
      "Epoch 39/100\n",
      "173/173 [==============================] - ETA: 0s - loss: 3.2669 - accuracy: 0.1548\n",
      "Epoch 39: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2669 - accuracy: 0.1548 - val_loss: 3.3328 - val_accuracy: 0.1223 - lr: 1.1005e-05\n",
      "Epoch 40/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.2621 - accuracy: 0.1536\n",
      "Epoch 40: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2690 - accuracy: 0.1519 - val_loss: 3.3321 - val_accuracy: 0.1214 - lr: 9.9574e-06\n",
      "Epoch 41/100\n",
      "168/173 [============================>.] - ETA: 0s - loss: 3.2637 - accuracy: 0.1496\n",
      "Epoch 41: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2670 - accuracy: 0.1488 - val_loss: 3.3316 - val_accuracy: 0.1214 - lr: 9.0098e-06\n",
      "Epoch 42/100\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 3.2624 - accuracy: 0.1505\n",
      "Epoch 42: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2635 - accuracy: 0.1509 - val_loss: 3.3310 - val_accuracy: 0.1223 - lr: 8.1524e-06\n",
      "Epoch 43/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.2590 - accuracy: 0.1490\n",
      "Epoch 43: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2635 - accuracy: 0.1489 - val_loss: 3.3306 - val_accuracy: 0.1214 - lr: 7.3766e-06\n",
      "Epoch 44/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.2618 - accuracy: 0.1528\n",
      "Epoch 44: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2620 - accuracy: 0.1533 - val_loss: 3.3302 - val_accuracy: 0.1214 - lr: 6.6747e-06\n",
      "Epoch 45/100\n",
      "173/173 [==============================] - ETA: 0s - loss: 3.2644 - accuracy: 0.1510\n",
      "Epoch 45: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2644 - accuracy: 0.1510 - val_loss: 3.3299 - val_accuracy: 0.1214 - lr: 6.0395e-06\n",
      "Epoch 46/100\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 3.2622 - accuracy: 0.1483\n",
      "Epoch 46: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2637 - accuracy: 0.1488 - val_loss: 3.3295 - val_accuracy: 0.1214 - lr: 5.4647e-06\n",
      "Epoch 47/100\n",
      "172/173 [============================>.] - ETA: 0s - loss: 3.2621 - accuracy: 0.1522\n",
      "Epoch 47: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2620 - accuracy: 0.1524 - val_loss: 3.3292 - val_accuracy: 0.1214 - lr: 4.9447e-06\n",
      "Epoch 48/100\n",
      "168/173 [============================>.] - ETA: 0s - loss: 3.2641 - accuracy: 0.1524\n",
      "Epoch 48: saving model to training_1/\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 3.2630 - accuracy: 0.1521 - val_loss: 3.3289 - val_accuracy: 0.1214 - lr: 4.4742e-06\n",
      "Epoch 49/100\n",
      "172/173 [============================>.] - ETA: 0s - loss: 3.2603 - accuracy: 0.1523\n",
      "Epoch 49: saving model to training_1/\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 3.2604 - accuracy: 0.1526 - val_loss: 3.3287 - val_accuracy: 0.1214 - lr: 4.0484e-06\n",
      "Epoch 50/100\n",
      "172/173 [============================>.] - ETA: 0s - loss: 3.2634 - accuracy: 0.1483\n",
      "Epoch 50: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2629 - accuracy: 0.1482 - val_loss: 3.3284 - val_accuracy: 0.1214 - lr: 3.6631e-06\n",
      "Epoch 51/100\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 3.2582 - accuracy: 0.1561\n",
      "Epoch 51: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2596 - accuracy: 0.1556 - val_loss: 3.3282 - val_accuracy: 0.1223 - lr: 3.3145e-06\n",
      "Epoch 52/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.2563 - accuracy: 0.1541\n",
      "Epoch 52: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2573 - accuracy: 0.1538 - val_loss: 3.3280 - val_accuracy: 0.1223 - lr: 2.9991e-06\n",
      "Epoch 53/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.2602 - accuracy: 0.1527\n",
      "Epoch 53: saving model to training_1/\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 3.2610 - accuracy: 0.1521 - val_loss: 3.3279 - val_accuracy: 0.1223 - lr: 2.7137e-06\n",
      "Epoch 54/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.2613 - accuracy: 0.1495\n",
      "Epoch 54: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2620 - accuracy: 0.1493 - val_loss: 3.3277 - val_accuracy: 0.1223 - lr: 2.4555e-06\n",
      "Epoch 55/100\n",
      "168/173 [============================>.] - ETA: 0s - loss: 3.2577 - accuracy: 0.1547\n",
      "Epoch 55: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2587 - accuracy: 0.1550 - val_loss: 3.3276 - val_accuracy: 0.1223 - lr: 2.2218e-06\n",
      "Epoch 56/100\n",
      "169/173 [============================>.] - ETA: 0s - loss: 3.2578 - accuracy: 0.1524\n",
      "Epoch 56: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2607 - accuracy: 0.1525 - val_loss: 3.3275 - val_accuracy: 0.1223 - lr: 2.0104e-06\n",
      "Epoch 57/100\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 3.2549 - accuracy: 0.1530\n",
      "Epoch 57: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2579 - accuracy: 0.1526 - val_loss: 3.3274 - val_accuracy: 0.1223 - lr: 1.8191e-06\n",
      "Epoch 58/100\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 3.2555 - accuracy: 0.1547\n",
      "Epoch 58: saving model to training_1/\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 3.2593 - accuracy: 0.1532 - val_loss: 3.3273 - val_accuracy: 0.1223 - lr: 1.6460e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "164/173 [===========================>..] - ETA: 0s - loss: 3.2554 - accuracy: 0.1557\n",
      "Epoch 59: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2587 - accuracy: 0.1543 - val_loss: 3.3272 - val_accuracy: 0.1223 - lr: 1.4893e-06\n",
      "Epoch 60/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.2526 - accuracy: 0.1563\n",
      "Epoch 60: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2582 - accuracy: 0.1554 - val_loss: 3.3271 - val_accuracy: 0.1223 - lr: 1.3476e-06\n",
      "Epoch 61/100\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 3.2574 - accuracy: 0.1525\n",
      "Epoch 61: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2585 - accuracy: 0.1525 - val_loss: 3.3270 - val_accuracy: 0.1223 - lr: 1.2194e-06\n",
      "Epoch 62/100\n",
      "168/173 [============================>.] - ETA: 0s - loss: 3.2585 - accuracy: 0.1512\n",
      "Epoch 62: saving model to training_1/\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 3.2602 - accuracy: 0.1512 - val_loss: 3.3270 - val_accuracy: 0.1223 - lr: 1.1033e-06\n",
      "Epoch 63/100\n",
      "168/173 [============================>.] - ETA: 0s - loss: 3.2565 - accuracy: 0.1529\n",
      "Epoch 63: saving model to training_1/\n",
      "173/173 [==============================] - 2s 12ms/step - loss: 3.2593 - accuracy: 0.1529 - val_loss: 3.3269 - val_accuracy: 0.1223 - lr: 9.9832e-07\n",
      "Epoch 64/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.2585 - accuracy: 0.1541\n",
      "Epoch 64: saving model to training_1/\n",
      "173/173 [==============================] - 3s 18ms/step - loss: 3.2591 - accuracy: 0.1545 - val_loss: 3.3268 - val_accuracy: 0.1223 - lr: 9.0332e-07\n",
      "Epoch 65/100\n",
      "170/173 [============================>.] - ETA: 0s - loss: 3.2549 - accuracy: 0.1534\n",
      "Epoch 65: saving model to training_1/\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 3.2576 - accuracy: 0.1528 - val_loss: 3.3268 - val_accuracy: 0.1223 - lr: 8.1735e-07\n",
      "Epoch 66/100\n",
      "170/173 [============================>.] - ETA: 0s - loss: 3.2575 - accuracy: 0.1582\n",
      "Epoch 66: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2578 - accuracy: 0.1575 - val_loss: 3.3267 - val_accuracy: 0.1223 - lr: 7.3957e-07\n",
      "Epoch 67/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.2596 - accuracy: 0.1525\n",
      "Epoch 67: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2609 - accuracy: 0.1521 - val_loss: 3.3267 - val_accuracy: 0.1223 - lr: 6.6919e-07\n",
      "Epoch 68/100\n",
      "169/173 [============================>.] - ETA: 0s - loss: 3.2567 - accuracy: 0.1506\n",
      "Epoch 68: saving model to training_1/\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 3.2608 - accuracy: 0.1497 - val_loss: 3.3267 - val_accuracy: 0.1223 - lr: 6.0551e-07\n",
      "Epoch 69/100\n",
      "170/173 [============================>.] - ETA: 0s - loss: 3.2596 - accuracy: 0.1538\n",
      "Epoch 69: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2607 - accuracy: 0.1542 - val_loss: 3.3266 - val_accuracy: 0.1223 - lr: 5.4789e-07\n",
      "Epoch 70/100\n",
      "168/173 [============================>.] - ETA: 0s - loss: 3.2559 - accuracy: 0.1523\n",
      "Epoch 70: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2580 - accuracy: 0.1526 - val_loss: 3.3266 - val_accuracy: 0.1223 - lr: 4.9575e-07\n",
      "Epoch 71/100\n",
      "173/173 [==============================] - ETA: 0s - loss: 3.2586 - accuracy: 0.1516\n",
      "Epoch 71: saving model to training_1/\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 3.2586 - accuracy: 0.1516 - val_loss: 3.3266 - val_accuracy: 0.1223 - lr: 4.4857e-07\n",
      "Epoch 72/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.2584 - accuracy: 0.1540\n",
      "Epoch 72: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2592 - accuracy: 0.1536 - val_loss: 3.3266 - val_accuracy: 0.1223 - lr: 4.0589e-07\n",
      "Epoch 73/100\n",
      "168/173 [============================>.] - ETA: 0s - loss: 3.2562 - accuracy: 0.1532\n",
      "Epoch 73: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2581 - accuracy: 0.1528 - val_loss: 3.3265 - val_accuracy: 0.1223 - lr: 3.6726e-07\n",
      "Epoch 74/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.2537 - accuracy: 0.1524\n",
      "Epoch 74: saving model to training_1/\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 3.2566 - accuracy: 0.1516 - val_loss: 3.3265 - val_accuracy: 0.1223 - lr: 3.3231e-07\n",
      "Epoch 75/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.2573 - accuracy: 0.1495\n",
      "Epoch 75: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2575 - accuracy: 0.1495 - val_loss: 3.3265 - val_accuracy: 0.1223 - lr: 3.0069e-07\n",
      "Epoch 76/100\n",
      "163/173 [===========================>..] - ETA: 0s - loss: 3.2524 - accuracy: 0.1519\n",
      "Epoch 76: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2578 - accuracy: 0.1506 - val_loss: 3.3265 - val_accuracy: 0.1223 - lr: 2.7207e-07\n",
      "Epoch 77/100\n",
      "164/173 [===========================>..] - ETA: 0s - loss: 3.2507 - accuracy: 0.1516\n",
      "Epoch 77: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2556 - accuracy: 0.1518 - val_loss: 3.3265 - val_accuracy: 0.1223 - lr: 2.4618e-07\n",
      "Epoch 78/100\n",
      "170/173 [============================>.] - ETA: 0s - loss: 3.2582 - accuracy: 0.1534\n",
      "Epoch 78: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2573 - accuracy: 0.1536 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 2.2276e-07\n",
      "Epoch 79/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.2568 - accuracy: 0.1502\n",
      "Epoch 79: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2585 - accuracy: 0.1501 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 2.0156e-07\n",
      "Epoch 80/100\n",
      "163/173 [===========================>..] - ETA: 0s - loss: 3.2565 - accuracy: 0.1519\n",
      "Epoch 80: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2603 - accuracy: 0.1512 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 1.8238e-07\n",
      "Epoch 81/100\n",
      "168/173 [============================>.] - ETA: 0s - loss: 3.2561 - accuracy: 0.1515\n",
      "Epoch 81: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2573 - accuracy: 0.1516 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 1.6502e-07\n",
      "Epoch 82/100\n",
      "169/173 [============================>.] - ETA: 0s - loss: 3.2560 - accuracy: 0.1530\n",
      "Epoch 82: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2575 - accuracy: 0.1527 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 1.4932e-07\n",
      "Epoch 83/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.2553 - accuracy: 0.1509\n",
      "Epoch 83: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2583 - accuracy: 0.1510 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 1.3511e-07\n",
      "Epoch 84/100\n",
      "167/173 [===========================>..] - ETA: 0s - loss: 3.2547 - accuracy: 0.1545\n",
      "Epoch 84: saving model to training_1/\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 3.2587 - accuracy: 0.1533 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 1.2225e-07\n",
      "Epoch 85/100\n",
      "170/173 [============================>.] - ETA: 0s - loss: 3.2590 - accuracy: 0.1501\n",
      "Epoch 85: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2609 - accuracy: 0.1501 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 1.1062e-07\n",
      "Epoch 86/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.2589 - accuracy: 0.1525\n",
      "Epoch 86: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2591 - accuracy: 0.1526 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 1.0009e-07\n",
      "Epoch 87/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.2566 - accuracy: 0.1524\n",
      "Epoch 87: saving model to training_1/\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 3.2578 - accuracy: 0.1524 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 9.0566e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "165/173 [===========================>..] - ETA: 0s - loss: 3.2532 - accuracy: 0.1530\n",
      "Epoch 88: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2596 - accuracy: 0.1521 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 8.1947e-08\n",
      "Epoch 89/100\n",
      "170/173 [============================>.] - ETA: 0s - loss: 3.2573 - accuracy: 0.1510\n",
      "Epoch 89: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2585 - accuracy: 0.1510 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 7.4149e-08\n",
      "Epoch 90/100\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 3.2588 - accuracy: 0.1520\n",
      "Epoch 90: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2601 - accuracy: 0.1526 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 6.7093e-08\n",
      "Epoch 91/100\n",
      "170/173 [============================>.] - ETA: 0s - loss: 3.2574 - accuracy: 0.1528\n",
      "Epoch 91: saving model to training_1/\n",
      "173/173 [==============================] - 2s 11ms/step - loss: 3.2586 - accuracy: 0.1529 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 6.0708e-08\n",
      "Epoch 92/100\n",
      "169/173 [============================>.] - ETA: 0s - loss: 3.2546 - accuracy: 0.1536\n",
      "Epoch 92: saving model to training_1/\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 3.2555 - accuracy: 0.1534 - val_loss: 3.3264 - val_accuracy: 0.1223 - lr: 5.4931e-08\n",
      "Epoch 93/100\n",
      "168/173 [============================>.] - ETA: 0s - loss: 3.2605 - accuracy: 0.1523\n",
      "Epoch 93: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2589 - accuracy: 0.1528 - val_loss: 3.3263 - val_accuracy: 0.1223 - lr: 4.9703e-08\n",
      "Epoch 94/100\n",
      "170/173 [============================>.] - ETA: 0s - loss: 3.2559 - accuracy: 0.1526\n",
      "Epoch 94: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2567 - accuracy: 0.1527 - val_loss: 3.3263 - val_accuracy: 0.1223 - lr: 4.4974e-08\n",
      "Epoch 95/100\n",
      "166/173 [===========================>..] - ETA: 0s - loss: 3.2518 - accuracy: 0.1550\n",
      "Epoch 95: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2561 - accuracy: 0.1545 - val_loss: 3.3263 - val_accuracy: 0.1223 - lr: 4.0694e-08\n",
      "Epoch 96/100\n",
      "169/173 [============================>.] - ETA: 0s - loss: 3.2576 - accuracy: 0.1471\n",
      "Epoch 96: saving model to training_1/\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 3.2579 - accuracy: 0.1478 - val_loss: 3.3263 - val_accuracy: 0.1223 - lr: 3.6821e-08\n",
      "Epoch 97/100\n",
      "171/173 [============================>.] - ETA: 0s - loss: 3.2555 - accuracy: 0.1550\n",
      "Epoch 97: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2560 - accuracy: 0.1546 - val_loss: 3.3263 - val_accuracy: 0.1223 - lr: 3.3317e-08\n",
      "Epoch 98/100\n",
      "173/173 [==============================] - ETA: 0s - loss: 3.2577 - accuracy: 0.1538\n",
      "Epoch 98: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2577 - accuracy: 0.1538 - val_loss: 3.3263 - val_accuracy: 0.1223 - lr: 3.0147e-08\n",
      "Epoch 99/100\n",
      "164/173 [===========================>..] - ETA: 0s - loss: 3.2534 - accuracy: 0.1550\n",
      "Epoch 99: saving model to training_1/\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 3.2575 - accuracy: 0.1546 - val_loss: 3.3263 - val_accuracy: 0.1223 - lr: 2.7278e-08\n",
      "Epoch 100/100\n",
      "170/173 [============================>.] - ETA: 0s - loss: 3.2540 - accuracy: 0.1515\n",
      "Epoch 100: saving model to training_1/\n",
      "173/173 [==============================] - 2s 13ms/step - loss: 3.2575 - accuracy: 0.1508 - val_loss: 3.3263 - val_accuracy: 0.1223 - lr: 2.4682e-08\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>â–â–‚â–„â–…â–†â–†â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/loss</td><td>â–ˆâ–†â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/lr</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/val_accuracy</td><td>â–â–‚â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch/val_loss</td><td>â–ˆâ–†â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.15083</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0</td></tr><tr><td>epoch/loss</td><td>3.25753</td></tr><tr><td>epoch/lr</td><td>0.0</td></tr><tr><td>epoch/val_accuracy</td><td>0.12233</td></tr><tr><td>epoch/val_loss</td><td>3.32634</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">driven-sweep-3</strong> at: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/s40qjd21\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/s40qjd21</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230130_142956-s40qjd21/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i7qq8l8a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta_1: 1.0125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta_2: 0.9915\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.02\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon: 1e-07\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230130_143253-i7qq8l8a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/i7qq8l8a\" target=\"_blank\">olive-sweep-4</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/i7qq8l8a\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/i7qq8l8a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "96/99 [============================>.] - ETA: 0s - loss: 4.0198 - accuracy: 0.0202\n",
      "Epoch 1: saving model to training_1/\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 4.0188 - accuracy: 0.0203 - val_loss: 3.9591 - val_accuracy: 0.0243 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "94/99 [===========================>..] - ETA: 0s - loss: 3.9311 - accuracy: 0.0263\n",
      "Epoch 2: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 3.9293 - accuracy: 0.0262 - val_loss: 3.8935 - val_accuracy: 0.0262 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "95/99 [===========================>..] - ETA: 0s - loss: 3.8718 - accuracy: 0.0325\n",
      "Epoch 3: saving model to training_1/\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 3.8708 - accuracy: 0.0325 - val_loss: 3.8544 - val_accuracy: 0.0330 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "97/99 [============================>.] - ETA: 0s - loss: 3.8345 - accuracy: 0.0399\n",
      "Epoch 4: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 3.8348 - accuracy: 0.0402 - val_loss: 3.8351 - val_accuracy: 0.0340 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "97/99 [============================>.] - ETA: 0s - loss: 3.8181 - accuracy: 0.0436\n",
      "Epoch 5: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 3.8179 - accuracy: 0.0436 - val_loss: 3.8308 - val_accuracy: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "98/99 [============================>.] - ETA: 0s - loss: 3.8131 - accuracy: 0.0483\n",
      "Epoch 6: saving model to training_1/\n",
      "99/99 [==============================] - 1s 11ms/step - loss: 3.8133 - accuracy: 0.0481 - val_loss: 3.8387 - val_accuracy: 0.0524 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "97/99 [============================>.] - ETA: 0s - loss: 3.8196 - accuracy: 0.0503\n",
      "Epoch 7: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 3.8196 - accuracy: 0.0502 - val_loss: 3.8555 - val_accuracy: 0.0505 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "97/99 [============================>.] - ETA: 0s - loss: 3.8356 - accuracy: 0.0518\n",
      "Epoch 8: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 3.8354 - accuracy: 0.0520 - val_loss: 3.8799 - val_accuracy: 0.0524 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "93/99 [===========================>..] - ETA: 0s - loss: 3.8541 - accuracy: 0.0494\n",
      "Epoch 9: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 3.8563 - accuracy: 0.0494 - val_loss: 3.9092 - val_accuracy: 0.0534 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "93/99 [===========================>..] - ETA: 0s - loss: 3.8807 - accuracy: 0.0484\n",
      "Epoch 10: saving model to training_1/\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 3.8817 - accuracy: 0.0479 - val_loss: 3.9409 - val_accuracy: 0.0524 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "94/99 [===========================>..] - ETA: 0s - loss: 3.9031 - accuracy: 0.0476\n",
      "Epoch 11: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 3.9077 - accuracy: 0.0469 - val_loss: 3.9710 - val_accuracy: 0.0505 - lr: 9.0484e-05\n",
      "Epoch 12/100\n",
      "96/99 [============================>.] - ETA: 0s - loss: 3.9331 - accuracy: 0.0468\n",
      "Epoch 12: saving model to training_1/\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 3.9325 - accuracy: 0.0473 - val_loss: 3.9999 - val_accuracy: 0.0476 - lr: 8.1873e-05\n",
      "Epoch 13/100\n",
      "93/99 [===========================>..] - ETA: 0s - loss: 3.9543 - accuracy: 0.0441\n",
      "Epoch 13: saving model to training_1/\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 3.9560 - accuracy: 0.0445 - val_loss: 4.0265 - val_accuracy: 0.0466 - lr: 7.4082e-05\n",
      "Epoch 14/100\n",
      "94/99 [===========================>..] - ETA: 0s - loss: 3.9793 - accuracy: 0.0440\n",
      "Epoch 14: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 3.9788 - accuracy: 0.0441 - val_loss: 4.0517 - val_accuracy: 0.0485 - lr: 6.7032e-05\n",
      "Epoch 15/100\n",
      "97/99 [============================>.] - ETA: 0s - loss: 3.9974 - accuracy: 0.0441\n",
      "Epoch 15: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 3.9986 - accuracy: 0.0441 - val_loss: 4.0750 - val_accuracy: 0.0495 - lr: 6.0653e-05\n",
      "Epoch 16/100\n",
      "97/99 [============================>.] - ETA: 0s - loss: 4.0176 - accuracy: 0.0425\n",
      "Epoch 16: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 4.0190 - accuracy: 0.0422 - val_loss: 4.0964 - val_accuracy: 0.0485 - lr: 5.4881e-05\n",
      "Epoch 17/100\n",
      "98/99 [============================>.] - ETA: 0s - loss: 4.0353 - accuracy: 0.0444\n",
      "Epoch 17: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 4.0362 - accuracy: 0.0443 - val_loss: 4.1163 - val_accuracy: 0.0485 - lr: 4.9659e-05\n",
      "Epoch 18/100\n",
      "93/99 [===========================>..] - ETA: 0s - loss: 4.0504 - accuracy: 0.0425\n",
      "Epoch 18: saving model to training_1/\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 4.0529 - accuracy: 0.0426 - val_loss: 4.1341 - val_accuracy: 0.0476 - lr: 4.4933e-05\n",
      "Epoch 19/100\n",
      "97/99 [============================>.] - ETA: 0s - loss: 4.0694 - accuracy: 0.0434\n",
      "Epoch 19: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 4.0681 - accuracy: 0.0431 - val_loss: 4.1510 - val_accuracy: 0.0476 - lr: 4.0657e-05\n",
      "Epoch 20/100\n",
      "97/99 [============================>.] - ETA: 0s - loss: 4.0817 - accuracy: 0.0429\n",
      "Epoch 20: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 4.0821 - accuracy: 0.0424 - val_loss: 4.1660 - val_accuracy: 0.0485 - lr: 3.6788e-05\n",
      "Epoch 21/100\n",
      "93/99 [===========================>..] - ETA: 0s - loss: 4.0946 - accuracy: 0.0438\n",
      "Epoch 21: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 4.0944 - accuracy: 0.0431 - val_loss: 4.1798 - val_accuracy: 0.0485 - lr: 3.3287e-05\n",
      "Epoch 22/100\n",
      "96/99 [============================>.] - ETA: 0s - loss: 4.1069 - accuracy: 0.0438\n",
      "Epoch 22: saving model to training_1/\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 4.1062 - accuracy: 0.0433 - val_loss: 4.1923 - val_accuracy: 0.0476 - lr: 3.0119e-05\n",
      "Epoch 23/100\n",
      "97/99 [============================>.] - ETA: 0s - loss: 4.1182 - accuracy: 0.0429\n",
      "Epoch 23: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 4.1166 - accuracy: 0.0432 - val_loss: 4.2036 - val_accuracy: 0.0466 - lr: 2.7253e-05\n",
      "Epoch 24/100\n",
      "98/99 [============================>.] - ETA: 0s - loss: 4.1252 - accuracy: 0.0435\n",
      "Epoch 24: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 4.1260 - accuracy: 0.0434 - val_loss: 4.2140 - val_accuracy: 0.0466 - lr: 2.4660e-05\n",
      "Epoch 25/100\n",
      "98/99 [============================>.] - ETA: 0s - loss: 4.1342 - accuracy: 0.0430\n",
      "Epoch 25: saving model to training_1/\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 4.1345 - accuracy: 0.0430 - val_loss: 4.2235 - val_accuracy: 0.0466 - lr: 2.2313e-05\n",
      "Epoch 26/100\n",
      "98/99 [============================>.] - ETA: 0s - loss: 4.1409 - accuracy: 0.0426\n",
      "Epoch 26: saving model to training_1/\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 4.1412 - accuracy: 0.0427 - val_loss: 4.2321 - val_accuracy: 0.0466 - lr: 2.0190e-05\n",
      "Epoch 27/100\n",
      "98/99 [============================>.] - ETA: 0s - loss: 4.1499 - accuracy: 0.0446\n",
      "Epoch 27: saving model to training_1/\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 4.1498 - accuracy: 0.0445 - val_loss: 4.2399 - val_accuracy: 0.0466 - lr: 1.8268e-05\n",
      "Epoch 28/100\n",
      "95/99 [===========================>..] - ETA: 0s - loss: 4.1598 - accuracy: 0.0429\n",
      "Epoch 28: saving model to training_1/\n",
      "99/99 [==============================] - 1s 11ms/step - loss: 4.1570 - accuracy: 0.0427 - val_loss: 4.2471 - val_accuracy: 0.0466 - lr: 1.6530e-05\n",
      "Epoch 29/100\n",
      "94/99 [===========================>..] - ETA: 0s - loss: 4.1610 - accuracy: 0.0436\n",
      "Epoch 29: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 4.1615 - accuracy: 0.0437 - val_loss: 4.2535 - val_accuracy: 0.0466 - lr: 1.4957e-05\n",
      "Epoch 30/100\n",
      "95/99 [===========================>..] - ETA: 0s - loss: 4.1670 - accuracy: 0.0428\n",
      "Epoch 30: saving model to training_1/\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 4.1676 - accuracy: 0.0429 - val_loss: 4.2593 - val_accuracy: 0.0466 - lr: 1.3534e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>â–â–‚â–„â–…â–†â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†</td></tr><tr><td>epoch/epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–</td></tr><tr><td>epoch/loss</td><td>â–…â–ƒâ–‚â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch/lr</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–</td></tr><tr><td>epoch/val_accuracy</td><td>â–â–â–ƒâ–ƒâ–…â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–†</td></tr><tr><td>epoch/val_loss</td><td>â–ƒâ–‚â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.04288</td></tr><tr><td>epoch/epoch</td><td>29</td></tr><tr><td>epoch/learning_rate</td><td>1e-05</td></tr><tr><td>epoch/loss</td><td>4.16764</td></tr><tr><td>epoch/lr</td><td>1e-05</td></tr><tr><td>epoch/val_accuracy</td><td>0.0466</td></tr><tr><td>epoch/val_loss</td><td>4.25933</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">olive-sweep-4</strong> at: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/i7qq8l8a\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/i7qq8l8a</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230130_143253-i7qq8l8a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0xvabav0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta_1: 1.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta_2: 0.984\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon: 1.125e-07\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fridadesigley/pico/tiny-ml/wandb/run-20230130_143432-0xvabav0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/0xvabav0\" target=\"_blank\">sunny-sweep-5</a></strong> to <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/sweeps/zzno0kjd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/tiny-ml/wake_word_detection/runs/0xvabav0\" target=\"_blank\">https://wandb.ai/tiny-ml/wake_word_detection/runs/0xvabav0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "268/276 [============================>.] - ETA: 0s - loss: 4.1061 - accuracy: 0.0207\n",
      "Epoch 1: saving model to training_1/\n",
      "276/276 [==============================] - 4s 14ms/step - loss: 4.1146 - accuracy: 0.0204 - val_loss: 4.3465 - val_accuracy: 0.0165 - lr: 8.0000e-04\n",
      "Epoch 2/100\n",
      "270/276 [============================>.] - ETA: 0s - loss: 4.6775 - accuracy: 0.0197\n",
      "Epoch 2: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: 4.6809 - accuracy: 0.0197 - val_loss: 4.8342 - val_accuracy: 0.0194 - lr: 8.0000e-04\n",
      "Epoch 3/100\n",
      "268/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0229\n",
      "Epoch 3: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0229 - val_loss: nan - val_accuracy: 0.0262 - lr: 8.0000e-04\n",
      "Epoch 4/100\n",
      "270/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0189\n",
      "Epoch 4: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 8.0000e-04\n",
      "Epoch 5/100\n",
      "270/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0188\n",
      "Epoch 5: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 8.0000e-04\n",
      "Epoch 6/100\n",
      "266/276 [===========================>..] - ETA: 0s - loss: nan - accuracy: 0.0189\n",
      "Epoch 6: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 8.0000e-04\n",
      "Epoch 7/100\n",
      "273/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0186\n",
      "Epoch 7: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 8.0000e-04\n",
      "Epoch 8/100\n",
      " 92/276 [=========>....................] - ETA: 0s - loss: nan - accuracy: 0.0177"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0187\n",
      "Epoch 8: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 8.0000e-04\n",
      "Epoch 9/100\n",
      "274/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0187\n",
      "Epoch 9: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 8.0000e-04\n",
      "Epoch 10/100\n",
      "270/276 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.0184\n",
      "Epoch 10: saving model to training_1/\n",
      "276/276 [==============================] - 1s 5ms/step - loss: nan - accuracy: 0.0187 - val_loss: nan - val_accuracy: 0.0262 - lr: 8.0000e-04\n",
      "Epoch 11/100\n",
      "140/276 [==============>...............] - ETA: 0s - loss: nan - accuracy: 0.0168"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id=sweep,function=pre_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b6c064",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluate our locally saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2483cbb9",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 13:34:28.500761: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open training_1/.: FAILED_PRECONDITION: training_1; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2c09dde40>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint('training_1')\n",
    "baseline_model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec365219",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Evaluate our Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "638700bf",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 - 1s - loss: 2.9813 - accuracy: 0.2124 - 708ms/epoch - 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.9812943935394287, 0.21235857903957367]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.evaluate(test_ds, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed189bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Dowload model artifact/ our registered model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c0444ac4",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1bbc16d45ac0c40992e7cbcea9edb31f ['latest', 'epoch_29']\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "artifact = api.artifact(f'tiny-ml/wake_word_detection/run_{run.id}_model:latest', type='model')\n",
    "print(artifact.digest,artifact.aliases)\n",
    "file = artifact.download()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4217e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Load our pre trained tiny model from wandb artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "28eada02",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 - 0s - loss: 2.4910 - accuracy: 0.3246 - 86ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.4910478591918945, 0.32463011145591736]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.load_weights(f'artifacts/run_ex78039g_model:v50/cp.ckpt')\n",
    "baseline_model.evaluate(test_ds, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4a280760",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 2587.09it/s]\u001b[A\n",
      "\n",
      "  0%|                                                   | 0/349 [00:00<?, ?it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 349/349 [00:00<00:00, 2219.79it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  9.97it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410/410 [00:00<00:00, 2716689.79it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:01<00:00, 463.50it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [00:00<00:00, 2146579.82it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:01<00:00, 389.99it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 16000\n",
    "get_arm_spectrogram = Arm_spect().get_arm_spectrogram\n",
    "path = Path('./data/') \n",
    "paths = {pth.name:list(map(str,filter(lambda x:x.name != '.gitkeep',pth.iterdir()))) \n",
    "         for pth in path.iterdir() if pth.name in ['background','yes']}\n",
    "paths = {k:np.stack(list(filter(lambda x:len(x)==16384,map(read_wav,tqdm(v))))) \n",
    "         for (k,v) in tqdm(paths.items())}\n",
    "\n",
    "for k,v in paths.items():\n",
    "    filler_x = [augmenter(x,sample_rate) \n",
    "                for x in v[np.random.choice(v.shape[0], 500-len(v), replace=True)]]\n",
    "    filler_x = np.stack(list(map(lambda x:x[:16384], tqdm(filler_x))))\n",
    "    paths[k]=np.array(list(map(get_arm_spectrogram,tqdm(np.vstack([v,filler_x]))))).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ae8548cc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([array([[[1.07015625e+02, 6.92031250e+01, 1.55312500e+01, ...,\n",
       "         1.71875000e-01, 1.56250000e-01, 1.56250000e-01],\n",
       "        [1.22921875e+02, 6.72656250e+01, 4.59375000e+00, ...,\n",
       "         6.25000000e-02, 6.25000000e-02, 7.81250000e-02],\n",
       "        [1.27843750e+02, 6.41562500e+01, 1.71875000e-01, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.56250000e-02],\n",
       "        ...,\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[1.07015625e+02, 6.92031250e+01, 1.55312500e+01, ...,\n",
       "         1.71875000e-01, 1.56250000e-01, 1.56250000e-01],\n",
       "        [1.22921875e+02, 6.72656250e+01, 4.59375000e+00, ...,\n",
       "         6.25000000e-02, 6.25000000e-02, 7.81250000e-02],\n",
       "        [1.27843750e+02, 6.41562500e+01, 1.71875000e-01, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.56250000e-02],\n",
       "        ...,\n",
       "        [1.20250000e+02, 6.59843750e+01, 6.90625000e+00, ...,\n",
       "         2.34375000e-01, 9.37500000e-02, 3.12500000e-02],\n",
       "        [1.11859375e+02, 5.81406250e+01, 1.35000000e+01, ...,\n",
       "         4.37500000e-01, 1.56250000e-01, 1.56250000e-02],\n",
       "        [1.05609375e+02, 4.52812500e+01, 1.74062500e+01, ...,\n",
       "         8.12500000e-01, 4.37500000e-01, 2.81250000e-01]],\n",
       "\n",
       "       [[1.06062500e+02, 6.84843750e+01, 1.47656250e+01, ...,\n",
       "         9.53125000e-01, 8.43750000e-01, 7.81250000e-01],\n",
       "        [1.22250000e+02, 6.70625000e+01, 4.28125000e+00, ...,\n",
       "         6.71875000e-01, 6.25000000e-01, 5.93750000e-01],\n",
       "        [1.27546875e+02, 6.42812500e+01, 3.28125000e-01, ...,\n",
       "         2.96875000e-01, 2.96875000e-01, 2.81250000e-01],\n",
       "        ...,\n",
       "        [1.25968750e+02, 6.32343750e+01, 2.06250000e+00, ...,\n",
       "         6.71875000e-01, 6.71875000e-01, 6.71875000e-01],\n",
       "        [1.26984375e+02, 6.44531250e+01, 7.81250000e-01, ...,\n",
       "         3.43750000e-01, 2.96875000e-01, 2.50000000e-01],\n",
       "        [1.26968750e+02, 6.44218750e+01, 8.43750000e-01, ...,\n",
       "         3.43750000e-01, 3.28125000e-01, 2.81250000e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[2.53125000e+00, 2.41562500e+01, 2.01875000e+01, ...,\n",
       "         5.62500000e-01, 5.00000000e-01, 2.65625000e-01],\n",
       "        [1.80937500e+01, 2.48437500e+01, 1.39218750e+01, ...,\n",
       "         7.34375000e-01, 3.59375000e-01, 1.71875000e-01],\n",
       "        [2.99687500e+01, 2.12187500e+01, 8.37500000e+00, ...,\n",
       "         8.90625000e-01, 5.00000000e-01, 2.50000000e-01],\n",
       "        ...,\n",
       "        [4.29687500e+00, 5.51562500e+00, 3.57812500e+00, ...,\n",
       "         3.53125000e+00, 6.92187500e+00, 8.76562500e+00],\n",
       "        [1.82812500e+00, 4.67187500e+00, 3.67187500e+00, ...,\n",
       "         4.01562500e+00, 6.07812500e+00, 7.18750000e+00],\n",
       "        [7.81250000e-01, 3.40625000e+00, 3.25000000e+00, ...,\n",
       "         3.65625000e+00, 3.67187500e+00, 4.57812500e+00]]], dtype=float32), array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[1.38593750e+01, 1.93906250e+01, 2.77656250e+01, ...,\n",
       "         1.06250000e+00, 1.68750000e+00, 1.15625000e+00],\n",
       "        [1.53437500e+01, 1.72812500e+01, 3.24531250e+01, ...,\n",
       "         1.28125000e+00, 1.73437500e+00, 1.71875000e-01],\n",
       "        [1.25937500e+01, 9.03125000e+00, 3.09062500e+01, ...,\n",
       "         1.85937500e+00, 1.62500000e+00, 3.75000000e-01],\n",
       "        ...,\n",
       "        [1.13718750e+02, 5.88906250e+01, 2.48437500e+00, ...,\n",
       "         1.50000000e+00, 1.59375000e+00, 1.23437500e+00],\n",
       "        [1.16046875e+02, 5.88437500e+01, 2.00000000e+00, ...,\n",
       "         1.64062500e+00, 1.45312500e+00, 1.17187500e+00],\n",
       "        [1.16875000e+02, 5.83437500e+01, 1.37500000e+00, ...,\n",
       "         1.51562500e+00, 8.90625000e-01, 7.96875000e-01]],\n",
       "\n",
       "       [[8.99687500e+01, 6.44218750e+01, 2.23593750e+01, ...,\n",
       "         2.20312500e+00, 1.89062500e+00, 2.09375000e+00],\n",
       "        [8.74375000e+01, 5.73750000e+01, 1.28750000e+01, ...,\n",
       "         3.25000000e+00, 2.85937500e+00, 2.54687500e+00],\n",
       "        [6.95937500e+01, 4.29375000e+01, 1.07812500e+01, ...,\n",
       "         3.64062500e+00, 3.53125000e+00, 1.60937500e+00],\n",
       "        ...,\n",
       "        [9.90625000e+01, 4.96718750e+01, 2.40625000e+00, ...,\n",
       "         2.57812500e+00, 3.42187500e+00, 2.48437500e+00],\n",
       "        [9.97812500e+01, 4.78125000e+01, 1.32812500e+00, ...,\n",
       "         1.20312500e+00, 3.25000000e+00, 2.32812500e+00],\n",
       "        [1.01093750e+02, 4.97656250e+01, 3.09375000e+00, ...,\n",
       "         1.62500000e+00, 3.09375000e+00, 5.62500000e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.07656250e+01, 4.04375000e+01, 1.40781250e+01, ...,\n",
       "         1.92187500e+00, 9.37500000e-01, 1.50000000e+00],\n",
       "        [2.98906250e+01, 3.95937500e+01, 3.63125000e+01, ...,\n",
       "         1.78125000e+00, 1.71875000e-01, 1.31250000e+00],\n",
       "        [1.77031250e+01, 3.81875000e+01, 4.53437500e+01, ...,\n",
       "         1.81250000e+00, 1.07812500e+00, 9.06250000e-01],\n",
       "        ...,\n",
       "        [3.96093750e+01, 3.30937500e+01, 2.56093750e+01, ...,\n",
       "         3.07812500e+00, 3.31250000e+00, 2.01562500e+00],\n",
       "        [3.00937500e+01, 9.57812500e+00, 2.68750000e+01, ...,\n",
       "         5.14062500e+00, 4.46875000e+00, 3.75000000e-01],\n",
       "        [3.82812500e+01, 2.75468750e+01, 2.87343750e+01, ...,\n",
       "         5.87500000e+00, 5.18750000e+00, 2.98437500e+00]],\n",
       "\n",
       "       [[1.09531250e+01, 3.32343750e+01, 1.72343750e+01, ...,\n",
       "         5.17187500e+00, 3.34375000e+00, 2.00000000e+00],\n",
       "        [3.02812500e+01, 3.33125000e+01, 2.43281250e+01, ...,\n",
       "         5.29687500e+00, 3.04687500e+00, 1.92187500e+00],\n",
       "        [3.29062500e+01, 3.23750000e+01, 2.01250000e+01, ...,\n",
       "         4.57812500e+00, 1.85937500e+00, 1.37500000e+00],\n",
       "        ...,\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.28031250e+02, 6.39843750e+01, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[1.10062500e+02, 6.20625000e+01, 1.24687500e+01, ...,\n",
       "         3.12500000e-01, 1.39062500e+00, 3.12500000e-01],\n",
       "        [1.13093750e+02, 5.27343750e+01, 1.00312500e+01, ...,\n",
       "         5.62500000e-01, 1.09375000e+00, 6.40625000e-01],\n",
       "        [1.13937500e+02, 5.28125000e+01, 9.68750000e+00, ...,\n",
       "         9.84375000e-01, 7.34375000e-01, 1.37500000e+00],\n",
       "        ...,\n",
       "        [1.18296875e+02, 6.30468750e+01, 5.32812500e+00, ...,\n",
       "         1.25000000e-01, 1.09375000e-01, 7.81250000e-02],\n",
       "        [1.22015625e+02, 6.52187500e+01, 4.76562500e+00, ...,\n",
       "         9.37500000e-02, 9.37500000e-02, 1.71875000e-01],\n",
       "        [1.21625000e+02, 6.23125000e+01, 4.10937500e+00, ...,\n",
       "         4.21875000e-01, 4.37500000e-01, 4.68750000e-01]]], dtype=float32)])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ee8cdee4",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y = np.concatenate([np.zeros((1,500)).astype(np.uint16)[0], np.ones((1,500)).astype(np.uint16)[0]])\n",
    "x = np.vstack(list(paths.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "80441b99",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_wake_data = tf.data.Dataset.from_tensor_slices((tf.cast(x,tf.float32), y))\n",
    "train_wake_loader = train_wake_data.cache().shuffle(2000, seed=42).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3ea53ccf",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " target = 0, \n",
      " spectrogram = \n",
      " [[1.07015625e+02 6.92031250e+01 1.55312500e+01 ... 1.71875000e-01\n",
      "  1.56250000e-01 1.56250000e-01]\n",
      " [1.22921875e+02 6.72656250e+01 4.59375000e+00 ... 6.25000000e-02\n",
      "  6.25000000e-02 7.81250000e-02]\n",
      " [1.27843750e+02 6.41562500e+01 1.71875000e-01 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.56250000e-02]\n",
      " ...\n",
      " [1.28031250e+02 6.39843750e+01 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.28031250e+02 6.39843750e+01 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.28031250e+02 6.39843750e+01 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(train_wake_data.take(1)))\n",
    "print(f' target = {y}, \\n spectrogram = \\n {x}')\n",
    "input_shape = tf.expand_dims(x, axis=-1).shape\n",
    "norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "norm_layer.adapt(train_wake_data.map(lambda x, y: tf.reshape(x, input_shape)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "539a8df6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249, 257, 1)\n"
     ]
    }
   ],
   "source": [
    "input_shape = tf.expand_dims(x, axis=-1).shape\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "be5316e1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def fine_tune(run=None):\n",
    "    base_model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Input(shape=input_shape),\n",
    "      tf.keras.layers.experimental.preprocessing.Resizing(32, 32, interpolation=\"nearest\"), \n",
    "      norm_layer,\n",
    "      tf.keras.layers.Conv2D(8, kernel_size=(8,8), strides=(2, 2), activation=\"relu\"),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dropout(0.25),\n",
    "      tf.keras.layers.Dense(50, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Initialize a new W&B run\n",
    "    run = wandb.init(config={\"bs\": 12},entity='tiny-ml',project = 'wake_word_detection', group='training',settings={'quiet':True,'silent':True,'show_warnings':False,'show_info':False})\n",
    "\n",
    "\n",
    "    EPOCHS = 100\n",
    "    history = wake_word_model.fit(\n",
    "        train_wake_loader, \n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    model_body = tf.keras.Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "    classifier_head = tf.keras.layers.Dense(1, activation=\"sigmoid\")(model_body.output)\n",
    "    wake_word_model = tf.keras.Model(model_body.input, classifier_head)\n",
    "\n",
    "    checkpoint_path = \"training_2/\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    print(checkpoint_dir)\n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(verbose=0, patience=25), \n",
    "        tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    ,cp_callback,WandbMetricsLogger(),WandbModelCheckpoint(checkpoint_path)]\n",
    "\n",
    "    wake_word_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "\n",
    "    EPOCHS = 100\n",
    "    history = wake_word_model.fit(\n",
    "        train_wake_loader, \n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "860affaf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 249, 257, 1)]     0         \n",
      "                                                                 \n",
      " resizing_1 (Resizing)       (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " normalization_1 (Normalizat  (None, 32, 32, 1)        3         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 8)         520       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 8)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 288)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 288)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 812\n",
      "Trainable params: 809\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wake_word_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8c48223b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for layer in wake_word_model.layers:\n",
    "    layer.trainable = False\n",
    "wake_word_model.layers[-1].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1d073e6d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_2\n"
     ]
    }
   ],
   "source": [
    "# Initialize a new W&B run\n",
    "run = wandb.init(config={\"bs\": 12},entity='tiny-ml',project = 'wake_word_detection', group='training',settings={'quiet':True,'silent':True,'show_warnings':False,'show_info':False})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d0615265",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wake_word_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=METRICS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f8697694",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1/32 [..............................] - ETA: 8s - loss: 0.8022 - accuracy: 0.3438WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 32ms/step - loss: 0.6432 - accuracy: 0.6460 - lr: 0.0030\n",
      "Epoch 2/100\n",
      "23/32 [====================>.........] - ETA: 0s - loss: 0.4952 - accuracy: 0.7894WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.4941 - accuracy: 0.7900 - lr: 0.0030\n",
      "Epoch 3/100\n",
      "25/32 [======================>.......] - ETA: 0s - loss: 0.4435 - accuracy: 0.8138WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.4514 - accuracy: 0.8150 - lr: 0.0030\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4206 - accuracy: 0.8390WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 30ms/step - loss: 0.4206 - accuracy: 0.8390 - lr: 0.0030\n",
      "Epoch 5/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.4099 - accuracy: 0.8380WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.4056 - accuracy: 0.8430 - lr: 0.0030\n",
      "Epoch 6/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3998 - accuracy: 0.8330WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 34ms/step - loss: 0.4035 - accuracy: 0.8330 - lr: 0.0030\n",
      "Epoch 7/100\n",
      "23/32 [====================>.........] - ETA: 0s - loss: 0.3844 - accuracy: 0.8519WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 32ms/step - loss: 0.3826 - accuracy: 0.8520 - lr: 0.0030\n",
      "Epoch 8/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3971 - accuracy: 0.8438WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 24ms/step - loss: 0.3957 - accuracy: 0.8460 - lr: 0.0030\n",
      "Epoch 9/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3906 - accuracy: 0.8479WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3859 - accuracy: 0.8500 - lr: 0.0030\n",
      "Epoch 10/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3720 - accuracy: 0.8527WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3788 - accuracy: 0.8510 - lr: 0.0030\n",
      "Epoch 11/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3628 - accuracy: 0.8631WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3677 - accuracy: 0.8600 - lr: 0.0027\n",
      "Epoch 12/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.5840 - accuracy: 0.7188WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 32ms/step - loss: 0.3583 - accuracy: 0.8590 - lr: 0.0025\n",
      "Epoch 13/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.3783 - accuracy: 0.8438WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3601 - accuracy: 0.8590 - lr: 0.0022\n",
      "Epoch 14/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3573 - accuracy: 0.8629WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3561 - accuracy: 0.8640 - lr: 0.0020\n",
      "Epoch 15/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3533 - accuracy: 0.8650WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3529 - accuracy: 0.8650 - lr: 0.0018\n",
      "Epoch 16/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3576 - accuracy: 0.8522WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3523 - accuracy: 0.8560 - lr: 0.0016\n",
      "Epoch 17/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3773 - accuracy: 0.8594WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 34ms/step - loss: 0.3643 - accuracy: 0.8630 - lr: 0.0015\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3386 - accuracy: 0.8580WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3386 - accuracy: 0.8580 - lr: 0.0013\n",
      "Epoch 19/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3467 - accuracy: 0.8510WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3438 - accuracy: 0.8520 - lr: 0.0012\n",
      "Epoch 20/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2350 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 32ms/step - loss: 0.3560 - accuracy: 0.8670 - lr: 0.0011\n",
      "Epoch 21/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3445 - accuracy: 0.8627WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 24ms/step - loss: 0.3528 - accuracy: 0.8600 - lr: 9.9861e-04\n",
      "Epoch 22/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3448 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3385 - accuracy: 0.8770 - lr: 9.0358e-04\n",
      "Epoch 23/100\n",
      "25/32 [======================>.......] - ETA: 0s - loss: 0.3576 - accuracy: 0.8575WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3490 - accuracy: 0.8620 - lr: 8.1760e-04\n",
      "Epoch 24/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3490 - accuracy: 0.8685WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3502 - accuracy: 0.8670 - lr: 7.3979e-04\n",
      "Epoch 25/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3615 - accuracy: 0.8583WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3520 - accuracy: 0.8650 - lr: 6.6939e-04\n",
      "Epoch 26/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.1878 - accuracy: 0.9688WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3400 - accuracy: 0.8710 - lr: 6.0569e-04\n",
      "Epoch 27/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3468 - accuracy: 0.8578WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3446 - accuracy: 0.8600 - lr: 5.4805e-04\n",
      "Epoch 28/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3455 - accuracy: 0.8650WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3455 - accuracy: 0.8630 - lr: 4.9590e-04\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3538 - accuracy: 0.8570WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3538 - accuracy: 0.8570 - lr: 4.4871e-04\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.8630WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3444 - accuracy: 0.8630 - lr: 4.0601e-04\n",
      "Epoch 31/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3405 - accuracy: 0.8675WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3369 - accuracy: 0.8680 - lr: 3.6737e-04\n",
      "Epoch 32/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2964 - accuracy: 0.8438WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3337 - accuracy: 0.8660 - lr: 3.3241e-04\n",
      "Epoch 33/100\n",
      "23/32 [====================>.........] - ETA: 0s - loss: 0.3429 - accuracy: 0.8628WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3499 - accuracy: 0.8660 - lr: 3.0078e-04\n",
      "Epoch 34/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3484 - accuracy: 0.8702WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3535 - accuracy: 0.8650 - lr: 2.7215e-04\n",
      "Epoch 35/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2382 - accuracy: 0.9688WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3373 - accuracy: 0.8660 - lr: 2.4626e-04\n",
      "Epoch 36/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3390 - accuracy: 0.8657WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3385 - accuracy: 0.8690 - lr: 2.2282e-04\n",
      "Epoch 37/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3442 - accuracy: 0.8623WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3384 - accuracy: 0.8660 - lr: 2.0162e-04\n",
      "Epoch 38/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3469 - accuracy: 0.8621WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 32ms/step - loss: 0.3478 - accuracy: 0.8590 - lr: 1.8243e-04\n",
      "Epoch 39/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3445 - accuracy: 0.8708WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 24ms/step - loss: 0.3445 - accuracy: 0.8710 - lr: 1.6507e-04\n",
      "Epoch 40/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3420 - accuracy: 0.8638WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3447 - accuracy: 0.8650 - lr: 1.4936e-04\n",
      "Epoch 41/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3321 - accuracy: 0.8707WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 34ms/step - loss: 0.3312 - accuracy: 0.8680 - lr: 1.3515e-04\n",
      "Epoch 42/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3321 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 24ms/step - loss: 0.3357 - accuracy: 0.8720 - lr: 1.2229e-04\n",
      "Epoch 43/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2738 - accuracy: 0.9688WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 24ms/step - loss: 0.3396 - accuracy: 0.8720 - lr: 1.1065e-04\n",
      "Epoch 44/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3372 - accuracy: 0.8669WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3391 - accuracy: 0.8680 - lr: 1.0012e-04\n",
      "Epoch 45/100\n",
      "25/32 [======================>.......] - ETA: 0s - loss: 0.3234 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 32ms/step - loss: 0.3309 - accuracy: 0.8690 - lr: 9.0592e-05\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.8760WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3207 - accuracy: 0.8760 - lr: 8.1971e-05\n",
      "Epoch 47/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3425 - accuracy: 0.8599WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 24ms/step - loss: 0.3442 - accuracy: 0.8610 - lr: 7.4171e-05\n",
      "Epoch 48/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3345 - accuracy: 0.8649WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 39ms/step - loss: 0.3350 - accuracy: 0.8640 - lr: 6.7112e-05\n",
      "Epoch 49/100\n",
      "19/32 [================>.............] - ETA: 0s - loss: 0.3543 - accuracy: 0.8602WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 34ms/step - loss: 0.3374 - accuracy: 0.8740 - lr: 6.0726e-05\n",
      "Epoch 50/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3508 - accuracy: 0.8600WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3471 - accuracy: 0.8620 - lr: 5.4947e-05\n",
      "Epoch 51/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3330 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3297 - accuracy: 0.8780 - lr: 4.9718e-05\n",
      "Epoch 52/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3299 - accuracy: 0.8786WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3252 - accuracy: 0.8820 - lr: 4.4987e-05\n",
      "Epoch 53/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3457 - accuracy: 0.8605WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3411 - accuracy: 0.8660 - lr: 4.0706e-05\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3355 - accuracy: 0.8700WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 33ms/step - loss: 0.3355 - accuracy: 0.8700 - lr: 3.6832e-05\n",
      "Epoch 55/100\n",
      "22/32 [===================>..........] - ETA: 0s - loss: 0.3362 - accuracy: 0.8608WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 39ms/step - loss: 0.3272 - accuracy: 0.8690 - lr: 3.3327e-05\n",
      "Epoch 56/100\n",
      "20/32 [=================>............] - ETA: 0s - loss: 0.3415 - accuracy: 0.8578WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 27ms/step - loss: 0.3340 - accuracy: 0.8670 - lr: 3.0156e-05\n",
      "Epoch 57/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.4667 - accuracy: 0.7812WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 24ms/step - loss: 0.3378 - accuracy: 0.8610 - lr: 2.7286e-05\n",
      "Epoch 58/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3450 - accuracy: 0.8620WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3470 - accuracy: 0.8590 - lr: 2.4689e-05\n",
      "Epoch 59/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.3197 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 32ms/step - loss: 0.3355 - accuracy: 0.8530 - lr: 2.2340e-05\n",
      "Epoch 60/100\n",
      "23/32 [====================>.........] - ETA: 0s - loss: 0.3412 - accuracy: 0.8655WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3422 - accuracy: 0.8700 - lr: 2.0214e-05\n",
      "Epoch 61/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3286 - accuracy: 0.8718WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3322 - accuracy: 0.8710 - lr: 1.8290e-05\n",
      "Epoch 62/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2629 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 24ms/step - loss: 0.3328 - accuracy: 0.8660 - lr: 1.6550e-05\n",
      "Epoch 63/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3268 - accuracy: 0.8815WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3337 - accuracy: 0.8780 - lr: 1.4975e-05\n",
      "Epoch 64/100\n",
      "25/32 [======================>.......] - ETA: 0s - loss: 0.3424 - accuracy: 0.8612WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3377 - accuracy: 0.8680 - lr: 1.3550e-05\n",
      "Epoch 65/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2981 - accuracy: 0.9062WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 24ms/step - loss: 0.3412 - accuracy: 0.8690 - lr: 1.2260e-05\n",
      "Epoch 66/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3409 - accuracy: 0.8672WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 35ms/step - loss: 0.3419 - accuracy: 0.8650 - lr: 1.1094e-05\n",
      "Epoch 67/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3289 - accuracy: 0.8639WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3287 - accuracy: 0.8640 - lr: 1.0038e-05\n",
      "Epoch 68/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3405 - accuracy: 0.8621WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3402 - accuracy: 0.8620 - lr: 9.0827e-06\n",
      "Epoch 69/100\n",
      "21/32 [==================>...........] - ETA: 0s - loss: 0.3199 - accuracy: 0.8705WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 69: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3350 - accuracy: 0.8610 - lr: 8.2183e-06\n",
      "Epoch 70/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.3019 - accuracy: 0.9062WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 32ms/step - loss: 0.3333 - accuracy: 0.8710 - lr: 7.4363e-06\n",
      "Epoch 71/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3529 - accuracy: 0.8610WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 24ms/step - loss: 0.3422 - accuracy: 0.8670 - lr: 6.7286e-06\n",
      "Epoch 72/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.3856 - accuracy: 0.7500WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3468 - accuracy: 0.8610 - lr: 6.0883e-06\n",
      "Epoch 73/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.3993 - accuracy: 0.8125WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3289 - accuracy: 0.8740 - lr: 5.5089e-06\n",
      "Epoch 74/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3499 - accuracy: 0.8642WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 74: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 34ms/step - loss: 0.3382 - accuracy: 0.8720 - lr: 4.9847e-06\n",
      "Epoch 75/100\n",
      "23/32 [====================>.........] - ETA: 0s - loss: 0.3408 - accuracy: 0.8668WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 33ms/step - loss: 0.3264 - accuracy: 0.8760 - lr: 4.5103e-06\n",
      "Epoch 76/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3322 - accuracy: 0.8730WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3328 - accuracy: 0.8730 - lr: 4.0811e-06\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3491 - accuracy: 0.8640WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3491 - accuracy: 0.8640 - lr: 3.6927e-06\n",
      "Epoch 78/100\n",
      "26/32 [=======================>......] - ETA: 0s - loss: 0.3236 - accuracy: 0.8726WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 32ms/step - loss: 0.3308 - accuracy: 0.8690 - lr: 3.3413e-06\n",
      "Epoch 79/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3312 - accuracy: 0.8761WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 7s 221ms/step - loss: 0.3273 - accuracy: 0.8740 - lr: 3.0234e-06\n",
      "Epoch 80/100\n",
      "25/32 [======================>.......] - ETA: 0s - loss: 0.3247 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3315 - accuracy: 0.8700 - lr: 2.7356e-06\n",
      "Epoch 81/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3439 - accuracy: 0.8594WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3391 - accuracy: 0.8630 - lr: 2.4753e-06\n",
      "Epoch 82/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3448 - accuracy: 0.8669WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3342 - accuracy: 0.8730 - lr: 2.2398e-06\n",
      "Epoch 83/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3317 - accuracy: 0.8679WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 83: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3317 - accuracy: 0.8680 - lr: 2.0266e-06\n",
      "Epoch 84/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3357 - accuracy: 0.8698WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3390 - accuracy: 0.8710 - lr: 1.8338e-06\n",
      "Epoch 85/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3299 - accuracy: 0.8704WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 85: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3299 - accuracy: 0.8720 - lr: 1.6593e-06\n",
      "Epoch 86/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3480 - accuracy: 0.8599WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 32ms/step - loss: 0.3484 - accuracy: 0.8610 - lr: 1.5014e-06\n",
      "Epoch 87/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3375 - accuracy: 0.8690WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3359 - accuracy: 0.8700 - lr: 1.3585e-06\n",
      "Epoch 88/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3273 - accuracy: 0.8795WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 34ms/step - loss: 0.3290 - accuracy: 0.8750 - lr: 1.2292e-06\n",
      "Epoch 89/100\n",
      "24/32 [=====================>........] - ETA: 0s - loss: 0.3402 - accuracy: 0.8646WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 33ms/step - loss: 0.3366 - accuracy: 0.8680 - lr: 1.1122e-06\n",
      "Epoch 90/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3317 - accuracy: 0.8687WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3358 - accuracy: 0.8670 - lr: 1.0064e-06\n",
      "Epoch 91/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.3021 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3367 - accuracy: 0.8630 - lr: 9.1062e-07\n",
      "Epoch 92/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2359 - accuracy: 0.9688WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 44ms/step - loss: 0.3416 - accuracy: 0.8670 - lr: 8.2396e-07\n",
      "Epoch 93/100\n",
      "27/32 [========================>.....] - ETA: 0s - loss: 0.3432 - accuracy: 0.8762WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step - loss: 0.3373 - accuracy: 0.8790 - lr: 7.4555e-07\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.8710WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 33ms/step - loss: 0.3306 - accuracy: 0.8710 - lr: 6.7460e-07\n",
      "Epoch 95/100\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 0.3378 - accuracy: 0.8605WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 32ms/step - loss: 0.3315 - accuracy: 0.8660 - lr: 6.1041e-07\n",
      "Epoch 96/100\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.3344 - accuracy: 0.8761WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3300 - accuracy: 0.8770 - lr: 5.5232e-07\n",
      "Epoch 97/100\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.3472 - accuracy: 0.8609WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3455 - accuracy: 0.8620 - lr: 4.9976e-07\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3501 - accuracy: 0.8570WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3501 - accuracy: 0.8570 - lr: 4.5220e-07\n",
      "Epoch 99/100\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.3312 - accuracy: 0.8781WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step - loss: 0.3289 - accuracy: 0.8790 - lr: 4.0917e-07\n",
      "Epoch 100/100\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.3597 - accuracy: 0.8438WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100: saving model to training_2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: training_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3408 - accuracy: 0.8750 - lr: 3.7023e-07\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "history = wake_word_model.fit(\n",
    "    train_wake_loader, \n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "78f7893a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for layer in wake_word_model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "38208ba5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 249, 257, 1)]     0         \n",
      "                                                                 \n",
      " resizing_7 (Resizing)       (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " normalization_7 (Normalizat  (None, 32, 32, 1)        3         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " quant_conv2d_7 (QuantizeWra  (None, 13, 13, 8)        539       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 6, 6, 8)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 288)               0         \n",
      "                                                                 \n",
      " quant_dropout_7 (QuantizeWr  (None, 288)              1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_11 (QuantizeWra  (None, 1)                294       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 837\n",
      "Trainable params: 809\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "def apply_qat_to_dense_and_cnn(layer):\n",
    "  if isinstance(layer, (tf.keras.layers.Dense, tf.keras.layers.Conv2D)):\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "  return layer\n",
    "\n",
    "annotated_model = tf.keras.models.clone_model(\n",
    "    wake_word_model,\n",
    "    clone_function=apply_qat_to_dense_and_cnn,\n",
    ")\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "51c5120e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 1s 3ms/step - loss: 0.3295 - accuracy: 0.8700\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2932 - accuracy: 0.8800\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3004 - accuracy: 0.8810\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2875 - accuracy: 0.8880\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3177 - accuracy: 0.8870\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2913 - accuracy: 0.8890\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2545 - accuracy: 0.8830\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2657 - accuracy: 0.9040\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2729 - accuracy: 0.8990\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2448 - accuracy: 0.9070\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2551 - accuracy: 0.8960\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2521 - accuracy: 0.8960\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2439 - accuracy: 0.9050\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2345 - accuracy: 0.9060\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.8990\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2560 - accuracy: 0.9040\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2104 - accuracy: 0.9220\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.9040\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2440 - accuracy: 0.9010\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2081 - accuracy: 0.9190\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2047 - accuracy: 0.9160\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2220 - accuracy: 0.9110\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.9090\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2317 - accuracy: 0.9050\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9220\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.9170\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 0.9130\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2121 - accuracy: 0.9140\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2112 - accuracy: 0.9220\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2074 - accuracy: 0.9190\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2049 - accuracy: 0.9210\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1982 - accuracy: 0.9150\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2113 - accuracy: 0.9150\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.9240\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2140 - accuracy: 0.9140\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1909 - accuracy: 0.9230\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.9180\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1968 - accuracy: 0.9200\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1919 - accuracy: 0.9260\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2012 - accuracy: 0.9200\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.9170\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1883 - accuracy: 0.9210\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1974 - accuracy: 0.9230\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.1991 - accuracy: 0.9170\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1987 - accuracy: 0.9240\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1903 - accuracy: 0.9240\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9310\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1984 - accuracy: 0.9270\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9190\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1762 - accuracy: 0.9260\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1887 - accuracy: 0.9240\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1944 - accuracy: 0.9090\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1796 - accuracy: 0.9300\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1756 - accuracy: 0.9320\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1776 - accuracy: 0.9260\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1815 - accuracy: 0.9280\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1793 - accuracy: 0.9300\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1642 - accuracy: 0.9360\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1929 - accuracy: 0.9300\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1892 - accuracy: 0.9190\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9300\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.9290\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1798 - accuracy: 0.9270\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1721 - accuracy: 0.9320\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1649 - accuracy: 0.9330\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1795 - accuracy: 0.9290\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1630 - accuracy: 0.9350\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1795 - accuracy: 0.9310\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1706 - accuracy: 0.9320\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1716 - accuracy: 0.9230\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1611 - accuracy: 0.9360\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1701 - accuracy: 0.9380\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1708 - accuracy: 0.9320\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1791 - accuracy: 0.9210\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1568 - accuracy: 0.9390\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1683 - accuracy: 0.9300\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1646 - accuracy: 0.9330\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1658 - accuracy: 0.9320\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1694 - accuracy: 0.9320\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1761 - accuracy: 0.9270\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1745 - accuracy: 0.9320\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.9330\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1619 - accuracy: 0.9320\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1634 - accuracy: 0.9350\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1693 - accuracy: 0.9330\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1794 - accuracy: 0.9300\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1690 - accuracy: 0.9350\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1643 - accuracy: 0.9350\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1597 - accuracy: 0.9320\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1420 - accuracy: 0.9420\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1490 - accuracy: 0.9430\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1544 - accuracy: 0.9390\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1467 - accuracy: 0.9460\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1689 - accuracy: 0.9330\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9310\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1574 - accuracy: 0.9340\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1712 - accuracy: 0.9300\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1386 - accuracy: 0.9430\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1566 - accuracy: 0.9360\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9320\n"
     ]
    }
   ],
   "source": [
    "quant_aware_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=METRICS,\n",
    ")\n",
    "\n",
    "EPOCHS=100\n",
    "quant_aware_history = quant_aware_model.fit(\n",
    "    train_wake_loader,  \n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b4d4d4c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, dropout_7_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/tmp4_1be5sa/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/tmp4_1be5sa/assets\n",
      "/Users/fridadesigley/.pyenv/versions/3.10.6/envs/tiny-ml/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-01-29 22:08:23.786690: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-01-29 22:08:23.786702: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-01-29 22:08:23.786789: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/tmp4_1be5sa\n",
      "2023-01-29 22:08:23.788279: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-01-29 22:08:23.788285: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/tmp4_1be5sa\n",
      "2023-01-29 22:08:23.792562: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-01-29 22:08:23.815996: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /var/folders/q8/9dhs6y4d21s87vxtnwqflq5m0000gn/T/tmp4_1be5sa\n",
      "2023-01-29 22:08:23.822177: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 35389 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "596831a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6016"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_models_dir = Path(\"tf_lite_model\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "tflite_model_file = tflite_models_dir/\"wake_word.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "59546720",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"alignas(8) const unsigned char tflite_model[] = {\" > tf_lite_model/wake_word.tflite.h\n",
    "cat tf_lite_model/wake_word.tflite | xxd -i                        >> tf_lite_model/wake_word.tflite.h\n",
    "echo \"};\"                                               >> tf_lite_model/wake_word.tflite.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f54793df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wake_word.tflite   wake_word.tflite.h\r\n"
     ]
    }
   ],
   "source": [
    "!ls tf_lite_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daba4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "rise": {
   "backimage": "wandb_bg.png",
   "centre": false,
   "controls": false,
   "embedded": false,
   "enable_chalkboard": false,
   "height": "99%",
   "margin": 0.09,
   "progress": true,
   "scroll": false,
   "slideNumber": false,
   "start_slideshow_at": "selected",
   "theme": "black",
   "transition": "convex",
   "width": "100%"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
